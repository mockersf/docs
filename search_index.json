{"LANDED-table":{"location":"worker-internals.html#LANDED-table","title":"LANDED","text":"A worker in this state has successfully waited for all non-interruptible jobs on it after having concourse land-worker called. It will no longer be used to schedule any new containers or create volumes until it registers as RUNNING again.\n\n","depth":5,"section_tag":"worker-lifecycle"},"LANDING-table":{"location":"worker-internals.html#LANDING-table","title":"LANDING","text":"The concourse land-worker command will put a worker in the LANDING state to safely drain its assignments for temporary downtime.\n\nThe ATC will wait for builds on the worker for jobs which aren't interruptible to finish, and transition the worker into LANDED state.\n\n","depth":5,"section_tag":"worker-lifecycle"},"RETIRING-table":{"location":"worker-internals.html#RETIRING-table","title":"RETIRING","text":"The concourse retire-worker command will put a worker in the RETIRING state to remove it from the cluster permanently.\n\nThe ATC will wait for builds on the worker for jobs which aren't interruptible to finish, and remove the worker.\n\n","depth":5,"section_tag":"worker-lifecycle"},"RUNNING-table":{"location":"worker-internals.html#RUNNING-table","title":"RUNNING","text":"A worker in this state is registered with the cluster and ready to start running containers and storing volumes.\n\n","depth":5,"section_tag":"worker-lifecycle"},"STALLED-table":{"location":"worker-internals.html#STALLED-table","title":"STALLED","text":"A worker in this state was previously registered with the cluster, but stopped advertising itself for some reason. Ususally this is due to network connectivity issues, or the worker stopping unexpectedly.\n\nIf the worker remains in this state and cannot be recovered, it can be removed using the fly prune-worker command.\n\n","depth":5,"section_tag":"worker-lifecycle"},"abstract-objects":{"location":"database-schema.html#abstract-objects","title":"Abstract Objects","text":"Concourse's database manages the abstractions around resources, resource configuration, resource versioning, and resource types. These tables help make the Resources concept operate as expected, scale across many pipelines, and to many different types.\n\n","depth":5,"section_tag":"abstract-objects"},"adding-cf-users-to-the-main-team":{"location":"cf-uaa-auth.html#adding-cf-users-to-the-main-team","title":"Adding CF Users to the main Team","text":"CloudFoundry users and org/space members can be added to the The main team authorization config by setting the following env on the Running a web node:\n\nCONCOURSE_MAIN_TEAM_CF_USER=username\nCONCOURSE_MAIN_TEAM_CF_SPACE=org-name:space-name\nCONCOURSE_MAIN_TEAM_CF_SPACE_GUID=SPACE_GUID\nCONCOURSE_MAIN_TEAM_CF_ORG=org-name\nMultiple users, spaces, etc. may be specified by comma-separating them.\n\n","depth":6,"section_tag":"adding-cf-users-to-the-main-team"},"adding-to-this-list":{"location":"community.html#adding-to-this-list","title":"Adding to this list","text":"Fork the concourse/docs repository, add a file describing your resource type in  lit/reference/resource-types/comunity-list/, then make a pull request.\n\n","depth":3,"section_tag":"adding-to-this-list"},"administration":{"location":"administration.html","title":"Administration","text":"","depth":3,"section_tag":"administration"},"aggregate":{"location":"aggregate-step.html#aggregate","title":"aggregate","text":"Performs the given steps in parallel.\n\nIf any sub-steps in an aggregate result in an error, the aggregate step as a whole is considered to have errored.\n\nSimilarly, when aggregating task steps, if any fail, the aggregate step will fail.\n\n","depth":4,"section_tag":"aggregate-step"},"aggregate-step":{"location":"aggregate-step.html","title":"aggregate step","text":"","depth":4,"section_tag":"aggregate-step"},"architecture":{"location":"architecture.html","title":"Architecture","text":"Concourse is a fairly simple distributed system built up from the following components. You'll see them referenced here and there throughout the documentation, so you may want to skim this page just to get an idea of what they are.\n\n{image: images/concourse_architecture.png}\n\n","depth":2,"section_tag":"architecture"},"architecture-worker":{"location":"architecture.html#architecture-worker","title":"Workers: container runtime \u0026 cache management","text":"Workers are machines running Garden and Baggageclaim servers and registering themselves via the TSA.\n\nWorkers have no important state configured on their machines, as everything runs in a container and thus shouldn't care about what packages are installed on the host (well, except for those that allow it to be a worker in the first place). This is very different from workers in other non-containerized CI solutions, where the state of packages on the worker is crucial to whether your pipeline works or not.\n\nEach worker registers itself with the Concourse cluster via the TSA.\n\nWorkers by default listen on port 7777 for Garden and port 7788 for Baggageclaim. If they are within a private network reachable by the ATC, they'll probably bind on all addresses (0.0.0.0) and register themselves directly. Otherwise they should bind on 127.0.0.1 and forward themselves through the TSA.\n\n","depth":3,"section_tag":"architecture-worker"},"attempts":{"location":"attempts-step-modifier.html#attempts","title":"attempts","text":"The total number of times a step should be tried should it fail, e.g. 5 will try the step up to 5 times before giving up.\n\nWhen the number of attempts is reached and the step has still not succeeded then the step will fail.\n\n","depth":4,"section_tag":"attempts-step-modifier"},"attempts-step-modifier":{"location":"attempts-step-modifier.html","title":"attempts step modifier","text":"Any step can set the number of times it should be attempted by attaching an attempts parameter with the number of times it should be tried.\n\nAttempts will retry on a Concourse error as well as build failure.\n\n","depth":4,"section_tag":"attempts-step-modifier"},"auth":{"location":"auth.html","title":"Auth \u0026 Teams","text":"A single Concourse installation can accomodate many projects and users.\n\nPipelines, builds, and all other user data are owned by teams. A team is just a conceptual owner and a separate namespace, tied to an authorization config. For example, a team may authorize all members of the concourse GitHub organization to be a member.\n\nWhen a user authenticates, each team's authorization config is checked against the user to determine which role, if any, to grant for the team. This information is then stored in the user's token to determine access control for future requests.\n\n","depth":2,"section_tag":"auth"},"authenticating-with-vault":{"location":"vault-credential-manager.html#authenticating-with-vault","title":"Authenticating with Vault","text":"There are many ways to authenticate with a Vault server. The web-node can be configured with either a token or an arbitrary auth backend and arbitrary auth params, so just about all of them should be configurable.\n\nWhen the web node acquires a token, either by logging in with an auth backend or by being given one directly, it will continuously renew the token to ensure it doesn't expire. The renewal interval is half of the token's lease duration.\n\n","depth":5,"section_tag":"authenticating-with-vault"},"aws-asm-credential-manager":{"location":"aws-asm-credential-manager.html","title":"The AWS Secrets Manager credential manager","text":"","depth":4,"section_tag":"aws-asm-credential-manager"},"aws-secretsmanager-access-key":{"location":"aws-asm-credential-manager.html#aws-secretsmanager-access-key","title":"aws-secretsmanager-access-key","text":"A valid AWS access key.\n\nEnvironment variable CONCOURSE_AWS_SECRETSMANAGER_ACCESS_KEY.\n\n","depth":5,"section_tag":"configuration"},"aws-secretsmanager-pipeline-secret-template":{"location":"aws-asm-credential-manager.html#aws-secretsmanager-pipeline-secret-template","title":"aws-secretsmanager-pipeline-secret-template","text":"The base path used when attempting to locate a pipeline-level secret.\n\nEnvironment variable CONCOURSE_AWS_SECRETSMANAGER_PIPELINE_SECRET_TEMPLATE.\n\nExample:\n\nDefault: /concourse/{{.Team}}/{{.Pipeline}}/{{.Secret}}\n\n","depth":5,"section_tag":"configuration"},"aws-secretsmanager-region":{"location":"aws-asm-credential-manager.html#aws-secretsmanager-region","title":"aws-secretsmanager-region","text":"The AWS region that requests to Secrets Manager will be sent to.\n\nEnvironment variable AWS_REGION.\n\n","depth":5,"section_tag":"configuration"},"aws-secretsmanager-secret-key":{"location":"aws-asm-credential-manager.html#aws-secretsmanager-secret-key","title":"aws-secretsmanager-secret-key","text":"The secret key that corresponds to the access key defined above.\n\nEnvironment variable CONCOURSE_AWS_SECRETSMANAGER_SECRET_KEY.\n\n","depth":5,"section_tag":"configuration"},"aws-secretsmanager-session-token":{"location":"aws-asm-credential-manager.html#aws-secretsmanager-session-token","title":"aws-secretsmanager-session-token","text":"A valid AWS session token.\n\nEnvironment variable CONCOURSE_AWS_SECRETSMANAGER_SESSION_TOKEN.\n\n","depth":5,"section_tag":"configuration"},"aws-secretsmanager-team-secret-template":{"location":"aws-asm-credential-manager.html#aws-secretsmanager-team-secret-template","title":"aws-secretsmanager-team-secret-template","text":"The base path used when attempting to locate a team-level secret.\n\nEnvironment variable CONCOURSE_AWS_SECRETSMANAGER_TEAM_SECRET_TEMPLATE.\n\nExample:\n\nDefault: /concourse/{{.Team}}/{{.Secret}}\n\n","depth":5,"section_tag":"configuration"},"aws-ssm-access-key":{"location":"aws-ssm-credential-manager.html#aws-ssm-access-key","title":"aws-ssm-access-key","text":"A valid AWS access key.\n\nEnvironment variable CONCOURSE_AWS_SSM_ACCESS_KEY.\n\n","depth":5,"section_tag":"configuration"},"aws-ssm-credential-manager":{"location":"aws-ssm-credential-manager.html","title":"The AWS SSM credential manager","text":"","depth":4,"section_tag":"aws-ssm-credential-manager"},"aws-ssm-pipeline-secret-template":{"location":"aws-ssm-credential-manager.html#aws-ssm-pipeline-secret-template","title":"aws-ssm-pipeline-secret-template","text":"The base path used when attempting to locate a pipeline-level secret.\n\nEnvironment variable CONCOURSE_AWS_SSM_PIPELINE_SECRET_TEMPLATE.\n\nExample:\n\nDefault: /concourse/{{.Team}}/{{.Pipeline}}/{{.Secret}}\n\n","depth":5,"section_tag":"configuration"},"aws-ssm-region":{"location":"aws-ssm-credential-manager.html#aws-ssm-region","title":"aws-ssm-region","text":"The AWS region that requests to parameter store will be sent to.\n\nEnvironment variable AWS_REGION.\n\n","depth":5,"section_tag":"configuration"},"aws-ssm-secret-key":{"location":"aws-ssm-credential-manager.html#aws-ssm-secret-key","title":"aws-ssm-secret-key","text":"The secret key that corresponds to the access key defined above.\n\nEnvironment variable CONCOURSE_AWS_SSM_SECRET_KEY.\n\n","depth":5,"section_tag":"configuration"},"aws-ssm-session-token":{"location":"aws-ssm-credential-manager.html#aws-ssm-session-token","title":"aws-ssm-session-token","text":"A valid AWS session token.\n\nEnvironment variable CONCOURSE_AWS_SSM_SESSION_TOKEN.\n\n","depth":5,"section_tag":"configuration"},"aws-ssm-team-secret-template":{"location":"aws-ssm-credential-manager.html#aws-ssm-team-secret-template","title":"aws-ssm-team-secret-template","text":"The base path used when attempting to locate a team-level secret.\n\nEnvironment variable CONCOURSE_AWS_SSM_TEAM_SECRET_TEMPLATE.\n\nExample:\n\nDefault: /concourse/{{.Team}}/{{.Secret}}\n\n","depth":5,"section_tag":"configuration"},"base_resource_types-table":{"location":"database-schema.html#base_resource_types-table","title":"base_resource_types","text":"An entry in the base resource types table represents a resource type which can be provided by any worker. It is used to keep resource_configs as general as possible, and not tied to particular workers. This in turn allows resource_caches to also be general, as is desired for Concourse's higher-level pipeline resource caching semantics.\n\n","depth":5,"section_tag":"abstract-objects"},"basing-inputs-on-a-job-in-your-pipeline-with---inputs-from":{"location":"running-tasks.html#basing-inputs-on-a-job-in-your-pipeline-with---inputs-from","title":"Basing inputs on a job in your pipeline with --inputs-from","text":"If the --inputs-from flag is given, the specified job will be looked up in the pipeline, and the one-off build will base its inputs on those currently configured for the job.\n\nIf any --input flags are given (see above), they will override the base set of inputs.\n\nFor example:\n\n$ fly -t example execute \\\n    --config task.yml \\\n    --inputs-from main/integration \\\n    --input foo=./foo\nThis will trigger a one-off-build using the task.yml task config, basing its inputs on the latest candidates for the integration job in the main pipeline, with the foo input overridden to specify local code to run.\n\nThis can be used to more closely replicate the state in CI when weeding out flakiness, or as a shortcut for local development so that you don't have to upload every single resource from your local machine.\n\n","depth":5,"section_tag":"basing-inputs-on-a-job-in-your-pipeline-with---inputs-from"},"benefits-of-global-resources":{"location":"global-resources.html#benefits-of-global-resources","title":"Benefits of Global Resources","text":"","depth":4,"section_tag":"benefits-of-global-resources"},"binaries":{"location":"install.html","title":"Install","text":"A Concourse installation is composed of a Running a web node, a Running a worker node, and a Running a PostgreSQL node.\n\nThe web and worker nodes both run via a single concourse CLI. The concourse CLI is available from the Download page - be sure to grab the appropriate archive for your platform of choice.\n\nEach archive contains the following:\n\nconcourse/bin/concourse\nconcourse/bin/gdn            # on Linux\nconcourse/fly-assets/...\nconcourse/resource-types/... # on Linux\nWhen extracted, the concourse binary will auto-discover its sibling assets based on its file location, so you may extract it anywhere.\n\nOn Linux a typical install location is /usr/local/concourse:\n\ntar -zxf concourse-*.tgz -C /usr/local\nFrom there, you can either add /usr/local/concourse/bin to your $PATH, or just execute /usr/local/concourse/bin/concourse directly.\n\nConfiguration basics\n\nAll Concourse node configuration is defined statically via flags. For a full list of flags, you can pass --help to any command.\n\nEach flag can also be set via an environment variable. The env var for each flag is based on the flag name, preceded with CONCOURSE_. These are also shown in --help.\n\nVarious sections in documentation may refer to configuration via env vars rather than flags, but they are both equivalent an interchangeable. Env vars are just easier to reference in isolation and are more useful to copy-paste.\n\n","depth":2,"section_tag":"install"},"bitbucket-cloud-auth":{"location":"bitbucket-cloud-auth.html","title":"BitBucket Cloud auth","text":"A Concourse server can authenticate against BitBucket Cloud to leverage its permission model.\n\n","depth":4,"section_tag":"bitbucket-cloud-auth"},"bitbucket-cloud-authentication":{"location":"bitbucket-cloud-auth.html#bitbucket-cloud-authentication","title":"Authentication","text":"First, you'll need to create an OAuth consumer on Bitbucket Cloud.\n\nThe consumer will need the following permissions:\n\n* Account:\n\n  * Email\n\n  * Read\n\n* Team membership:\n\n  * Read\n\nThe \"Callback URL\" must be the URL of your Concourse server with /sky/issuer/callback appended. This address must be reachable by BitBucket Cloud - it can't be localhost.\n\nFor example, Concourse's own CI server's callback URL would be:\n\nhttps://ci.concourse-ci.org/sky/issuer/callback\nYou will be given a Client ID and a Client Secret for your new application. The client ID and secret must then be configured on the Running a web node by setting the following env:\n\nCONCOURSE_BITBUCKET_CLOUD_CLIENT_ID=myclientid\nCONCOURSE_BITBUCKET_CLOUD_CLIENT_SECRET=myclientsecret\n","depth":5,"section_tag":"bitbucket-cloud-authentication"},"bitbucket-cloud-authorization":{"location":"bitbucket-cloud-auth.html#bitbucket-cloud-authorization","title":"Authorization","text":"BitBucket users and teams can be authorized for a team by passing the following flags to fly set-team:\n\n--bitbucket-cloud-user=LOGIN: Authorize an individual user.\n\n\n--bitbucket-cloud-team=TEAM_NAME: Authorize an entire organization's members.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --bitbucket-cloud-user my-bitbucket-login \\\n    --bitbucket-cloud-team my-bitbucket-team\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  bitbucket:\n    users: [\"my-bitbucket-login\"]\n    teams: [\"my-bitbucket-team\"]\n","depth":5,"section_tag":"bitbucket-cloud-authorization"},"build finished":{"location":"metrics.html#build finished","title":"build finished","text":"This event is emitted when a build ends. Its value is the duration of the build in milliseconds. You can use this metric in conjunction with build started to annotate your metrics with when builds started and stopped.\n\nAttributes pipeline\n\n: The pipeline which contains the build that finished.\n\n\njob\n\n: The job which configured the build that finished.\n\n\nbuild_name\n\n: The name of the build that finished. (Remember that build numbers in Concourse are actually names and are strings).\n\n\nbuild_id\n\n: The ID of the build that finished.\n\n\nbuild_status\n\n: The resulting status of the build; one of \"succeeded\", \"failed\", \"errored\", or \"aborted\".\n\n\n\n\n\n","depth":4,"section_tag":"whats-emitted"},"build started":{"location":"metrics.html#build started","title":"build started","text":"This event is emitted when a build starts. Its value is the build ID of the build. However, it is most useful for annotating your metrics with the start and end of different jobs.\n\nAttributes pipeline\n\n: The pipeline which contains the build being started.\n\n\njob\n\n: The job which configured the build being started.\n\n\nbuild_name\n\n: The name of the build being started. (Remember that build numbers in Concourse are actually names and are strings).\n\n\nbuild_id\n\n: The ID of the build being started.\n\n\n\n\n\n","depth":4,"section_tag":"whats-emitted"},"build-plans":{"location":"steps.html","title":"Steps","text":"Each Job has a single build plan. When a build of a job is created, the plan determines what happens.\n\nA build plan is a sequence of steps to execute. These steps may fetch down or update Resources, or execute Tasks.\n\nA new build of the job is scheduled whenever get steps with trigger: true have new versions available.\n\nTo visualize the job in the pipeline, resources that appear as get steps are drawn as inputs, and resources that appear in put steps appear as outputs.\n\n","depth":3,"section_tag":"steps"},"build-retention":{"location":"caching-and-retention.html#build-retention","title":"Build Retention","text":"","depth":5,"section_tag":"build-retention"},"build_image_resource_caches-table":{"location":"database-schema.html#build_image_resource_caches-table","title":"build_image_resource_caches","text":"A build image resource cache is a join table between a build and a resource cache.\n\nA build image resource cache is used for keeping caches that were used for an image_resource in a build, as part of the resource retention policy.\n\n","depth":5,"section_tag":"abstract-objects"},"builds":{"location":"builds.html","title":"Builds","text":"A build is an execution of a build plan, which is either configured as a sequence of steps in a job, or submitted directly to Concourse as a one-off build via fly execute.\n\nContainers and volumes are created as get steps, put steps, and task steps run. When a build completes successfully, these containers go away.\n\nA failed build's containers and volumes are kept around so that you can debug the build via fly intercept. If the build belongs to a job, the containers will go away when the next build starts. If the build is a one-off, its containers will be removed immediately, so make sure you intercept while it's running if you want to debug.\n\n","depth":2,"section_tag":"builds"},"builds-table":{"location":"database-schema.html#builds-table","title":"builds","text":"The builds table tracks the details of every run of every job or one-off build, including its name, the status of the build, whether the build has been scheduled or completed, and information about the start and end times of the build.\n\n","depth":5,"section_tag":"pipelineapi-objects"},"cache-path":{"location":"tasks.html#cache-path","title":"caches.path","text":"Required. The path to a directory to be cached.\n\nPaths are relative to the working directory of the task. Absolute paths are not respected.\n\nNote that this value must not overlap with any other caches in the same task. Each cache results in a new empty directory that your task can place artifacts in; if the path overlaps it'll clobber whatever files used to be there.\n\n","depth":2,"section_tag":"tasks"},"caches":{"location":"tasks.html#caches","title":"caches","text":"Where cache is:\n\nOptional. The cached directories shared between task runs.\n\nOn the task's first run, all cache directories will be empty. It is the responsibility of the task to populate these directories with any artifacts to be cached. On subsequent runs, the cached directories will contain those artifacts.\n\nCaches are scoped to the worker the task is run on, so you will not get a cache hit when subsequent builds run on different workers. This also means that caching is not intended to share state between workers, and your task should be able to run whether or not the cache is warmed.\n\nCaches are also scoped to a particular task name inside of a pipeline's job. As a consequence, if the job name, step name or cache path are changed, the cache will not be used. This also means that caches do not exist for one-off builds.\n\n","depth":2,"section_tag":"tasks"},"caching-and-retention":{"location":"caching-and-retention.html","title":"Caching \u0026 Retention","text":"TODO:\n\n* How it relates to garbage-collection\n\n* How it relates to the schema\n\n* Resource cache retention policy\n\n* Container retention policy\n\n","depth":4,"section_tag":"caching-and-retention"},"cf-authentication":{"location":"cf-uaa-auth.html#cf-authentication","title":"Authentication","text":"You'll need to configure your UAA with a concourse client by setting the following under uaa.clients:\n\nconcourse:\n  id: myclientid\n  secret: myclientsecret\n  scope: openid,cloud_controller.read\n  authorized-grant-types: \"authorization_code,refresh_token\"\n  access-token-validity: 3600\n  refresh-token-validity: 3600\n  redirect-uri: https://concourse.example.com/sky/issuer/callback\nThe value for redirect-uri must be the external URL of your Concourse server with /sky/issuer/callback appended.\n\nFor example, Concourse's own CI server's callback URL would be:\n\nhttps://ci.concourse-ci.org/sky/issuer/callback\nNext, you'll need to take the same client ID and secret and configure it on the Running a web node by setting the following env:\n\nCONCOURSE_CF_API_URL=http://mycf.example.com\nCONCOURSE_CF_CLIENT_ID=myclientid\nCONCOURSE_CF_CLIENT_SECRET=myclientsecret\nNote: if you're integrating with Cloud Foundry, you're probably also deploying Concourse via BOSH - in which case you'll want to set the cf_auth.* properties in your manifest instead of setting the above env.\n\n","depth":5,"section_tag":"cf-authentication"},"cf-authorization":{"location":"cf-uaa-auth.html#cf-authorization","title":"Authorization","text":"CloudFoundry users and org/space members can be authorized for a team by passing the following flags to fly set-team:\n\n--cf-user=USERNAME: Authorize an individual user.\n\n\n--cf-org=ORG_NAME: Authorize an entire organization's members.\n\n\n--cf-space=ORG_NAME:SPACE_NAME: Authorize the members of a space within an organization.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --cf-user my-username \\\n    --cf-org my-org \\\n    --cf-space my-other-org:my-space\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  cf:\n    users: [\"my-username\"]\n    orgs: [\"my-org\"]\n    spaces: [\"my-other-org:my-space\"]\n","depth":5,"section_tag":"cf-authorization"},"cf-uaa-auth":{"location":"cf-uaa-auth.html","title":"CF/UAA auth","text":"Cloud Foundry (CF) auth can be used for operators who wish to authenticate their users configured against their Cloud Foundry instance via the UAA auth component.\n\n","depth":4,"section_tag":"cf-uaa-auth"},"community":{"location":"community.html","title":"Community","text":"* GitHub repo\n\n* Blog\n\n* Forums for support, announcements, and general discussion\n\n* Stack Overflow concourse tag\n\n* Discord to chat with other contributors.\n\n  If you're looking for help, we'd really appreciate it if you used the forums instead - threading and persistent messages make the support burden of an open-source project much easier to bear.\n\n","depth":1,"section_tag":"community"},"community-resources":{"location":"community.html#community-resources","title":"Community Resources","text":"These are third-party resources. Use at your own risk!\n\nAt some point we'd like to put together some sort of registry. We're not there yet though, so a list will have to do.\n\nBe sure to read the README file for a resource to know what it does before using it!\n\n| Resource Type Name | Maintained By |\n| Slack Reading and Posting | by @jleben |\n| Slack notifications | by @cloudfoundry-community |\n| Github Pull Requests | by @telia-oss |\n| GitLab Merge Requests | by @swisscom |\n| OpenStack Swift | by @sapcc |\n| Key Value resource | by @swce |\n| Flowdock notifications | by @starkandwayne |\n| Email | by @pivotal-cf |\n| Formatted Email | by @santoshjpawar |\n| Email with integrated MTA | by @mdomke |\n| Bintray | by @jamiemonserrate |\n| Perforce | by @olhtbr |\n| BOSH Errands | by @starkandwayne |\n| BOSH Config: cloud-config and runtime-config | by @EMC-Dojo |\n| BOSH Release | by @dpb587 |\n| Pool Trigger | by @sfmobile |\n| Pivotal Network | by @pivotal-cf-experimental |\n| FTP | by @aequitas |\n| lftp, access resources via ftp, http, sftp and fish | by @openSUSE |\n| Cloudformation | by @pivotal-cf-experimental |\n| Generic HTTP API | by @aequitas |\n| Hockey App | by @seadowg |\n| Concourse Pipelines | by @robdimsdale |\n| Twitter | by @ECSTeam |\n| HipChat Notifications | by @cloudfoundry-community |\n| Matrix Notifications | by @freelock |\n| Smuggler: generic resource framework | by @redfactorlabs |\n| GitHub Commit Status | by @dpb587 |\n| GitHub Deployment | by @ahume |\n| AMI Updates | by @jdub |\n| Debian/Ubuntu Sources Updates | by @jdub |\n| Open Build Service | by @SUSE |\n| Travis-ci | by @Orange-OpenSource |\n| Bitbucket Notifications | by @karunamon |\n| Terraform | by @ljfranklin |\n| bbl (BOSH Bootloader) state | by @cloudfoundry |\n| Azure Blobstore | by @pivotal-cf |\n| Rsync | by @mrsixw |\n| Rsync: rsyncd, for local net | by @chemist |\n| PyPI Packages | by @cf-platform-eng |\n| Devpi Server: PyPI mirror and package server | by @mdomke |\n| Jira Integration | by @danrspencer |\n| Google Drive | by @jpatel-pivotal |\n| Google Calendar | by @henrytk |\n| pagerduty Notifications | by @FidelityInternational |\n| Telegram | by @w32blaster |\n| Telegram | by @carlo-colombo |\n| Telegram | by @Cuttlerat |\n| Google Cloud Storage | by @frodenas |\n| Fly | by @troykinsella |\n| Kubernetes | by @jcderr |\n| Kubernetes | by @zlabjp |\n| K8s-Kubernetes | by @srinivasa-vasu |\n| Kubernetes Helm | by @linkyard |\n| Helm Chart resource | by @linkyard |\n| Helm Chart repository (ChartMuseum) resource | by @cathive |\n| Vault | by @Docurated |\n| AWS Key Management Service | by @everpeace |\n| Marathon | by @ckaznocha |\n| RSS | by @suhlig |\n| Maven Resource | by @nulldriver |\n| cURL File Resource | by @pivotalservices |\n| Pivnet Resource (rsamban) | by @rsamban |\n| Cloud Foundry Events | by @mevansam |\n| Apache Directory Index | by @mastertinner |\n| Cloud Foundry CLI Resource | by @nulldriver |\n| Counter | by @jinlee |\n| Capistrano | by @SHyx0rmZ |\n| aptly CLI | by @SHyx0rmZ |\n| Prometheus Alertmanager | by @frodenas |\n| Git Bitbucket Pull Request Resource | by @zarplata |\n| Bitbucket Pull Request Resource | by @halter-corp |\n| NPM Cache Resource | by @ymedlop |\n| Gerrit Resource | by @google |\n| Repo Resource | by @google |\n| Trigger jobs using Slack | by @ahelal |\n| Hashicorp Releases | by @starkandwayne |\n| Pushover notifications | by @redfactorlabs |\n| SonarQube static code analysis | by @cathive |\n| SonarQube Notifier | by @elgohr |\n| Metadata resource | by @swce |\n| Spinnaker Resource | by @pivotal-cf |\n| New Relic Deploy Resource | by @crstamps2 |\n| Artifactory Resource | by @spring-io |\n| Artifactory Resource | by @emerald-squad |\n| CF Zero Downtime Resource | by @emerald-squad |\n| FiaaS Resource | by @leboncoin |\n| IRC notifications | by @flavorjones |\n| PhraseApp Trigger | by @tenjaa |\n| Debian/Ubuntu archive uploads | by @seveas |\n| Launchpad PPA packages | by @seveas |\n| Fedora COPR packages | by @seveas |\n| Google Hangouts Notification Resource | by @CloudInn |\n| Yahoo! Weather Resource | by @Kehrlann |\n| Slack build alerts | by @arbourd |\n| Spring Initializr Resource | by @jghiloni |\n| HTTP Resource | by @aequitas |\n| Android SDK Resource | by @xaethos |\n| DigitalOcean Worker Provision Resource | by @CloudInn |\n| Knative Service Resource | by @jchesterpivotal |\n| Sentry Releases Resource | by @rubenv |\n| Gate Resource | by @Meshcloud |\n| Packer | by @Snapkitchen |\n| Github Webhook Resource | by @homedepot |\n| Ofcourse Resource Generator | by @cloudboss |\n| HTTP PUT Resource | by @lorands |\n| Serverspec Resource | by @opicaud |\n| Ansible Playbook | by @troykinsella |\n| Artifactory Deb | by @troykinsella |\n| Generic Artifactory | by @troykinsella |\n| Docker Compose | by @troykinsella |\n| RubyGems | by @troykinsella |\n| Medium Resource | by @cappyzawa |\n| GitHub List Repos | by @ari-becker |\n| Slack Notifier | by @mockersf |\n\n","depth":2,"section_tag":"community-resources"},"complications-with-reusing-containers":{"location":"global-resources.html#complications-with-reusing-containers","title":"Complications with reusing containers","text":"There is an exception to sharing check containers within a deployment, which is workers belonging to a team and workers with tags.\n\nIf a resource has tags configured, and the resource's check interval ends up acquiring the checking lock, a new container will be created on a worker matching the appropriate tags, even if a check container already exists for the same resource config elsewhere.\n\nSimilarly, if a team has its own workers, and their check interval ended up acquiring the lock, a new container will be created on the team's workers, rather than re-using a container from the shared worker pool.\n\nThis is a bit complicated to reason about and we plan to stop re-using check containers to simplify all of this. See concourse #3079 for more information.\n\n","depth":6,"section_tag":"complications-with-reusing-containers"},"component-atc":{"location":"architecture.html#component-atc","title":"ATC: web UI \u0026 build scheduler","text":"The ATC is the heart of Concourse. It runs the web UI and API and is responsible for all pipeline scheduling. It connects to PostgreSQL, which it uses to store pipeline data (including build logs).\n\nMultiple ATCs can be running as one cluster; as long as they're all pointing to the same database, they'll synchronize using basic locking mechanisms and roughly spread work across the cluster.\n\nThe ATC by default listens on port 8080, and is usually colocated with the TSA and sitting behind a load balancer.\n\nNote: for fly intercept to function, make sure your load balancer is configured to do TCP or SSL forwarding, not HTTP or HTTPS.\n\n","depth":3,"section_tag":"component-atc"},"component-baggageclaim":{"location":"architecture.html#component-baggageclaim","title":"BaggageClaim: volume management","text":"","depth":4,"section_tag":"component-baggageclaim"},"component-garden":{"location":"architecture.html#component-garden","title":"Garden: container orchestration","text":"","depth":4,"section_tag":"component-garden"},"component-tsa":{"location":"architecture.html#component-tsa","title":"TSA: worker registration \u0026 forwarding","text":"The TSA is a custom-built SSH server that is used solely for securely registering workers with the ATC.\n\nThe TSA only supports two commands: register-worker and forward-worker.\n\nThe register-worker command is used to register a worker directly with the ATC. This should be used if the worker is running in the same (private) network as the ATC.\n\nThe forward-worker command is used to reverse-tunnel a worker's addresses through the TSA and register the forwarded connections with the ATC. This allows workers running in arbitrary networks to register securely, so long as they can reach the TSA. This is much safer than opening the worker up to the outside world.\n\nThe TSA by default listens on port 2222, and is usually colocated with the ATC and sitting behind a load balancer.\n\n","depth":3,"section_tag":"component-tsa"},"concourse-admin":{"location":"user-roles.html#concourse-admin","title":"Concourse Admin","text":"Admin is a special user attribute granted only to owners of the The main team.\n\nAdmins have the ability to administrate teams using fly set-team, fly destroy-team, fly rename-team, etc.\n\n","depth":4,"section_tag":"concourse-admin"},"concourse-cli":{"location":"install.html","title":"Install","text":"A Concourse installation is composed of a Running a web node, a Running a worker node, and a Running a PostgreSQL node.\n\nThe web and worker nodes both run via a single concourse CLI. The concourse CLI is available from the Download page - be sure to grab the appropriate archive for your platform of choice.\n\nEach archive contains the following:\n\nconcourse/bin/concourse\nconcourse/bin/gdn            # on Linux\nconcourse/fly-assets/...\nconcourse/resource-types/... # on Linux\nWhen extracted, the concourse binary will auto-discover its sibling assets based on its file location, so you may extract it anywhere.\n\nOn Linux a typical install location is /usr/local/concourse:\n\ntar -zxf concourse-*.tgz -C /usr/local\nFrom there, you can either add /usr/local/concourse/bin to your $PATH, or just execute /usr/local/concourse/bin/concourse directly.\n\nConfiguration basics\n\nAll Concourse node configuration is defined statically via flags. For a full list of flags, you can pass --help to any command.\n\nEach flag can also be set via an environment variable. The env var for each flag is based on the flag name, preceded with CONCOURSE_. These are also shown in --help.\n\nVarious sections in documentation may refer to configuration via env vars rather than flags, but they are both equivalent an interchangeable. Env vars are just easier to reference in isolation and are more useful to copy-paste.\n\n","depth":2,"section_tag":"install"},"concourse-generate-key":{"location":"concourse-generate-key.html","title":"Generating Keys","text":"Concourse's various components use RSA keys to verify tokens and worker registration requests.\n\nA minimal deployment will require the following keys:\n\nsession_signing_key: Used by the Running a web node for signing and verifying user session tokens.\n\n\ntsa_host_key: Used by the Running a web node for the SSH worker registration gateway server (\"TSA\").\n\nThe public key is given to each Running a worker node to verify the remote host when connecting via SSH.\n\n\nworker_key (one per worker): Each Running a worker node verifies its registration with the Running a web node via a SSH key.\n\nThe public key must be listed in the Running a web node's authorized keys configuration in order for the worker to register.\n\n\n\nTo generate these keys, run:\n\nconcourse generate-key -t rsa -f ./session_signing_key\nconcourse generate-key -t ssh -f ./tsa_host_key\nconcourse generate-key -t ssh -f ./worker_key\n...and we'll also start on an authorized_keys file, currently listing this initial worker key:\n\ncp worker_key.pub authorized_worker_keys\n","depth":3,"section_tag":"concourse-generate-key"},"concourse-web":{"location":"concourse-web.html","title":"Running a web node","text":"The web node is responsible for running the web UI, API, and as well as performing all pipeline scheduling. It's basically the brain of Concourse.\n\n","depth":3,"section_tag":"concourse-web"},"concourse-worker":{"location":"concourse-worker.html","title":"Running a worker node","text":"The worker node registers with the Running a web node and is then used for executing builds and performing resource checks. It doesn't really decide much on its own.\n\n","depth":3,"section_tag":"concourse-worker"},"configuration":{"location":"php-example.html#configuration","title":"Pipeline Configuration","text":"---\nresources:\n  - name: larvel-websockets-git\n    type: git\n    source:\n      uri: https://github.com/beyondcode/laravel-websockets.git\n\njobs:\n  - name: test\n    public: true\n    plan:\n      - get: larvel-websockets-git\n        trigger: true\n      - task: run-tests\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source: { repository: php, tag: 7.2-cli }\n          inputs:\n            - name: larvel-websockets-git\n          run:\n            path: /bin/sh\n            args:\n              - -c\n              - |\n                apt-get update\n                apt-get install -y git unzip\n\n                php -r \"copy('https://getcomposer.org/installer', 'composer-setup.php');\"\n                php -r \"if (hash_file('sha384', 'composer-setup.php') === '93b54496392c062774670ac18b134c3b3a95e5a5e5c8f1a9f115f203b75bf9a129d5daa8ba6a13e2cc8a1da0806388a8') { echo 'Installer verified'; } else { echo 'Installer corrupt'; unlink('composer-setup.php'); } echo PHP_EOL;\"\n                php composer-setup.php --filename=composer --install-dir=/usr/bin\n                php -r \"unlink('composer-setup.php');\"\n\n                cd larvel-websockets-git\n\n                composer install\n                vendor/bin/phpunit --coverage-text --coverage-clover=coverage.clover\n","depth":4,"section_tag":"configuration"},"configuring-auth":{"location":"configuring-auth.html","title":"Configuring Auth","text":"The very first thing to configure with Concourse is how users will log in, and what those users should be able to do.\n\nThis is configured in two separate tiers:\n\n* Authentication, how users identify themselves, is configured on the Running a web node.\n\n* Authorization, how user access is determined, is configured on each team.\n\nConcourse currently supports the following auth methods:\n\nAny number of providers can be enabled at any one time. Users will be given a choice when logging in as to which one they would like to use.\n\nConcourse uses a fork of Dex for its authentication. You can find additional documentation on the supported auth providers in the Dex connectors documentation.\n\nAdding a new auth provider to Concourse is as simple as submitting a pull request to our fork of Dex and then adding a bit of configuration to the skymarshal component.\n\n","depth":3,"section_tag":"configuring-auth"},"configuring-credential-caching":{"location":"vault-credential-manager.html#configuring-credential-caching","title":"Configuring credential caching","text":"By default, credentials are fetched each time they're used. This can add up when many pipelines are configured, resulting in a ton of requests to Vault.\n\nTo reduce load on your Vault server you may want to enable caching, by setting the following env on the Running a web node:\n\nCONCOURSE_VAULT_CACHE=true\nWhen enabled, credentials are cached for half of their lease duration.\n\nTo set an upper bound and force cache busting after a certain amount of time, set the following env:\n\nCONCOURSE_VAULT_MAX_LEASE=1m\nWith this set, credentials will only be cached for up to 1 minute.\n\n","depth":5,"section_tag":"configuring-credential-caching"},"configuring-ldap-group-search":{"location":"ldap-auth.html#configuring-ldap-group-search","title":"Configuring LDAP group search","text":"The LDAP provider can also be configured with group search configuration, so that users can be configured for team authorization by their 'group' in LDAP.\n\nFor example, to find groups and identify them by their ou attribute, you would configure:\n\nCONCOURSE_LDAP_GROUP_SEARCH_BASE_DN='cn=groups,dc=example,dc=com'\nCONCOURSE_LDAP_GROUP_SEARCH_NAME_ATTR=ou\nThe attributes correlating a user to a group must be specified like so:\n\nCONCOURSE_LDAP_GROUP_SEARCH_USER_ATTR=uid\nCONCOURSE_LDAP_GROUP_SEARCH_GROUP_ATTR=members\nThis specifies that the uid attribute of the user must be present in the members attribute of the group.\n\nAn additional filter may be specified, just like with users:\n\nCONCOURSE_LDAP_GROUP_SEARCH_FILTER='(objectClass=posixGroup)'\n","depth":6,"section_tag":"configuring-ldap-group-search"},"configuring-main-team-authorization":{"location":"generic-oauth.html#configuring-main-team-authorization","title":"Configuring main Team Authorization","text":"OAuth users and groups can be added to the The main team authorization config by setting the following env on the Running a web node:\n\nCONCOURSE_MAIN_TEAM_OAUTH_USER=my-user\nCONCOURSE_MAIN_TEAM_OAUTH_GROUP=my-group\nMultiple users and groups may be specified by comma-separating them.\n\n","depth":6,"section_tag":"configuring-main-team-authorization"},"configuring-metrics":{"location":"metrics.html#configuring-metrics","title":"Configuring Metrics","text":"The ATC: web UI \u0026 build scheduler can be configured to emit metrics on start.\n\nCurrently supported metrics emitters are InfluxDB, NewRelic, Prometheus, Datadog, and Riemann. There is also a dummy emitter that will just spit the metrics out in to the logs at DEBUG level, which can be enabled with the --emit-to-logs flag.\n\nThere are various flags for different emitters; run concourse web --help and look for \"Metric Emitter\" to see what's available.\n\n","depth":4,"section_tag":"configuring-metrics"},"configuring-the-secrets-engine":{"location":"vault-credential-manager.html#configuring-the-secrets-engine","title":"Configuring the secrets engine","text":"Concourse is currently limited to looking under a single path, meaning only one secrets engine is supported: kv, version 1. This may change in the future - we're still collecting ideas in (RF)RFC #5.\n\nSo, let's configure the kv secrets engine and mount it at /concourse:\n\n$ vault secrets enable -version=1 -path=concourse kv\nNext, you'll want to create a policy to allow Concourse to read from this path.\n\npath \"concourse/*\" {\n  policy = \"read\"\n}\nSave this to concourse-policy.hcl, and then run:\n\nvault policy write concourse ./concourse-policy.hcl\nThis configuration will allow Concourse to read all credentials under /concourse. This should match your configured path prefix.\n\n","depth":5,"section_tag":"configuring-the-secrets-engine"},"container-collection":{"location":"garbage-collection.html#container-collection","title":"Container Collection","text":"First, a fairly simple query is executed to find containers that meet one of the following conditions:\n\n* If it has a NULL reference for all four dependent columns:\n\n  * containers (build_id)\n\n  * containers (image_check_container_id)\n\n  * containers (image_get_container_id)\n\n  * containers (worker_resource_config_check_session_id)\n\n  This is the simplest case: the things that needed the container are now gone, so it can go away.\n\n* The containers (build_id) referenced by the container is no longer interceptible. See Build Retention.\n\n* The containers (image_check_container_id) or containers (image_get_container_id) referenced by the container is no longer in CREATING state (likely CREATED).\n\nOnce these containers are found, they are all deleted in parallel, with a max-in-flight limit per worker so that the worker doesn't get hammered by a burst of writes.\n\nThe deletion of every container is a careful process to ensure they never leak and are never deleted while a user is hijacked into them:\n\n* If the container is CREATING, we mark it CREATED. This is a bit wonky but makes it easier to just step it through the rest of the lifecycle, since if there was a container being created on the worker, we need to clean it up.\n\n* If the container is CREATED, we first check to see if it was hijacked. If not, we transition it to DESTROYING.\n\n  If the container is hijacked, we try to find the container in the worker.\n\n  If the worker container is found, we set a grace time on it (a period of inactivity after which the container will be reaped by the worker itself), mark the database container as discontinued, and transition the container to DESTROYING.\n\n  If the worker container is not found, we transition the container to DESTROYING, just to funnel it down the same code path as below.\n\n* If the container is DESTROYING, and the container is discontinued, we check if the container has expired yet (via the grace time) by looking for it on the worker. If it's still there, we leave it alone, and leave the container in the database. If it's gone, we reap the container from the database.\n\n  If the container is not discontinued, we destroy the container on the worker and reap the container from the database.\n\nNote that if any point of the above process fails, the container is left in its current state in the database. A container is only ever removed from the database when it's guaranteed that everything has been cleaned up.\n\n","depth":6,"section_tag":"container-collection"},"container-internals":{"location":"container-internals.html","title":"Containers","text":"","depth":4,"section_tag":"container-internals"},"container-lifecycle":{"location":"container-internals.html#container-lifecycle","title":"Lifecycle","text":"Containers can be in one of 3 states; CREATING, CREATED, DESTROYING.\n\nCREATING containers are still being initialized on the worker and are not yet ready to be used. CREATING, containers can only transition to CREATED.\n\nCREATED containers are initialized on the worker and are ready to be used. A CREATED container can only be transitioned to DESTROYING.\n\nDESTROYING containers are marked for removal on the worker, and should no longer be used; they will be removed from the database when they no longer exist on the worker.\n\n","depth":5,"section_tag":"container-lifecycle"},"container-placement":{"location":"container-placement.html","title":"Container Placement","text":"Each step in a build is executed inside a container. The Running a web node distributes containers across the worker cluster depending on the configured strategy.\n\n","depth":3,"section_tag":"container-placement"},"containers-build_id":{"location":"database-schema.html#containers-build_id","title":"build_id","text":"If this container is for a build step, this column refers to the builds (id) this container is related to.\n\n","depth":5,"section_tag":"runtime"},"containers-handle":{"location":"database-schema.html#containers-handle","title":"handle","text":"The unique identifier of the container in Garden.\n\n","depth":5,"section_tag":"runtime"},"containers-image_check_container_id":{"location":"database-schema.html#containers-image_check_container_id","title":"image_check_container_id","text":"This signifies that the container is dependant on another container which is busy checking for the image this container will be based on. This is used in the case of custom resource types, or tasks with image_resource.  This container will be in the CREATING state until the image is fetched later.\n\n","depth":5,"section_tag":"runtime"},"containers-image_get_container_id":{"location":"database-schema.html#containers-image_get_container_id","title":"image_get_container_id","text":"This signifies that the container is dependant on another container which is busy downloading the bits for the image this container will be based on. This is used in the case of custom resource types, or tasks with image_resource. This container will be in the CREATING state until the image is fetched.\n\n","depth":5,"section_tag":"runtime"},"containers-resource_id":{"location":"database-schema.html#containers-resource_id","title":"worker_resource_config_check_session_id","text":"If this container is for running the check script for a resource, this column refers to a worker_resource_config_check_sessions (id).\n\n","depth":5,"section_tag":"runtime"},"containers-state":{"location":"database-schema.html#containers-state","title":"state","text":"The stage in the lifecycle of the container. See Lifecycle\n\n","depth":5,"section_tag":"runtime"},"containers-table":{"location":"database-schema.html#containers-table","title":"containers","text":"The containers table represents the set of all containers across all the workers, and keeps track of their state such that no container is ever left behind on a worker.\n\nContainers have a handful of really important attributes:\n\nContainers can be one of four types, each with individual references to their relating object.\n\n","depth":5,"section_tag":"runtime"},"containers-worker_name":{"location":"database-schema.html#containers-worker_name","title":"worker_name","text":"The name of the worker where the container is located.\n\n","depth":5,"section_tag":"runtime"},"contribute":{"location":"contribute.html","title":"Contribute","text":"Concourse is a free and Open Source software project that relies on the contributions of sponsors and volunteers from around the world. As a growing community of continuous thing-doers, the team is always in need of more people to help out in our community.\n\nEven if you're just getting started with Concourse you can contribute in a few ways:\n\n* Discuss and Share your experiences with Concourse in our Discord forums\n\n* Blog about Concourse. We would love to hear how you got started with Concourse, the type of pipelines you're building, or any tips \u0026 tricks that you can share with the community.\n\n* Answer questions about Concourse in the Support section of our forums or on Stack Overflow\n\nAs you become more comfortable with Concourse, you might be interested in contributing to Concourse's development: * Review issues and report bugs in the concourse/concourse repo. We are by no means experts in every subject area; it sometimes takes us while to understand a problem space well enough to figure out things fit into Concourse's puritanical world. You can help us by: * Voting for issues by adding an emoji reaction to the issue\n\n  * Submitting new bugs when you run into a new problem with Concourse\n\n  * Review GitHub issues submitted by other members of the community; ask for (or provide!) clarification when necessary. A lot of the issues that come in are simply unclear and we end up spending a lot of time on clarifying issues.\n\n* Writing Documentation for the project. You can get started by reviewing the docs code at concourse/docs and making PRs against the project.\n\n* Contributing Code. If you're interested in contributing some work to the core project, you can get started by reviewing the CONTRIBUTING.md getting started guide. You can also get an overview of the Concourse architecture by reviewing some of the documentation under Concourse Architecture. If you're curious about what we're working on you can follow along with the team's progress on our public projects page.\n\n\n\nThe Concourse project is committed to fostering an open and welcoming environment for all project members, maintainers and community members. The Concourse project pledges to make our community  a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation. You can read more about our commitment in the project's Contributor Code of Conduct\n\n","depth":2,"section_tag":"contribute"},"credential-lookup-rules":{"location":"aws-asm-credential-manager.html#credential-lookup-rules","title":"Credential Lookup Rules","text":"When resolving a parameter such as ((foo_param)), Concourse will look in the following paths, in order:\n\n* /concourse/TEAM_NAME/PIPELINE_NAME/foo_param\n\n* /concourse/TEAM_NAME/foo_param\n\nThe leading /concourse can be changed by specifying --aws-secretsmanager-pipeline-secret-template or --aws-secretsmanager-team-secret-template variables.\n\n","depth":5,"section_tag":"credential-lookup-rules"},"credhub-credential-manager":{"location":"credhub-credential-manager.html","title":"The CredHub credential manager","text":"","depth":4,"section_tag":"credhub-credential-manager"},"creds":{"location":"creds.html","title":"Credential Management","text":"Going beyond Encryption, explicit credential management will provide credentials to your builds for a brief amount of time, without being persisted anywhere. It also allows for credentials to be rotated and managed external to the pipeline or team, and prevents them from being revealed by fly get-pipeline.\n\nCredential management works by replacing the credentials with ((vars)) in your pipeline or task config. When the Concourse is about to run the step or check that is configured with vars, it will resolve them by fetching the values from the credential manager. If the values are not present, the action will error.\n\nThe following configurations can be parameterized with a credential manager:\n\n* source under Resources in a pipeline\n\n* source under Resource Types in a pipeline\n\n* webhook_token under Resources in a pipeline\n\n* params on a task step in a pipeline\n\n* Tasks in their entirety - whether from file or config in a pipeline, or a config executed with fly execute\n\nWhere these values are looked up and how the credential manager is configured depends on the backend. Consult the relevant section below for whichever backend you want to use.\n\nConcourse currently supports the following credential managers:\n\nCommon Configuration Parameters\n\nWhen a request to the credential manager fails due to an intermittent error (e.g. a timeout or connection refused), Concourse will automatically try the request again up to 5 times before giving up. After all attempts fail, the error will be surfaced in the UI for the resource check or build step that initiated the request.\n\nThe retry logic can be configured by specifying the following env on the Running a web node:\n\nCONCOURSE_SECRET_RETRY_ATTEMPTS=5   # how many times to try\nCONCOURSE_SECRET_RETRY_INTERVAL=10s # how long to wait between attempts\n","depth":3,"section_tag":"creds"},"database-schema":{"location":"database-schema.html","title":"Database Schema","text":"","depth":4,"section_tag":"database-schema"},"db-prerequisites":{"location":"postgresql-node.html#db-prerequisites","title":"Prerequisites","text":"PostgreSQL 9.5 or above is required, though the latest available version is recommended.\n\n","depth":4,"section_tag":"db-prerequisites"},"db-properties":{"location":"postgresql-node.html#db-properties","title":"Properties","text":"CPU usage: this is one of the most volatile metrics, and one we try pretty hard to keep down. There will be near-constant database queries running, and while we try to keep them very simple, there is always more work to do. Expect to feed your database with at least a couple cores, ideally four to eight. Monitor this closely as the size of your deployment and the amount of traffic it's handling increases, and scale accordingly.\n\nMemory usage: similar to CPU usage, but not quite as volatile.\n\nDisk usage: pipeline configurations and various bookkeeping metadata for keeping track of jobs, builds, resources, containers, and volumes. In addition, all build logs are stored in the database. This is the primary source of disk usage. To mitigate this, users can configure build_logs_to_retain on a job, but currently there is no operator control for this setting. As a result, disk usage on the database can grow arbitrarily large.\n\nBandwidth usage: well, it's a database, so it most definitely uses the network (duh). Not much should stand out here, though build logs can result in an arbitrary amount of data being sent over the network to the database. This should be nothing compared to worker bandwidth, though.\n\nHighly available: up to you. Clustered PostgreSQL is kind of new and probably tricky to deploy, but there are various cloud solutions for this.\n\nHorizontally scalable: I...don't think so?\n\nOutbound traffic:\n\n* none\n\nInbound traffic:\n\n* only ever from the web node\n\n","depth":4,"section_tag":"db-properties"},"db-running":{"location":"postgresql-node.html#db-running","title":"Running","text":"How this node is managed is up to you; Concourse doesn't actually have much of an opinion on it, it just needs a database.\n\nHow to install PostgreSQL is really dependent on your platform. Please refer to your Linux distribution or operating system's documentation.\n\nFor the most part, the instruction on Linux should look something like this:\n\nsudo apt install postgresql\nsudo su postgres -c \"createuser $(whoami)\"\nsudo su postgres -c \"createdb --owner=$(whoami) atc\"\nThis will install PostgreSQL (assuming your distro uses apt), create a user, and create a database that the current UNIX user can access, assuming this same user is going to be running the Running a web node. This is a reasonable default for distros like Ubuntu and Debian which default PostgreSQL to peer auth.\n\n","depth":4,"section_tag":"db-running"},"development":{"location":"learning.html#development","title":"Development","text":"Developing a Custom Concourse Resource by @BrianMMcClain\n\n","depth":3,"section_tag":"development"},"disabling-encryption":{"location":"encryption.html#disabling-encryption","title":"Disabling Encryption","text":"To opt out of encryption entirely (I'm sure you have your reasons), simply pass --old-encryption-key (or old_encryption_key) alone. With no new encryption key, the ATC: web UI \u0026 build scheduler will decrypt all existing data on start.\n\n","depth":4,"section_tag":"disabling-encryption"},"do":{"location":"do-step.html#do","title":"do","text":"Simply performs the given steps serially, with the same semantics as if they were at the top level step listing.\n\n","depth":4,"section_tag":"do-step"},"do-step":{"location":"do-step.html","title":"do step","text":"","depth":4,"section_tag":"do-step"},"docs":{"location":"docs.html","title":"Docs","text":"Concourse is a pipeline-based continuous thing-doer.\n\nThe word \"pipeline\" is all the rage in CI these days, so being more specific about this term is kind of important; Concourse's pipelines are significantly different from the rest.\n\nPipelines are built around Resources, which represent all external state, and Jobs, which interact with them. Concourse pipelines represent a dependency flow, kind of like distributed Makefiles. Pipelines are designed to be self-contained so as to minimize server-wide configuration. Maximizing portability also mitigates risk, making it easier for projects to recover from CI disasters.\n\nResources like the git resource and s3 resource are used to express source code, dependencies, deployments, and any other external state. This interface is also used to model more abstract things like scheduled or interval triggers, via the time resource.\n\nResource Types are defined as part of the pipeline itself, making the pipelines more self-contained and keeping Concourse itself small and generic without resorting to a complicated plugin system.\n\nJobs are sequences get, put, and task steps to execute. These steps determine the job's inputs and outputs. Jobs are designed to be idempotent and loosely coupled, allowing the pipeline to grow with the project's needs without requiring engineers to keep too much in their head at a time.\n\nEverything in Concourse runs in a container. Instead of modifying workers to install build tools, Tasks describe their own container image (typically using Docker images via the registry-image resource).\n\n...What?\n\nConcourse admittedly has a steeper learning curve at first, and depending on your background it might be a lot to take in. A core goal of this project is for the curve to flatten out shortly after and result in higher productivity and less stress over time.\n\nIf this all sounds like gobbeldigook, that's OK - you may want to just continue on, start kicking the tires a bit, and use the above as a quick reference of the \"big picture\" as the mental model sets in.\n\n","depth":1,"section_tag":"docs"},"downgrading":{"location":"concourse-web.html#downgrading","title":"Downgrading","text":"If you're stuck in a pinch and need to downgrade from one version of Concourse to another, you can use the concourse migrate command.\n\nNote: support for down migrations is a fairly recent addition to Concourse; it is not supported for downgrading to v3.6.0 and below.\n\nFirst, grab the desired migration version by running the following:\n\n# make sure this is the *old* Concourse binary\n$ concourse migrate --supported-db-version\n1551110547\nThat number (yours will be different) is the expected migration version for that version of Concourse.\n\nNext, run the following with the new Concourse binary:\n\n$ concourse migrate --migrate-db-to-version=1551110547\nThis will need the same CONCOURSE_POSTGRES_* configuration described in Running.\n\nOnce this completes, switch all web nodes back to the older concourse binary and you should be good to go.\n\n","depth":5,"section_tag":"downgrading"},"download":{"location":"download.html","title":"Download","text":"","depth":1,"section_tag":"download"},"downloads":{"location":"download.html","title":"Download","text":"","depth":1,"section_tag":"download"},"enabling-encryption":{"location":"encryption.html#enabling-encryption","title":"Enabling Encryption","text":"To enable encryption, you'll just need to come up with a 16 or 32-byte random character sequence and configure it as --encryption-key flag to the web command. For BOSH, this is the encryption_key property.\n\nOn startup, the ATC: web UI \u0026 build scheduler will encrypt all existing plaintext data, and any new data being written will be encrypted before it's sent over the network to the database.\n\nThe initial bulk encryption shouldn't take too long, but it will scale linearly with the amount of data that you have, and if another ATC is running it'll suddenly not be able to read the data until it's also given the key. So, expect some downtime.\n\n","depth":4,"section_tag":"enabling-encryption"},"encryption":{"location":"encryption.html","title":"Encryption","text":"Automating everything means authorizing something to automate many things. This makes CI systems a high-risk target for security leaks.\n\nConcourse pipelines are loaded with credentials: resources are configured with private keys, tasks are given credentials to servers they integrate via credential manager variables, vars, or params, etc. If someone gets their hands on your config, they have access to everything.\n\nTo mitigate this, Concourse supports encrypting sensitive information before it reaches the database. This way the plaintext credentials only exist in memory for as long as they need to, and if someone gains access to your database, they can't so easily gain the keys to the kingdom.\n\nWe strongly encourage anyone running Concourse to configure encryption. Going further, it's best to have Concourse not store the credentials in the first place, in which case you may want to configure credential management as well.\n\n","depth":3,"section_tag":"encryption"},"ensure":{"location":"ensure-step-hook.html#ensure","title":"ensure","text":"The step to execute. Regardless of whether the parent step succeeds, fails, or errors, this step will be executed. The step will also be executed if the build was aborted, and its parent step was interrupted.\n\nIf the parent step succeeds and the ensured step fails, the parent step is considered to have failed.\n\nThe ensured step executes after any on_success step hooks or on_failure step hooks.\n\n","depth":4,"section_tag":"ensure-step-hook"},"ensure-step-hook":{"location":"ensure-step-hook.html","title":"ensure step hook","text":"Any step can have ensure tacked onto it, whose value is a second step to execute regardless of the result of the parent step.\n\n","depth":4,"section_tag":"ensure-step-hook"},"examples":{"location":"learning.html#examples","title":"Examples","text":"","depth":2,"section_tag":"examples"},"exposing":{"location":"exposing.html","title":"Pipeline \u0026 Build Visibility","text":"Every newly configured pipeline is hidden to anyone but the pipeline's team. To make a pipeline publicly viewable, both by other teams and unauthenticated users, see fly expose-pipeline.\n\nEven with a pipeline exposed, all build logs are hidden by default. This is because CI jobs are prone to leaking credentials and other...unsavory information. After you've determined that a job's builds should be safe for public consumption, you can set public: true on the job in your pipeline.\n\n","depth":3,"section_tag":"exposing"},"fewer-resource-checks-to-perform":{"location":"global-resources.html#fewer-resource-checks-to-perform","title":"Fewer resource checks to perform","text":"With global resources, all resources that have the same configuration will share the same version history and share only one checking interval. This reduces load on the worker and on the external services that the resources point to.\n\nFor example, prior to global resources if there were three resources with the same configuration between three team's pipelines it would result in three check containers performing three resource checks every minute to fetch the versions.\n\nWith global resources, this configuration will result in only one check container and one resource check every minute to fetch versions for all the resources.\n\nSince there will be only one resource check for all resources that have the same configuration, the resource that has the shortest check_every configured will result in its pipeline running the checks for that resource configuration.\n\n","depth":5,"section_tag":"fewer-resource-checks-to-perform"},"fewest-build-containers-strategy":{"location":"container-placement.html#fewest-build-containers-strategy","title":"The fewest-build-containers strategy","text":"When using the fewest-build-containers strategy, step containers are placed on the worker that has the fewest build containers (i.e. containers for other steps of other builds).\n\nTo use this strategy, set the following env var on the Running a web node:\n\nCONCOURSE_CONTAINER_PLACEMENT_STRATEGY=fewest-build-containers\n","depth":4,"section_tag":"fewest-build-containers-strategy"},"fly":{"location":"fly.html","title":"The fly CLI","text":"Concourse is primarily driven from the command-line; there is no GUI config wizard.\n\nSo, the first step to getting started with Concourse is to install the fly CLI tool. You can download fly from any Concourse installation. There are download links for common platforms in the bottom right hand corner of the main page.\n\nThroughout the Concourse documentation we'll stick to the long-form name of every command and flag. Once you've learned what the commands do, you may want to consult fly -h to learn the short forms.\n\n","depth":2,"section_tag":"fly"},"fly-abort-build":{"location":"builds.html#fly-abort-build","title":"fly abort-build","text":"To abort a build of a job, run:\n\n$ fly -t example abort-build --job my-pipeline/my-job --build 3\nThis will cancel build 3 of the my-job job in the my-pipeline pipeline.\n\n","depth":3,"section_tag":"fly-abort-build"},"fly-builds":{"location":"builds.html#fly-builds","title":"fly builds","text":"To list the most recent builds, run:\n\n$ fly -t example builds\nTo list the builds of a job, run:\n\n$ fly -t example builds -j pipeline-name/job-name\nThis can be useful for periodically monitoring the state of a job. The output also works well with tools like awk and grep.\n\nBy default the most recent 50 builds are shown. To see more builds, use the -c flag, like so:\n\n$ fly -t example builds -c 100\n","depth":3,"section_tag":"fly-builds"},"fly-check-resource":{"location":"managing-resources.html#fly-check-resource","title":"fly check-resource","text":"To force immediate checking for new versions of a resource, rather than waiting for the periodic checking, run:\n\n$ fly -t example check-resource --resource my-pipeline/my-resource\nTo check from a particular version, including the given version, append the --from flag like so:\n\n$ fly -t example check-resource --resource my-pipeline/my-resource \\\n    --from ref:abcdef\nThis can be useful for collecting versions that are older than the current ones, given that a newly configured resource will only start from the latest version.\n\nNote the ref: prefix is resource-dependent. For example, the bosh-io-release resource might use version:11.2 in place of ref:abcdef.\n\n","depth":4,"section_tag":"fly-check-resource"},"fly-check-resource-type":{"location":"managing-resource-types.html#fly-check-resource-type","title":"fly check-resource-type","text":"To force immediate checking for new versions of a resource type, rather than waiting for the periodic checking, run:\n\n$ fly -t example check-resource-type --resource-type my-pipeline/my-resource-type\nThis can be useful for forcing an update if you're iterating on your own resource type implementation.\n\n","depth":5,"section_tag":"fly-check-resource-type"},"fly-clear-task-cache":{"location":"managing-jobs.html#fly-clear-task-cache","title":"fly clear-task-cache","text":"If you've got a task cache that you need to clear out for whatever reason, this can be done like so:\n\n$ fly -t example clear-task-cache --job my-pipeline/my-job --step my-step-name\nThis will immediately invalidate the caches - they'll be garbage collected asynchronously and subsequent builds will run with empty caches.\n\nYou can also clear out a particular path for the given step's cache, using --cache-path:\n\n$ fly -t example clear-task-cache \\\n    --job my-pipeline/my-job \\\n    --step my-step-name \\\n    --cache-path go/pkg\nIf --cache-path is not specified, all caches for the given step will be cleared.\n\n","depth":4,"section_tag":"fly-clear-task-cache"},"fly-cli":{"location":"fly.html","title":"The fly CLI","text":"Concourse is primarily driven from the command-line; there is no GUI config wizard.\n\nSo, the first step to getting started with Concourse is to install the fly CLI tool. You can download fly from any Concourse installation. There are download links for common platforms in the bottom right hand corner of the main page.\n\nThroughout the Concourse documentation we'll stick to the long-form name of every command and flag. Once you've learned what the commands do, you may want to consult fly -h to learn the short forms.\n\n","depth":2,"section_tag":"fly"},"fly-containers":{"location":"administration.html#fly-containers","title":"fly containers","text":"To list the active containers across all your workers, run:\n\n$ fly -t example containers\nThis can be useful when discovering the containers available for fly intercepting.\n\n","depth":4,"section_tag":"fly-containers"},"fly-curl":{"location":"administration.html#fly-curl","title":"fly curl","text":"To execute an arbirary API request, you can run something like the following:\n\n$ fly -t example curl /api/v1/info\nThis command is just a shim that runs curl under the hood. To pass flags to curl, pass a -- argument after the path so that fly can distinguish them from its own flags:\n\n$ fly -t example curl /api/v1/builds -- \\\n    -X PUT \\\n    -H \"Content-type: application/json\" \\\n    -d @plan.json\nNote: if you use this command the assumption is that you know what you're doing. If you find yourself using this command often, let us know - perhaps there's a missing command!\n\n","depth":4,"section_tag":"fly-curl"},"fly-destroy-pipeline":{"location":"managing-pipelines.html#fly-destroy-pipeline","title":"fly destroy-pipeline","text":"Every now and then you just don't want a pipeline to be around anymore. Running fly destroy-pipeline will stop the pipeline activity and remove all data collected by the pipeline, including build history and collected versions.\n\nFor example, to destroy the my-pipeline pipeline, you would run:\n\n$ fly -t example destroy-pipeline --pipeline my-pipeline\n","depth":4,"section_tag":"fly-destroy-pipeline"},"fly-destroy-team":{"location":"managing-teams.html#fly-destroy-team","title":"fly destroy-team","text":"To remove a team, including all of its pipelines and one-off builds, first log in as the The main team, and then run:\n\n$ fly -t example destroy-team --team-name my-team\nCurrently, if there were any workers assigned specifically to this team, they'll be orphaned, without having their containers or volumes cleaned up.\n\n","depth":4,"section_tag":"fly-destroy-team"},"fly-execute":{"location":"running-tasks.html#fly-execute","title":"fly execute","text":"You can execute a task like this:\n\n$ fly -t example execute --config tests.yml\nYour files will be uploaded and the task will be executed with them. The working directory name will be used as the input name. If they do not match, you must specify -i name=. instead, where name is the input name from the task configuration.\n\nFly will automatically capture SIGINT and SIGTERM and abort the build when received. This allows it to be transparently composed with other toolchains.\n\nBy default, Fly will not send extra files or large files in your current directory that would normally be ignored by your version control system. You can use the --include-ignored flags in order to send ignored files to Concourse along with those that are not ignored.\n\nIf your task needs to run as root then you can specify the -p or --privileged flag.\n\n","depth":4,"section_tag":"fly-execute"},"fly-execute-vars":{"location":"running-tasks.html#fly-execute-vars","title":"Providing values for ((vars))","text":"Task config files can contain template variables in the form of ((foo-bar)), the same as pipeline ((vars)).\n\nThese vars can be set during fly execute by using the -v, -y and -l flags, the same as fly set-pipeline:\n\nfly -t example execute --config tests.yml \\\n  -l vars.yml \\\n  -v some_string=\"Hello World!\" \\\n  -y some_bool=true\nAny variables not satisfied via the above flags will be deferred to the configured credential manager.\n\nTo satisfy these vars when running the task in a pipeline, see vars.\n\n","depth":5,"section_tag":"fly-execute-vars"},"fly-expose-pipeline":{"location":"managing-pipelines.html#fly-expose-pipeline","title":"fly expose-pipeline","text":"By default, newly configured pipelines are only visible to the pipeline's team. To make a pipeline viewable by other teams and unauthenticated users, run:\n\n$ fly -t example expose-pipeline --pipeline my-pipeline\nThis feature is useful if you're using Concourse for an open source project and you'd like your community to be able to see into your build pipeline.\n\nTo undo this change, see fly hide-pipeline.\n\nExposing a pipeline reveals basically everything but the output of builds. The pipeline will be publicly viewable, as well as resource versions and metadata, which includes things like commit messages. Build output will remain hidden by default unless the job configures public: true.\n\nCurrently, step names within a build are visible in the API even without public: true. See concourse #2116 for more info.\n\n","depth":4,"section_tag":"fly-expose-pipeline"},"fly-format-pipeline":{"location":"setting-pipelines.html#fly-format-pipeline","title":"fly format-pipeline","text":"To format a pipeline config in a \"canonical\" form (i.e. keys are in normal order, with name first for example), run:\n\n$ fly format-pipeline --config pipeline.yml\nThis will print the formatted pipeline config to stdout. To update the file in-place, pass --write/-w.\n\n","depth":4,"section_tag":"fly-format-pipeline"},"fly-get-pipeline":{"location":"managing-pipelines.html#fly-get-pipeline","title":"fly get-pipeline","text":"Fly can be used to fetch and update the configuration for your pipelines. This is achieved by using the fly get-pipeline and fly set-pipeline commands. For example, to fetch the current configuration of your my-pipeline Concourse pipeline and print it on STDOUT run the following:\n\n$ fly -t example get-pipeline --pipeline my-pipeline\nTo get JSON instead of YAML you can use the -j or --json argument. This can be useful when inspecting your config with jq.\n\n","depth":4,"section_tag":"fly-get-pipeline"},"fly-hide-pipeline":{"location":"managing-pipelines.html#fly-hide-pipeline","title":"fly hide-pipeline","text":"If you realize that you've made a terrible mistake in exposing your pipeline, you can run:\n\n$ fly -t example hide-pipeline --pipeline my-pipeline\nIf you're panicking you can run the command's short form, hp, instead.\n\n","depth":4,"section_tag":"fly-hide-pipeline"},"fly-intercept":{"location":"builds.html#fly-intercept","title":"fly intercept","text":"Sometimes it's helpful to be on the same machine as your tasks so that you can profile or inspect them as they run or see the state the machine at the end of a run. Due to Concourse running tasks in containers on remote machines this would typically be hard to access.\n\nTo this end, there is a fly intercept command that will give you an interactive shell inside the specified container. Containers are identified by a few things, so you may need to specify a few flags to hone down the results. If there are multiple containers that the flags could refer to, an interactive prompt will show up allowing you to disambiguate.\n\nFor example, running the following will run a task and then enter the finished task's container:\n\n$ fly -t example execute\n$ fly -t example intercept --step build\nWhen intercepting a task running on a Windows worker, you will need to specifically tell fly to to run powershell:\n\n$ fly -t example intercept powershell\nContainers are around for a short time after a build finishes in order to allow people to intercept them.\n\nYou can also intercept builds that were run in your pipeline. By using --job, --build, and --step you can intercept a specific step from a build of a job in your pipeline. These flags also have short forms, like so:\n\n$ fly -t example intercept -j some-pipeline/some-job -b some-build -s some-step\nNote that --build can be omitted, and will default to the most recent build of the job. One-off builds can be reached by passing in their build ID to --build which can be found on the build list page.\n\nThe --step flag can also be omitted; this will let you pick the step interactively if you don't know the exact name.\n\nResource checking containers can also be intercepted with --check or -c:\n\n$ fly -t example intercept --check some-pipeline/some-resource\nA specific command can also be given, e.g. fly intercept ps auxf or fly intercept htop. This allows for patterns such as watch fly intercept ps auxf, which will continuously show the process tree of the current build's task, even as the \"current build\" changes.\n\nThe working directory and any relevant environment variables (e.g. those having come from params) used by the original process will also be used for the process run by intercept.\n\n","depth":3,"section_tag":"fly-intercept"},"fly-jobs":{"location":"managing-jobs.html#fly-jobs","title":"fly jobs","text":"To list the jobs configured in a pipeline, run:\n\n$ fly -t example jobs -p my-pipeline\n","depth":4,"section_tag":"fly-jobs"},"fly-land-worker":{"location":"administration.html#fly-land-worker","title":"fly land-worker","text":"To initiate landing of a worker and eventually (after draining) cause it to exit, run:\n\n$ fly -t example land-worker --worker worker-name\n","depth":4,"section_tag":"fly-land-worker"},"fly-login":{"location":"fly.html#fly-login","title":"fly login","text":"The first thing you'll want to do is authenticate with your target. This is done with the fly login command. This is also useful to save targets under a more convenient alias, so you don't have to type out the URL all the time:\n\nThe login command serves double duty: it authenticates with a given endpoint, and saves it under a more convenient name. The name and token are stored in ~/.flyrc (though you shouldn't really edit the file manually).\n\nConcourse deployments can be occupied by multiple teams. To specify the team to which to log in, specify the --team-name or -n flag. If not specified, this defaults to the The main team.\n\nSo, to log in to a team my-team an endpoint served at https://ci.example.com and save it as the more convenient name example, you would run:\n\n$ fly --target example login --team-name my-team \\\n    --concourse-url https://ci.example.com\nThe login command will see which authentication methods are available for the specified team and prompt you to choose one. For basic auth, it will ask your username and password and use them to acquire a token. For OAuth, it will give you a link to click, and after you've gone through the OAuth flow it will print an OAuth token on the page that you can then copy and paste into the prompt.\n\nNote that if no authentication methods are configured, fly will acquire a token without any prompting. You can then use the alias like normal.\n\nIn any case, a token is saved in your ~/.flyrc, which will expire after one day.\n\nIf your Concourse uses SSL but does not have a certificate signed by a trusted CA, you can use the --ca-cert flag so that fly can trust the connection, like so:\n\n$ fly -t example login -c https://ci.example.com --ca-cert ./ca.crt\nThis will read the value out of the file ./ca.crt and save it into ~/.flyrc so you don't have to pass it on every login invocation.\n\nAfter you've logged in you can use --target example (or -t example for short) to run a command against the saved target example. For eample, fly -t example builds will list the last few builds on the example Concourse instance.\n\nThe -t flag is intentionally stateless and must be explicitly added to each command. This reduces the risk of accidentally running a command against the wrong environment when you have multiple targets defined.\n\n","depth":3,"section_tag":"fly-login"},"fly-logout":{"location":"fly.html#fly-logout","title":"fly logout","text":"There are cases when you would like to remove all evidence of a particular target. This is achieved by the logout command. There are two variants of this command, one to get rid of a specific target, and another to remove all targets from the ~/.flyrc file.\n\nTo remove a specific target run:\n\n$ fly -t example logout\nTo remove all targets run:\n\n$ fly logout -a\nNote: These two variations are mutually exclusive. If the target parameter -t and all parameter -a are both specified, an error will occur.\n\n","depth":3,"section_tag":"fly-logout"},"fly-order-pipelines":{"location":"managing-pipelines.html#fly-order-pipelines","title":"fly order-pipelines","text":"To configure the ordering of pipelines, run:\n\n$ fly -t example order-pipelines \\\n    --pipeline pipeline-1 \\\n    --pipeline pipeline-2 \\\n    --pipeline pipeline-3\nNote that this command only ensures that the given pipelines are in the given order. If there are other pipelines that you haven't included in the command, they may appear in-between, before, or after the given set.\n\n","depth":4,"section_tag":"fly-order-pipelines"},"fly-pause-job":{"location":"managing-jobs.html#fly-pause-job","title":"fly pause-job","text":"To prevent scheduling and running builds of a job, run:\n\n$ fly -t example pause-job --job my-pipeline/my-job\nThis will prevent pending builds of the job from being scheduled, though builds that are in-flight will still run, and pending builds will still be created as normal.\n\n","depth":4,"section_tag":"fly-pause-job"},"fly-pause-pipeline":{"location":"managing-pipelines.html#fly-pause-pipeline","title":"fly pause-pipeline","text":"To pause a pipeline, run:\n\n$ fly -t example pause-pipeline --pipeline my-pipeline\nThis will prevent jobs from being scheduled and stop the periodic checking for new versions of resources. Builds that are in-flight will still finish.\n\n","depth":4,"section_tag":"fly-pause-pipeline"},"fly-pipelines":{"location":"managing-pipelines.html#fly-pipelines","title":"fly pipelines","text":"To list the currently-configured pipelines and their paused state, run:\n\n$ fly -t example pipelines\n","depth":4,"section_tag":"fly-pipelines"},"fly-prune-worker":{"location":"administration.html#fly-prune-worker","title":"fly prune-worker","text":"To remove a stalled, landing, landed, or retiring worker, run:\n\n$ fly -t example prune-worker --worker worker-name\nThis is for those cases where you know a worker is not coming back. Note that running workers cannot be pruned, since they'll just re-register themselves anyway.\n\n","depth":4,"section_tag":"fly-prune-worker"},"fly-rename-pipeline":{"location":"managing-pipelines.html#fly-rename-pipeline","title":"fly rename-pipeline","text":"To rename a pipeline, run:\n\n$ fly -t example rename-pipeline \\\n    --old-name my-pipeline \\\n    --new-name my-cool-pipeline\n","depth":4,"section_tag":"fly-rename-pipeline"},"fly-rename-team":{"location":"managing-teams.html#fly-rename-team","title":"fly rename-team","text":"To rename a team, run:\n\n$ fly -t example rename-team --old-name my-team --new-name cool-team\nThis can only be run by the main team.\n\n","depth":4,"section_tag":"fly-rename-team"},"fly-set-pipeline":{"location":"setting-pipelines.html#fly-set-pipeline","title":"fly set-pipeline","text":"To submit a pipeline configuration to Concourse from a file on your local disk you can use the -c or --config flag, like so:\n\n$ fly -t example set-pipeline \\\n    --pipeline my-pipeline \\\n    --config pipeline.yml\nThis will present a diff of the changes and ask you to confirm the changes. If you accept then Concourse's pipeline configuration will switch to the pipeline definition in the YAML file specified.\n\n","depth":4,"section_tag":"fly-set-pipeline"},"fly-set-team":{"location":"managing-teams.html#fly-set-team","title":"fly set-team","text":"Once you've logged in as the main team with fly, you can run fly set-team to create or update other teams. Users with a Team Owner role can also update their own configuration with the same command.\n\nFor example, to create a new team that authorizes the local foo user, you would run:\n\nfly -t example set-team --team-name my-team \\\n  --local-user foo\nNote that each time set-team is run, the team's authorization config is set as a whole - it is not a stateful operation.\n\nThere are many different ways to configure team auth; see Configuring Auth for more information.\n\nOnce the team has been created, you can use fly login to log in:\n\n$ fly login -n my-team\nAny newly configured pipelines (via fly set-pipeline) and one-off builds (via fly execute) will be owned by the authorized team. Commands that list content will be scoped to the current team by default, such as fly pipelines and fly builds. The web UI will reflect the same state.\n\nNewly configured pipelines are hidden by default, meaning other teams and unauthorized visitors cannot view them. To make them publicly viewable, see Pipeline \u0026 Build Visibility.\n\n","depth":4,"section_tag":"fly-set-team"},"fly-status":{"location":"fly.html#fly-status","title":"fly status","text":"To check your current authentication status with a given target, run:\n\n$ fly -t example status\nThis will let you know if the token has expired.\n\n","depth":3,"section_tag":"fly-status"},"fly-sync":{"location":"fly.html#fly-sync","title":"fly sync","text":"Occasionally we add additional features to fly or make changes to the communiction between it and Concourse's API server. To make sure you're running the latest and greatest version that works with the Concourse you are targeting we provide a command called sync that will update your local fly. It can be used like so:\n\n$ fly -t example sync\nThe fly command will also warn you if it notices that your CLI version is out of sync with the server.\n\n","depth":3,"section_tag":"fly-sync"},"fly-targets":{"location":"fly.html#fly-targets","title":"fly targets","text":"To see what targets are currently known to fly, run:\n\n$ fly targets\nThis will show each target's name, URL, and when its token expires.\n\n","depth":3,"section_tag":"fly-targets"},"fly-teams":{"location":"managing-teams.html#fly-teams","title":"fly teams","text":"To list all the teams, run:\n\n$ fly -t example teams\nThis can be useful if you've forgotten your team name.\n\nfly teams -d: With Details\n\nTo list all the teams with authentication details and members, run:\n\n$ fly -t example teams -d\nThis can be helpful when debugging OAuth, OIDC groups or listing all individual members.\n\n","depth":4,"section_tag":"fly-teams"},"fly-trigger-job":{"location":"managing-jobs.html#fly-trigger-job","title":"fly trigger-job","text":"To immediately queue a new build of a job, run:\n\n$ fly -t example trigger-job --job my-pipeline/my-job\nThis will enqueue a new build of the my-job job in the my-pipeline pipeline.\n\nTo start watching the newly created build, append the --watch flag like so:\n\n$ fly -t example trigger-job --job my-pipeline/my-job --watch\n","depth":4,"section_tag":"fly-trigger-job"},"fly-unpause-job":{"location":"managing-jobs.html#fly-unpause-job","title":"fly unpause-job","text":"To resume scheduling of a job, run:\n\n$ fly -t example unpause-job --job my-pipeline/my-job\nThis will resume scheduling of builds queued for the job.\n\n","depth":4,"section_tag":"fly-unpause-job"},"fly-unpause-pipeline":{"location":"managing-pipelines.html#fly-unpause-pipeline","title":"fly unpause-pipeline","text":"To unpause a pipeline, run:\n\n$ fly -t example unpause-pipeline --pipeline my-pipeline\nThis will resume job scheduling and resource checking.\n\n","depth":4,"section_tag":"fly-unpause-pipeline"},"fly-userinfo":{"location":"fly.html#fly-userinfo","title":"fly userinfo","text":"To check what user you're logged in as, as well as which teams you are currently authenticated to and which roles within each team you have, run:\n\n$ fly -t example userinfo\n","depth":3,"section_tag":"fly-userinfo"},"fly-validate-pipeline":{"location":"setting-pipelines.html#fly-validate-pipeline","title":"fly validate-pipeline","text":"To validate a local pipeline configuration without submitting it to Concourse, run validate-pipeline:\n\n$ fly validate-pipeline --config pipeline.yml\nBy default, pipeline errors will cause validate-pipeline to fail, but warnings won't. To fail on both errors and warnings, pass the `--strict` flag.\n\n","depth":4,"section_tag":"fly-validate-pipeline"},"fly-volumes":{"location":"administration.html#fly-volumes","title":"fly volumes","text":"To list the active volumes across all your workers, run:\n\n$ fly -t example volumes\nThis can be useful to observe the caches warming across your cluster, and could be a good indicator of disk use.\n\n","depth":4,"section_tag":"fly-volumes"},"fly-watch":{"location":"builds.html#fly-watch","title":"fly watch","text":"Concourse emits streaming colored logs on the website but it can be helpful to have the logs availiable to the command line. (e.g. so that they can be processed by other commands).\n\nThe watch command can be used to do just this. You can also view builds that are running in your pipeline, or builds that have already finished.\n\nNote that unlike fly execute, killing fly watch via SIGINT or SIGTERM will not abort the build.\n\nTo watch the most recent one-off build, just run fly watch with no arguments. To watch a specific build (one-off or no), pass --build with the ID of the build to watch. This ID is available at the start of fly execute's output or by browsing to the builds list in the web UI.\n\nBy using the --job and --build flags you can pick out a specific build of a job to watch. For example, the following command will either show the archived logs for an old build if it has finished running or it will stream the current logs if the build is still in progress.\n\n$ fly -t example watch --job my-pipeline/tests --build 52\nIf the --job flag is specified and --build is omitted, the most recent build of the specified job will be selected.\n\n","depth":3,"section_tag":"fly-watch"},"fly-workers":{"location":"administration.html#fly-workers","title":"fly workers","text":"To list the currently registered workers, including additional metadata, run:\n\n$ fly -t example workers\nThis can be useful for monitoring the status of your workers, if you suspect that one keeps dropping out of the pool or getting tasked with too many containers, etc.\n\n","depth":4,"section_tag":"fly-workers"},"garbage-collection":{"location":"garbage-collection.html","title":"Garbage Collection","text":"One key difference between Concourse and other CI systems is that everything runs in isolated environments. Where some CI systems may just run builds one at a time on a single VM and reusing a working directory, Concourse creates fresh Containers and Volumes to ensure things can safely run in a repeatable environment, isolated from other workloads running on the same worker.\n\nThis introduces a new problem of knowing when Concourse should remove these containers and volumes. Safely identifying things for removal and then getting rid of them, releasing their resources, is the process of garbage collection.\n\n","depth":4,"section_tag":"garbage-collection"},"generating-keys":{"location":"concourse-generate-key.html","title":"Generating Keys","text":"Concourse's various components use RSA keys to verify tokens and worker registration requests.\n\nA minimal deployment will require the following keys:\n\nsession_signing_key: Used by the Running a web node for signing and verifying user session tokens.\n\n\ntsa_host_key: Used by the Running a web node for the SSH worker registration gateway server (\"TSA\").\n\nThe public key is given to each Running a worker node to verify the remote host when connecting via SSH.\n\n\nworker_key (one per worker): Each Running a worker node verifies its registration with the Running a web node via a SSH key.\n\nThe public key must be listed in the Running a web node's authorized keys configuration in order for the worker to register.\n\n\n\nTo generate these keys, run:\n\nconcourse generate-key -t rsa -f ./session_signing_key\nconcourse generate-key -t ssh -f ./tsa_host_key\nconcourse generate-key -t ssh -f ./worker_key\n...and we'll also start on an authorized_keys file, currently listing this initial worker key:\n\ncp worker_key.pub authorized_worker_keys\n","depth":3,"section_tag":"concourse-generate-key"},"generic-oauth":{"location":"generic-oauth.html","title":"Generic oAuth","text":"A Concourse server can authenticate against any valid OAuth auth provider, though it's a bit \"closer to the metal\" as you'll need to explicitly configure the auth, token, and user-info URLs. You may want to see if you can use Generic OIDC auth if your auth provider is compatible with OIDC.\n\n","depth":4,"section_tag":"generic-oauth"},"generic-oauth-authentication":{"location":"generic-oauth.html#generic-oauth-authentication","title":"Authentication","text":"First you'll need to create a client with your oAuth provider.\n\nThe callback URL must be the URL of your Concourse server with /sky/issuer/callback appended. This address must be reachable by your oAuth provider - it can't be localhost.\n\nFor example, Concourse's own CI server's callback URL would be:\n\nhttps://ci.concourse-ci.org/sky/issuer/callback\nThe Generic oAuth provider has many values to set - for a full list consult concourse web --help.\n\nA typical Running a web node env config may look something like this:\n\nCONCOURSE_OAUTH_DISPLAY_NAME=Acme\nCONCOURSE_OAUTH_CLIENT_ID=myclientid\nCONCOURSE_OAUTH_CLIENT_SECRET=myclientsecret\nCONCOURSE_OAUTH_AUTH_URL=https://oauth.example.com/oauth2/auth\nCONCOURSE_OAUTH_TOKEN_URL=https://oauth.example.com/oauth2/token\nCONCOURSE_OAUTH_USERINFO_URL=https://oauth.example.com/oauth2/userinfo\nConsult concourse web --help for a full list of flags with descriptions.\n\n","depth":5,"section_tag":"generic-oauth-authentication"},"generic-oauth-authorization":{"location":"generic-oauth.html#generic-oauth-authorization","title":"Authorization","text":"OAuth users and groups can be authorized for a team by passing the following flags to fly set-team:\n\n--oauth-user=USERNAME: Authorize an individual user.\n\n\n--oauth-group=GROUP_NAME: Authorize anyone from the group.\n\nYou may only configure groups if the auth provider exposes this information in either the token itself, or in the contents of the userinfo endpoint.\n\nYou can configure which claim points to the groups information by specifying CONCOURSE_OAUTH_GROUPS_KEY on the Running a web node.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --oauth-user my-username \\\n    --oauth-group my-group\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  oauth:\n    users: [\"my-username\"]\n    groups: [\"my-group\"]\n","depth":5,"section_tag":"generic-oauth-authorization"},"generic-oidc-auth":{"location":"generic-oidc-auth.html","title":"Generic OIDC auth","text":"A Concourse server can authenticate against any valid OIDC auth provider. This provider is similar to Generic oAuth except it only requires an issuer URL rather than auth/token/userinfo URLs.\n\n","depth":4,"section_tag":"generic-oidc-auth"},"generic-oidc-authentication":{"location":"generic-oidc-auth.html#generic-oidc-authentication","title":"Authentication","text":"First you'll need to create a client with your oAuth provider.\n\nThe callback URL must be the URL of your Concourse server with /sky/issuer/callback appended. This address must be reachable by your OIDC provider - it can't be localhost.\n\nFor example, Concourse's own CI server's callback URL would be:\n\nhttps://ci.concourse-ci.org/sky/issuer/callback\nA typical Running a web node env config may look something like this:\n\nCONCOURSE_OIDC_DISPLAY_NAME=Acme\nCONCOURSE_OIDC_CLIENT_ID=myclientid\nCONCOURSE_OIDC_CLIENT_SECRET=myclientsecret\nCONCOURSE_OIDC_ISSUER=https://oidc.example.com\nConsult concourse web --help for a full list of flags with descriptions.\n\n","depth":5,"section_tag":"generic-oidc-authentication"},"generic-oidc-authorization":{"location":"generic-oidc-auth.html#generic-oidc-authorization","title":"Authorization","text":"OIDC users and groups can be authorized for a team by passing the following flags to fly set-team:\n\n--oidc-user=USERNAME: Authorize an individual user.\n\n\n--oidc-group=GROUP_NAME: Authorize anyone from the group.\n\nYou may only configure groups if the auth provider exposes this information in either the token itself, or in the contents of the userinfo endpoint.\n\nYou can configure which claim points to the groups information by specifying CONCOURSE_OIDC_GROUPS_KEY on the Running a web node.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --oidc-user my-username \\\n    --oidc-group my-group\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  oidc:\n    users: [\"my-username\"]\n    groups: [\"my-groups\"]\n","depth":5,"section_tag":"generic-oidc-authorization"},"get-step":{"location":"get-step.html","title":"get step","text":"Fetches a resource, making it available to subsequent steps via the given name.\n\n","depth":4,"section_tag":"get-step"},"get-step-get":{"location":"get-step.html#get-step-get","title":"get","text":"Required. The name of the resource once it is fetched. This name satisfies logical inputs to a Task, and may be referenced within the plan itself (e.g. in the file attribute of a task step).\n\nThis is also the name of the resource to fetch, if resource is not set.\n\n","depth":4,"section_tag":"get-step"},"get-step-params":{"location":"get-step.html#get-step-params","title":"params","text":"Optional. A map of arbitrary configuration to forward to the resource. Refer to the resource type's documentation to see what it supports.\n\n","depth":4,"section_tag":"get-step"},"get-step-passed":{"location":"get-step.html#get-step-passed","title":"passed","text":"Optional. When specified, only the versions of the resource that made it through the given list of jobs will be considered when triggering and fetching.\n\n","depth":4,"section_tag":"get-step"},"get-step-resource":{"location":"get-step.html#get-step-resource","title":"resource","text":"Optional. Defaults to get, the name. The resource to fetch, as configured in resources.\n\nUse this attribute to rename a resource from the overall pipeline context into the job-specific context.\n\n","depth":4,"section_tag":"get-step"},"get-step-trigger":{"location":"get-step.html#get-step-trigger","title":"trigger","text":"Optional. Default false. Set to true to auto-trigger new builds of the plan's job whenever this step has new versions available, as specified by the resource and any passed constraints.\n\nOtherwise, if no get steps set this to true, the job can only be manually triggered.\n\n","depth":4,"section_tag":"get-step"},"get-step-version":{"location":"get-step.html#get-step-version","title":"version","text":"Optional. Defaults to latest. The version of the resource to fetch.\n\nIf set to latest, scheduling will just find the latest available version of a resource and use it, allowing versions to be skipped.  This is usually what you want, e.g. if someone pushes 100 git commits.\n\nIf set to every, builds will walk through all available versions of the resource. Note that if passed is also configured, it will only step through the versions satisfying the constraints.\n\nIf set to a specific version (e.g. {ref: abcdef123}), only that version will be used. Note that the version must be available and detected by the resource, otherwise the input will never be satisfied. You may want to use fly check-resource to force detection of resource versions, if you need to use an older one that was never detected (as all newly configured resources start from the latest version).\n\n","depth":4,"section_tag":"get-step"},"github-auth":{"location":"github-auth.html","title":"GitHub auth","text":"A Concourse server can authenticate against GitHub to leverage their permission model and other security improvements in their infrastructure.\n\n","depth":4,"section_tag":"github-auth"},"github-authentication":{"location":"github-auth.html#github-authentication","title":"Authentication","text":"First, you'll need to create an OAuth application on GitHub.\n\nThe \"Authorization callback URL\" must be the URL of your Concourse server with /sky/issuer/callback appended. This address must be reachable by GitHub - it can't be localhost.\n\nFor example, Concourse's own CI server's callback URL would be:\n\nhttps://ci.concourse-ci.org/sky/issuer/callback\nYou will be given a Client ID and a Client Secret for your new application. The client ID and secret must then be configured on the Running a web node by setting the following env:\n\nCONCOURSE_GITHUB_CLIENT_ID=myclientid\nCONCOURSE_GITHUB_CLIENT_SECRET=myclientsecret\nNote that the client must be created under an organization if you want to authorize users based on organization/team membership. If the client is created under a personal account, only individual users can be authorized.\n\nIf you're configuring GitHub Enterprise, you'll also need to set the following env:\n\nCONCOURSE_GITHUB_HOST=github.example.com\nCONCOURSE_GITHUB_CA_CERT=/path/to/ca_cert\nThe GitHub Enterprise host must not contain a scheme, or a trailing slash.\n\n","depth":5,"section_tag":"github-authentication"},"github-authorization":{"location":"github-auth.html#github-authorization","title":"Authorization","text":"Users, teams, and entire organizations can be authorized for a team by passing the following flags to fly set-team:\n\n--github-user=LOGIN: Authorize an individual user.\n\n\n--github-org=ORG_NAME: Authorize an entire organization's members.\n\n\n--github-team=ORG_NAME:TEAM_NAME: Authorize a team's members within an organization.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --github-user my-github-login \\\n    --github-org my-org \\\n    --github-team my-other-org:my-team\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  github:\n    users: [\"my-github-login\"]\n    orgs: [\"my-org\"]\n    teams: [\"my-other-org:my-team\"]\n","depth":5,"section_tag":"github-authorization"},"gitlab-auth":{"location":"gitlab-auth.html","title":"GitLab auth","text":"A Concourse server can authenticate against GitLab to leverage their permission model.\n\n","depth":4,"section_tag":"gitlab-auth"},"gitlab-authentication":{"location":"gitlab-auth.html#gitlab-authentication","title":"Authentication","text":"First you need to create an OAuth application on GitLab.\n\nThe \"Authorization callback URL\" must be the URL of your Concourse server with /sky/issuer/callback appended. This address must be reachable by GitLab - it can't be localhost.\n\nFor example, Concourse's own CI server's callback URL would be:\n\nhttps://ci.concourse-ci.org/sky/issuer/callback\nYou will be given a Client ID and a Client Secret for your new application. The client ID and secret must then be configured on the Running a web node by setting the following env:\n\nCONCOURSE_GITLAB_CLIENT_ID=myclientid\nCONCOURSE_GITLAB_CLIENT_SECRET=myclientsecret\nIf you're configuring a self hosted GitLab instance, you'll also need to set the following flag:\n\nCONCOURSE_GITLAB_HOST=https://gitlab.example.com\nThe GitLab host must contain a scheme and not a trailing slash.\n\n","depth":5,"section_tag":"gitlab-authentication"},"gitlab-authorization":{"location":"gitlab-auth.html#gitlab-authorization","title":"Authorization","text":"Users and groups can be authorized for a team by passing the following flags to fly set-team:\n\n--gitlab-user=USERNAME: Authorize an individual user.\n\n\n--gitlab-group=GROUP_NAME: Authorize an entire groups's members.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --gitlab-user my-gitlab-user \\\n    --gitlab-team my-team\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  gitlab:\n    users: [\"my-gitlab-login\"]\n    orgs: [\"my-org\"]\n    teams: [\"my-other-org:my-team\"]\n","depth":5,"section_tag":"gitlab-authorization"},"global-resources":{"location":"global-resources.html","title":"Global Resources (experimental)","text":"Concourse v5.0 contains an experimental feature known as \"global resources\". It is enabled by passing the --enable-global-resources flag to the concourse web command.\n\nThe basic concept of global resources is to share detected resource versions between all resources that have the same type and source configuration.\n\nBefore v5.0.0, each pipeline resource had its own version history, associated to the resource by name. This meant that multiple pipelines with the same resource configs would redundantly collect the same version and metadata information.\n\nWith v5.0.0's experimental 'global resources' feature, resource versions are instead associated to an anonymous 'resource config' i.e. its type and source.\n\n","depth":3,"section_tag":"global-resources"},"goals":{"location":"garbage-collection.html#goals","title":"Goals","text":"Let's define our metrics for success:\n\n* Safe. There should never be a case where a build is running and a container or volume is removed out from under it, causing the build to fail. Resource checking should also never result in errors from check containers being removed. No one should even know garbage collection is happening.\n\n* Airtight. Everything Concourse creates, whether it's a container or volume on a worker or an entry in the database, should never leak. Each object should have a fully defined lifecycle such that there is a clear end to its use.  The ATC should be interruptible at any point in time and at the very least be able to remove any state it had created beforehand.\n\n* Resilient. Garbage collection should never be outpaced by the workload. A single misbehaving worker should not prevent garbage collection from being performed on other workers. A slow delete of a volume should not prevent garbage collecting of other things on the same worker.\n\n","depth":5,"section_tag":"goals"},"golang-library-example":{"location":"golang-library-example.html","title":"Golang Library","text":"","depth":3,"section_tag":"golang-library-example"},"gracefully-removing-a-worker":{"location":"concourse-worker.html#gracefully-removing-a-worker","title":"Gracefully Removing a Worker","text":"When a worker machine is going away, it should be retired. This is similar to landing, except at the end the worker is completely unregistered, along with its volumes and containers. This should be done when a worker's VM or container is being destroyed.\n\nTo retire a worker, send SIGUSR2 to the worker process. This will switch the worker to retiring state, and Concourse will stop scheduling new work on it. When all builds running on the worker have finished, the worker will be removed and the worker process will exit.\n\nJust like with landing, you may want to enforce a timeout for draining - that way a stuck build won't prevent your workers from being upgraded. This can be enforced by common tools like start-stop-daemon:\n\nstart-stop-daemon \\\n  --pidfile worker.pid \\\n  --stop \\\n  --retry USR2/300/TERM/15/KILL\nThis will send SIGUSR2, wait up to 5 minutes, and then send SIGTERM. If it's still running, it will be killed after an additional 15 seconds.\n\n","depth":5,"section_tag":"gracefully-removing-a-worker"},"group-jobs":{"location":"pipeline-groups.html#group-jobs","title":"jobs","text":"Optional. A list of jobs that should appear in this group. A job may appear in multiple groups. Neighbours of jobs in the current group will also appear on the same page in order to give context of the location of the group in the pipeline.\n\n","depth":3,"section_tag":"pipeline-groups"},"group-name":{"location":"pipeline-groups.html#group-name","title":"name","text":"Required. The name of the group. This should be short and simple as it will be used as the tab name for navigation.\n\n","depth":3,"section_tag":"pipeline-groups"},"group-resources":{"location":"pipeline-groups.html#group-resources","title":"resources","text":"Optional. A list of resources that should appear in this group. Resources that are inputs or outputs of jobs in the group are automatically added; they do not have to be explicitly listed here.\n\n","depth":3,"section_tag":"pipeline-groups"},"groups":{"location":"pipelines.html#groups","title":"groups","text":"A list of groups to use for cleaning up/organizing jobs in the web UI.\n\n","depth":2,"section_tag":"pipelines"},"hooks-example":{"location":"hooks-example.html","title":"Job \u0026 Task Hooks","text":"","depth":3,"section_tag":"hooks-example"},"how-it-works":{"location":"garbage-collection.html#how-it-works","title":"How it Works","text":"The garbage collector is a batch operation that runs every 30 seconds. This number was chosen arbitrarily and may be reduced in the future. It's important to note that the collector must be able to run frequently enough to not be outpaced by the workload producing things, and so the batch operation should be able to complete pretty quickly.\n\nThe batch operation first performs garbage collection within the database alone, removing rows that are no longer needed. The removal of rows from one stage will often result in removals in a later stage.  They are run in the following order:\n\n* builds that no longer meet the Build Retention criteria are marked non-interceptible\n\n* workers are stepped through their state machine. Unresponsive workers become STALLED, workers that are RETIRING are deleted once drained, and workers that are LANDING become LANDED once drained.\n\n* build_image_resource_caches are removed for builds that finished over 24 hours ago.\n\n* resource_cache_uses are removed for builds that are no longer interceptible.\n\n* resource_configs that are no longer referenced by a resource_caches (resource_config_id) or a resource_config_check_sessions (resource_config_id) are removed.\n\n* resource_caches that are no longer referenced by a resource_configs (resource_cache_id) or a resource_cache_uses (resource_cache_id) are removed.\n\n* resource_config_check_sessions that have exceeded their resource_config_check_sessions (expires_at) are removed.\n\nIf any of the above operations fail, the garbage collector will just log an error and move on. This is so that failure to collect one class of objects does not prevent everything else from being garbage collected. Failure at any part of the garbage collection is OK; it can just retry on the next pass.\n\nAfter the initial pass of garbage collection in the database, there should now be a set of volumes and containers that meet criteria for garbage collection. These two are a bit more complicated to garbage-collect; they both require talking to a worker, and waiting on a potentially slow delete.\n\nContainers and volumes are the costliest resources consumed by Concourse. There are also many of them created over time as builds execute and pipelines perform their resource checking. Therefore it is important to parallelize this aspect of garbage collection so that one slow delete or one slow worker does not cause them to pile up.\n\nSo, the next two steps are Container Collection and Volume Collection.\n\n","depth":5,"section_tag":"how-it-works"},"http response time":{"location":"metrics.html#http response time","title":"http response time","text":"This metric is emitted for each HTTP request to an ATC (both API and web requests). It contains the duration (in milliseconds) for each request and is useful for finding slow requests.\n\nAttributes route\n\n: The route which the HTTP request matched. i.e. /builds/:id\n\n\npath\n\n: The literal path of the HTTP request. i.e. /builds/1234\n\n\n\n\n\n","depth":4,"section_tag":"whats-emitted"},"iam-permissions":{"location":"aws-asm-credential-manager.html#iam-permissions","title":"IAM Permissions","text":"The following is an example of an IAM policy that can be used to grant permissions to an IAM user or instance role. Note that the Resource section can contain a wildcard to a secret or be restricted to an individual secret.\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n        \"Sid\": \"AllowAccessToSecretManagerParameters\",\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"secretsmanager:ListSecrets\"\n        ],\n          \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"AllowAccessGetSecret\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"secretsmanager:GetSecretValue\",\n                \"secretsmanager:DescribeSecret\"\n            ],\n            \"Resource\": [\n                \"arn:aws:secretsmanager:::secret:/concourse/*\",\n                \"arn:aws:secretsmanager:::secret:/concourse/TEAM_NAME/*\",\n                \"arn:aws:secretsmanager:::secret:/concourse/TEAM_NAME/PIPELINE_NAME/*\"\n            ]\n        }\n    ]\n}\nNote that the TEAM_NAME and PIPELINE_NAME text above should be replaced to fit your Concourse setup.\n\nFor more information on how to use IAM roles to restrict access to Secrets Manager, review the official documentation.\n\n","depth":5,"section_tag":"iam-permissions"},"image-resource-params":{"location":"tasks.html#image-resource-params","title":"image_resource.params","text":"Optional. A map of arbitrary configuration to forward to the resource. Refer to the resource type's documentation to see what it supports.\n\n","depth":2,"section_tag":"tasks"},"image-resource-source":{"location":"tasks.html#image-resource-source","title":"image_resource.source","text":"Required. The configuration for the resource; see source.\n\n","depth":2,"section_tag":"tasks"},"image-resource-type":{"location":"tasks.html#image-resource-type","title":"image_resource.type","text":"Required. The type of the resource. Usually docker-image.\n\n","depth":2,"section_tag":"tasks"},"image-resource-version":{"location":"tasks.html#image-resource-version","title":"image_resource.version","text":"Optional. A specific version of the resource to fetch. This should be a map with string keys and values. If not specified, the latest version will be fetched.\n\n","depth":2,"section_tag":"tasks"},"image_resource":{"location":"tasks.html#image_resource","title":"image_resource","text":"Where resource is:\n\nOptional. The base image of the container, as provided by a resource definition.\n\nYou can use any resource that returns a filesystem in the correct format (a /rootfs directory and a metadata.json file in the top level) but normally this will be the Docker Image resource. If you'd like to make a resource of your own that supports this please use that as a reference implementation for now.\n\nIf you want to use an artifact source within the plan containing an image, you must set the image in the plan step instead.\n\n","depth":2,"section_tag":"tasks"},"implementing-resource-types":{"location":"implementing-resource-types.html","title":"Implementing a Resource Type","text":"A resource type is implemented by a container image with three scripts:\n\n* /opt/resource/check for checking for new versions of the resource\n\n* /opt/resource/in for pulling a version of the resource down\n\n* /opt/resource/out for idempotently pushing a version up\n\nDistributing resource types as containers allows them to package their own dependencies. For example, the Git resource comes with git installed.\n\nAll resources must implement all three actions, though the actions can just be no-ops (which still must be correctly implemented as detailed below).\n\nResources can emit logs to the user by writing to stderr. ANSI escape codes (coloring, cursor movement, etc.) will be interpreted properly by the web UI, so you should make your output pretty.\n\n","depth":4,"section_tag":"implementing-resource-types"},"in":{"location":"implementing-resource-types.html#in","title":"in: Fetch a given resource.","text":"The in script is passed a destination directory as command line argument $1, and is given on stdin the configured source and a precise version of the resource to fetch.\n\nThe script must fetch the resource and place it in the given directory.\n\nIf the desired resource version is unavailable (for example, if it was deleted), the script must error.\n\nThe script must emit the fetched version, and may emit metadata as a list of key-value pairs. This data is intended for public consumption and will make it upstream, intended to be shown on the build's page.\n\nThe request will contain the following fields:\n\n* source is the same value as passed to check: Check for new versions..\n\n* version is the same type of value passed to check: Check for new versions., and specifies the version to fetch.\n\n* params is an arbitrary JSON object passed along verbatim from params on a get step.\n\nExample request, in this case for the git resource:\n\n{\n  \"source\": {\n    \"uri\": \"git://some-uri\",\n    \"branch\": \"develop\",\n    \"private_key\": \"...\"\n  },\n  \"version\": { \"ref\": \"61cebf\" }\n}\nUpon receiving this payload the git resource would probably do something like:\n\ngit clone --branch develop git://some-uri $1\ncd $1\ngit checkout 61cebf\nAnd output:\n\n{\n  \"version\": { \"ref\": \"61cebf\" },\n  \"metadata\": [\n    { \"name\": \"commit\", \"value\": \"61cebf\" },\n    { \"name\": \"author\", \"value\": \"Hulk Hogan\" }\n  ]\n}\n","depth":5,"section_tag":"in"},"included-resource-types":{"location":"included-resource-types.html","title":"Resource Types Provided With Concourse","text":"The following resource types come with Concourse out of the box, and are officially supported by the Concourse team:\n\n* The git resource can pull and push to git repositories.\n\n* The hg resource can pull and push to Mercurial repositories.\n\n* The time resource can start jobs on a schedule or timestamp outputs.\n\n* The s3 resource can fetch from and upload to S3 buckets.\n\n* The semver resource can set or bump version numbers.\n\n* The github-release resource can fetch and publish versioned GitHub resources.\n\n* The registry-image resource can fetch and push Docker images.\n\n* The docker-image resource can fetch, build, and push Docker images, though it requires privileged: true.\n\n* The pool resource allows you to configure how to serialize use of an external system. This lets you prevent test interference or overwork on shared systems.\n\n* The cf resource can deploy an application to Cloud Foundry.\n\n* The bosh-io-release resource can track and fetch new BOSH releases from bosh.io.\n\n* The bosh-io-stemcell resource can track and fetch new BOSH stemcells from bosh.io.\n\n","depth":4,"section_tag":"included-resource-types"},"index":{"location":"index.html","title":"Concourse","text":"Built on the simple mechanics of resources, tasks, and jobs, Concourse presents a general approach to automation that makes it great for CI/CD.\n\nYou can think of a pipeline as a distributed, higher-level, continuously-running Makefile.\n\nEach entry under resources is a dependency, and each entry under jobs describes a plan to run when the job is triggered (either manually or by a get step).\n\nJobs can depend on resources that have passed through prior jobs. The resulting sequence of jobs and resources is a dependency graph that continuously pushes your project forward, from source code to production.\n\nYour pipeline configuration is then visualized in the web UI, taking only one click to get from a red (failed) box to seeing why it failed.\n\nThe visualization also provides a \"gut check\" feedback loop - if it looks wrong, it probably is wrong.\n\nAll administration is done using the fly CLI. The fly set-pipeline command pushes the config up to Concourse. Once it looks good, you can then check the file in to source control. This makes it easy to recover if your Concourse server burns down.\n\nEverything runs in containers, ensuring a clean environment on every run. Each task specifies its own image, giving it full control over its dependencies, rather than managing them on your workers.\n\nThe fly intercept command will pop you right into one of your build's containers, which can be useful for debugging.\n\nThe fly execute command executes a task as a one-off build, with your local changes. This will run your code in exactly the same way it would run in your pipeline, without you having to repeatedly push broken commits until it works. Achieve the fabled green build #1!\n\nWhen a job fails, you can also use fly execute with -j flag to run with the same inputs as the failed job. You can then replace an input with your local changes with -i to test if your fix is valid.\n\nConcourse does not have a complex plugin system. Instead, it has a single strong abstraction.\n\nThe resources section of a pipeline lists Resources, which are abstract external locations where your pipeline will monitor for changes, fetch bits from, and push bits to.\n\nFor example, a resource with type git refers to a git repository, which will be cloned in a get step and pushed to in a put step. Behind the scenes, Concourse will continuously run git fetch to look for new commits that jobs may want to trigger on.\n\nAt its core, though, Concourse knows nothing about Git. It comes with a git resource type out of the box, but you could just as easily bring your own into your pipeline. Resource types are implemented as container images containing scripts - using the docker-image resource type, they can be fetched from a Docker registry.\n\n","depth":0,"section_tag":"index"},"input-name":{"location":"tasks.html#input-name","title":"inputs.name","text":"Required. The logical name of the input.\n\n","depth":2,"section_tag":"tasks"},"input-optional":{"location":"tasks.html#input-optional","title":"inputs.optional","text":"Optional. If true, then the input is not required by the task. The task may run even if this input is missing.\n\nAn optional input that is missing will not appear in the current directory of the running task.\n\n","depth":2,"section_tag":"tasks"},"input-path":{"location":"tasks.html#input-path","title":"inputs.path","text":"Optional. The path where the input will be placed. If not specified, the input's name is used.\n\nPaths are relative to the working directory of the task. Absolute paths are not respected.\n\n","depth":2,"section_tag":"tasks"},"input_mapping":{"location":"task-step.html#input_mapping","title":"input_mapping","text":"Optional. A map from task input names to concrete names in the build plan. This allows a task with generic input names to be used multiple times in the same plan, mapping its inputs to specific resources within the plan.\n\nFor example:\n\nplan:\n- get: diego-release\n- get: cf-release\n- get: ci-scripts\n- task: audit-diego-release\n  file: ci-scripts/audit-release.yml\n  input_mapping: {release-repo: diego-release}\n- task: audit-cf-release\n  file: ci-scripts/audit-release.yml\n  input_mapping: {release-repo: cf-release}\n","depth":4,"section_tag":"task-step"},"inputs_determined":{"location":"database-schema.html#inputs_determined","title":"inputs_determined","text":"The build scheduler does not schedule builds for jobs which have not yet had their inputs_determined.\n\n","depth":5,"section_tag":"pipelineapi-objects"},"install":{"location":"install.html","title":"Install","text":"A Concourse installation is composed of a Running a web node, a Running a worker node, and a Running a PostgreSQL node.\n\nThe web and worker nodes both run via a single concourse CLI. The concourse CLI is available from the Download page - be sure to grab the appropriate archive for your platform of choice.\n\nEach archive contains the following:\n\nconcourse/bin/concourse\nconcourse/bin/gdn            # on Linux\nconcourse/fly-assets/...\nconcourse/resource-types/... # on Linux\nWhen extracted, the concourse binary will auto-discover its sibling assets based on its file location, so you may extract it anywhere.\n\nOn Linux a typical install location is /usr/local/concourse:\n\ntar -zxf concourse-*.tgz -C /usr/local\nFrom there, you can either add /usr/local/concourse/bin to your $PATH, or just execute /usr/local/concourse/bin/concourse directly.\n\nConfiguration basics\n\nAll Concourse node configuration is defined statically via flags. For a full list of flags, you can pass --help to any command.\n\nEach flag can also be set via an environment variable. The env var for each flag is based on the flag name, preceded with CONCOURSE_. These are also shown in --help.\n\nVarious sections in documentation may refer to configuration via env vars rather than flags, but they are both equivalent an interchangeable. Env vars are just easier to reference in isolation and are more useful to copy-paste.\n\n","depth":2,"section_tag":"install"},"installation-and-configuration":{"location":"learning.html#installation-and-configuration","title":"Installation \u0026 Configuration","text":"Setting Up and Using Concourse CI on Ubuntu 16.04 by Digital Ocean\n\nGetting Started With Concourse on macOS by @osis\n\nConcourse Badges by Pivotal Engineering.\n\nConcourse with an HTTP/HTTPS Proxy by Pivotal Services. A guide on configuring your Concourse deployment in an environment with an HTTP/HTTPS proxy\n\nConcourse pipelines in an offline environment by Pivotal Services. A guide on configuring pipelines in an environment with limited or isolated access to the Internet\n\n","depth":3,"section_tag":"installation-and-configuration"},"installing":{"location":"install.html","title":"Install","text":"A Concourse installation is composed of a Running a web node, a Running a worker node, and a Running a PostgreSQL node.\n\nThe web and worker nodes both run via a single concourse CLI. The concourse CLI is available from the Download page - be sure to grab the appropriate archive for your platform of choice.\n\nEach archive contains the following:\n\nconcourse/bin/concourse\nconcourse/bin/gdn            # on Linux\nconcourse/fly-assets/...\nconcourse/resource-types/... # on Linux\nWhen extracted, the concourse binary will auto-discover its sibling assets based on its file location, so you may extract it anywhere.\n\nOn Linux a typical install location is /usr/local/concourse:\n\ntar -zxf concourse-*.tgz -C /usr/local\nFrom there, you can either add /usr/local/concourse/bin to your $PATH, or just execute /usr/local/concourse/bin/concourse directly.\n\nConfiguration basics\n\nAll Concourse node configuration is defined statically via flags. For a full list of flags, you can pass --help to any command.\n\nEach flag can also be set via an environment variable. The env var for each flag is based on the flag name, preceded with CONCOURSE_. These are also shown in --help.\n\nVarious sections in documentation may refer to configuration via env vars rather than flags, but they are both equivalent an interchangeable. Env vars are just easier to reference in isolation and are more useful to copy-paste.\n\n","depth":2,"section_tag":"install"},"integrations":{"location":"learning.html#integrations","title":"Integrations","text":"Vault Integration with Concourse by Pivotal Services. Describes Vault configurations for integration with a BOSH-deployed Concourse server.\n\nSecure credential automation with Vault by Pivotal Services. Examples of using Vault for credentials management in Concourse pipelines\n\nCredHub Integration with Concourse This guide provides samples of deployment manifests and pipelines for the integration of a BOSH-deployed Concourse server with CredHub\n\nConcourse integration with UAA by Pivotal Services. A guide on securing teams in Concourse using UAA as an identity provider\n\n","depth":3,"section_tag":"integrations"},"intercept-admin-only":{"location":"global-resources.html#intercept-admin-only","title":"Intercepting check containers is no longer safe","text":"Now that check containers are shared across teams, it would be dangerous to allow anyone to fly intercept to check containers. For this reason, this capability is limited to admin users.\n\nWe recognize that this will make it a bit more difficult for end users to debug things like failing checks. We plan to improve this by introducing a way to provision a new check container to facilitate debugging. See concourse #3344 for more information.\n\n","depth":5,"section_tag":"intercept-admin-only"},"internals":{"location":"architecture.html#internals","title":"Internals","text":"","depth":3,"section_tag":"internals"},"interruptible":{"location":"database-schema.html#interruptible","title":"interruptible","text":"Workers trying to land will wait until all builds are finished, unless the build is for an interruptible job.\n\n","depth":5,"section_tag":"pipelineapi-objects"},"job-build-logs-to-retain":{"location":"jobs.html#job-build-logs-to-retain","title":"build_logs_to_retain","text":"Optional. If configured, only the last specified number of builds will have their build logs persisted. This is useful if you have a job that runs periodically but after some amount of time the logs aren't worth keeping around.\n\nExample:\n\njobs:\n- name: smoke-tests\n  build_logs_to_retain: 100\n  plan:\n  - get: 10m\n  - task: smoke-tests\n    # ...\n","depth":2,"section_tag":"jobs"},"job-disable-manual-trigger":{"location":"jobs.html#job-disable-manual-trigger","title":"disable_manual_trigger","text":"Optional. Default false. If set to true, manual triggering of the job (via the web UI or fly trigger-job) will be disabled.\n\n","depth":2,"section_tag":"jobs"},"job-ensure":{"location":"jobs.html#job-ensure","title":"ensure","text":"Optional. Step to execute regardless of whether the job succeeds, fails, errors, or aborts. Equivalent to the ensure step attribute.\n\n","depth":2,"section_tag":"jobs"},"job-example":{"location":"job-example.html","title":"Job","text":"","depth":3,"section_tag":"job-example"},"job-interruptible":{"location":"jobs.html#job-interruptible","title":"interruptible","text":"Optional. Default false. Normally, when a worker is shutting down it will wait for builds with containers running on that worker to finish before exiting. If this value is set to true, the worker will not wait on the builds of this job. You may want this if e.g. you have a self-deploying Concourse or long-running-but-low-importance jobs.\n\n","depth":2,"section_tag":"jobs"},"job-max-in-flight":{"location":"jobs.html#job-max-in-flight","title":"max_in_flight","text":"Optional. If set, specifies a maximum number of builds to run at a time. If serial or serial_groups are set, they take precedence and force this value to be 1.\n\n","depth":2,"section_tag":"jobs"},"job-name":{"location":"jobs.html#job-name","title":"name","text":"Required. The name of the job. This should be short; it will show up in URLs.\n\n","depth":2,"section_tag":"jobs"},"job-on-abort":{"location":"jobs.html#job-on-abort","title":"on_abort","text":"Optional. Step to execute when the job aborts. Equivalent to the on_abort step attribute.\n\n","depth":2,"section_tag":"jobs"},"job-on-failure":{"location":"jobs.html#job-on-failure","title":"on_failure","text":"Optional. Step to execute when the job fails. Equivalent to the on_failure step attribute.\n\n","depth":2,"section_tag":"jobs"},"job-on-success":{"location":"jobs.html#job-on-success","title":"on_success","text":"Optional. Step to execute when the job succeeds. Equivalent to the on_success step attribute.\n\n","depth":2,"section_tag":"jobs"},"job-plan":{"location":"jobs.html#job-plan","title":"plan","text":"Required. The sequence of steps to execute.\n\n","depth":2,"section_tag":"jobs"},"job-public":{"location":"jobs.html#job-public","title":"public","text":"Optional. Default false. If set to true, the build log of this job will be viewable by unauthenticated users. Unauthenticated users will always be able to see the inputs, outputs, and build status history of a job. This is useful if you would like to expose your pipeline publicly without showing sensitive information in the build log.\n\n","depth":2,"section_tag":"jobs"},"job-serial":{"location":"jobs.html#job-serial","title":"serial","text":"Optional. Default false. If set to true, builds will queue up and execute one-by-one, rather than executing in parallel.\n\n","depth":2,"section_tag":"jobs"},"job-serial-groups":{"location":"jobs.html#job-serial-groups","title":"serial_groups","text":"Optional. Default []. When set to an array of arbitrary tag-like strings, builds of this job and other jobs referencing the same tags will be serialized.\n\nThis can be used to ensure that certain jobs do not run at the same time, like so:\n\njobs:\n- name: job-a\n  serial_groups: [some-tag]\n- name: job-b\n  serial_groups: [some-tag, some-other-tag]\n- name: job-c\n  serial_groups: [some-other-tag]\nIn this example, job-a and job-c can run concurrently, but neither job can run builds at the same time as job-b.\n\nThe builds are executed in their order of creation, across all jobs with common tags.\n\n","depth":2,"section_tag":"jobs"},"jobs":{"location":"jobs.html","title":"Jobs","text":"Jobs determine the actions of your pipeline. They determine how resources progress through it, and how the pipeline is visualized.\n\nThe most important attribute of a job is its build plan, configured as plan. This determines the sequence of Steps to execute in any builds of the job.\n\nJobs are listed under the jobs: key in the pipeline configuration. Each configured job consists of the following fields:\n\n","depth":2,"section_tag":"jobs"},"jobs-table":{"location":"database-schema.html#jobs-table","title":"jobs","text":"The jobs table tracks the details of all jobs in a Concourse deployment.  Jobs reference the pipeline they belong to, their Job configuration, and their name in the pipeline.\n\nThere are also a number of attributes which are used by the runtime:\n\n","depth":5,"section_tag":"pipelineapi-objects"},"ldap-auth":{"location":"ldap-auth.html","title":"LDAP auth","text":"The LDAP provider can be used for operators who wish to authenticate their users against an LDAP server.\n\n","depth":4,"section_tag":"ldap-auth"},"ldap-authentication":{"location":"ldap-auth.html#ldap-authentication","title":"Authentication","text":"The LDAP provider is configured by pointing it to an LDAP host with a read-only bind DN and password. This bind DN and password is used for authenticating with the LDAP host and querying the users.\n\nAdditionally, the base DN under which users are searched as well as the attribute of the users to associate to 'usernames' must also be configured.\n\nThese can be specified via env to the Running a web node like so:\n\nCONCOURSE_LDAP_DISPLAY_NAME=Acme # optional; default \"LDAP\"\nCONCOURSE_LDAP_HOST=ldap.example.com # port defaults to 389 or 636\nCONCOURSE_LDAP_BIND_DN='cn=read-only-admin,dc=example,dc=com'\nCONCOURSE_LDAP_BIND_PW=read-only-admin-password\nCONCOURSE_LDAP_USER_SEARCH_BASE_DN='cn=users,dc=example,dc=com'\nCONCOURSE_LDAP_USER_SEARCH_USERNAME=uid\nTo configure TLS, you may need to set a CA cert:\n\nCONCOURSE_LDAP_CA_CERT=/path/to/ca_cert\nIf your LDAP host does not use TLS, you must set:\n\nCONCOURSE_LDAP_INSECURE_NO_SSL=true\nTo fine-tune which users are queried, you can specify a user search filter like so:\n\nCONCOURSE_LDAP_USER_SEARCH_FILTER='(objectClass=person)'\nTo set which user attributes map to the token claims, you can set the following:\n\nCONCOURSE_LDAP_USER_SEARCH_ID_ATTR=uid         # default\nCONCOURSE_LDAP_USER_SEARCH_EMAIL_ATTR=mail     # default\nCONCOURSE_LDAP_USER_SEARCH_NAME_ATTR=some-attr # no default\n","depth":5,"section_tag":"ldap-authentication"},"ldap-authorization":{"location":"ldap-auth.html#ldap-authorization","title":"Authorization","text":"LDAP users and groups can be authorized for a team by passing the following flags to fly set-team:\n\n--ldap-user=USERNAME: Authorize an individual user.\n\n\n--ldap-group=GROUP_NAME: Authorize anyone from the group.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --ldap-user my-username \\\n    --ldap-group my-group\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  ldap:\n    users: [\"my-username\"]\n    groups: [\"my-groups\"]\n","depth":5,"section_tag":"ldap-authorization"},"learning":{"location":"learning.html","title":"Learning","text":"","depth":1,"section_tag":"learning"},"learning-concourse":{"location":"learning.html#learning-concourse","title":"Learning Concourse","text":"Concourse Tutorial by Stark \u0026 Wayne. A linear sequence of tutorials for learning how to use Concourse.\n\nHow To Set Up Continuous Integration Pipelines with Concourse CI on Ubuntu 16.04 by Digital Ocean\n\nConcourse Samples by Pivotal Services. This repository contains code samples and recipes for common Concourse pipelines and deployment patterns\n\nSpring Cloud Pipelines by Marcin Grzejszczak\n\nCreating a continuous integration pipeline in Concourse for a test-infused ASP.NET Core app by @rseroter\n\nConcourse caching for Java Maven and Gradle builds by @bijukunjummen\n\n","depth":3,"section_tag":"learning-concourse"},"local-auth":{"location":"local-auth.html","title":"Local User auth","text":"Local User auth is a primitive username/password-based auth mechanism. All users and passwords are configured statically.\n\nIn general, we recommend configuring one of the other providers instead, but for small deployments with only a few users, local user auth may be all you need.\n\n","depth":4,"section_tag":"local-auth"},"local-authentication":{"location":"local-auth.html#local-authentication","title":"Authentication","text":"Local users are configured on the Running a web node by setting the following env:\n\nCONCOURSE_ADD_LOCAL_USER=myuser:mypass,anotheruser:anotherpass\nThis configures two users, myuser and anotheruser, with their corresponding passwords.\n\nWhen local users are configured, the log-in page in the web UI will show a username/password prompt.\n\nLocal users can also log in via fly login with the --username and --password flags.\n\n","depth":5,"section_tag":"local-authentication"},"local-authorization":{"location":"local-auth.html#local-authorization","title":"Authorization","text":"Local users are granted access to teams via fly set-team, using the --local-user flag:\n\n$ fly set-team -n my-team \\\n    --local-user some_username\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  local:\n    users: [\"some_username\"]\n","depth":5,"section_tag":"local-authorization"},"main-team":{"location":"main-team.html","title":"The main team","text":"Out of the box, Concourse comes with a single team called main.\n\nThe main team is an admin team, meaning members (specifically, users with the owner role) can create and update other teams. Currently there is no way to promote a team to become an admin team, so main is a special-case.\n\nThe main team is different in that all flags normally passed to fly set-team are instead passed to the concourse web command, prefixed with --main-team-. The values set in these flags take effect whenever the web node starts up. This is done so that you can't get locked out.\n\nTo learn how to configure your main team, continue on to the appropriate section for your auth provider of choice under Configuring Auth.\n\n","depth":3,"section_tag":"main-team"},"managing-jobs":{"location":"managing-jobs.html","title":"Managing Jobs","text":"","depth":3,"section_tag":"managing-jobs"},"managing-pipelines":{"location":"managing-pipelines.html","title":"Managing Pipelines","text":"","depth":3,"section_tag":"managing-pipelines"},"managing-resource-types":{"location":"managing-resource-types.html","title":"Managing Resource Types","text":"","depth":4,"section_tag":"managing-resource-types"},"managing-resources":{"location":"managing-resources.html","title":"Managing Resources","text":"","depth":3,"section_tag":"managing-resources"},"managing-teams":{"location":"managing-teams.html","title":"Managing Teams","text":"","depth":3,"section_tag":"managing-teams"},"max_in_flight_reached":{"location":"database-schema.html#max_in_flight_reached","title":"max_in_flight_reached","text":"The build scheduler does not schedule builds for jobs that have had their build max_in_flight_reached.\n\n","depth":5,"section_tag":"pipelineapi-objects"},"metrics":{"location":"metrics.html","title":"Metrics","text":"Metrics are essential in understanding how any large system is behaving and performing. Concourse can emit metrics about both the system health itself and about the builds that it is running. Operators can tap into these metrics in order to observe the health of the system.\n\nIn the spirit of openness, the metrics from our deployment are public. We consider it a bug to emit anything sensitive or secret into our metrics pipeline.\n\n","depth":3,"section_tag":"metrics"},"nodejs-example":{"location":"nodejs-example.html","title":"Nodejs Application","text":"","depth":3,"section_tag":"nodejs-example"},"on-abort-step-hook":{"location":"on-abort-step-hook.html","title":"on_abort step hook","text":"Any step can have on_abort tacked onto it, whose value is a second step to execute only if the parent step aborts.\n\n","depth":4,"section_tag":"on-abort-step-hook"},"on-failure-step-hook":{"location":"on-failure-step-hook.html","title":"on_failure step hook","text":"Any step can have on_failure tacked onto it, whose value is a second step to execute only if the parent step fails.\n\n","depth":4,"section_tag":"on-failure-step-hook"},"on-success-step-hook":{"location":"on-success-step-hook.html","title":"on_success step hook","text":"Any step can have on_success tacked onto it, whose value is a second step to execute only if the parent step succeeds.\n\n","depth":4,"section_tag":"on-success-step-hook"},"on_abort":{"location":"on-abort-step-hook.html#on_abort","title":"on_abort","text":"The step to execute when the parent step aborts. If the attached step succeeds, the entire step is still aborted.\n\n","depth":4,"section_tag":"on-abort-step-hook"},"on_failure":{"location":"on-failure-step-hook.html#on_failure","title":"on_failure","text":"The step to execute when the parent step fails. If the attached step succeeds, the entire step is still failed.\n\n","depth":4,"section_tag":"on-failure-step-hook"},"on_success":{"location":"on-success-step-hook.html#on_success","title":"on_success","text":"The step to execute when the parent step succeeds. If the attached step fails, the outer step is considered to have failed.\n\n","depth":4,"section_tag":"on-success-step-hook"},"operation":{"location":"operation.html","title":"Operation","text":"The following sections are available to provide a deeper understanding of some of the concepts surrounding Concourse. They aren't necessary for using Concourse and understanding its general concept of pipelines and how to use them, but we do recommend at least learning how to set up Credential Management and Encryption.\n\n","depth":2,"section_tag":"operation"},"out":{"location":"implementing-resource-types.html#out","title":"out: Update a resource.","text":"The out script is called with a path to the directory containing the build's full set of sources as the first argument, and is given on stdin the configured params and the resource's source configuration.\n\nThe script must emit the resulting version of the resource. For example, the git resource emits the sha of the commit that it just pushed.\n\nAdditionally, the script may emit metadata as a list of key-value pairs. This data is intended for public consumption and will make it upstream, intended to be shown on the build's page.\n\nThe request will contain the following fields:\n\n* source is the same value as passed to check: Check for new versions..\n\n* params is an arbitrary JSON object passed along verbatim from params on a put step.\n\nExample request, in this case for the git resource:\n\n{\n  \"params\": {\n    \"branch\": \"develop\",\n    \"repo\": \"some-repo\"\n  },\n  \"source\": {\n    \"uri\": \"git@...\",\n    \"private_key\": \"...\"\n  }\n}\nUpon receiving this payload the git resource would probably do something like:\n\ncd $1/some-repo\ngit push origin develop\nAnd output:\n\n{\n  \"version\": { \"ref\": \"61cebf\" },\n  \"metadata\": [\n    { \"name\": \"commit\", \"value\": \"61cebf\" },\n    { \"name\": \"author\", \"value\": \"Mick Foley\" }\n  ]\n}\n","depth":5,"section_tag":"out"},"output-name":{"location":"tasks.html#output-name","title":"outputs.name","text":"Required. The logical name of the output. The contents under path will be made available to the rest of the plan under this name.\n\n","depth":2,"section_tag":"tasks"},"output-path":{"location":"tasks.html#output-path","title":"outputs.path","text":"Optional. The path to a directory where the output will be taken from. If not specified, the output's name is used.\n\nPaths are relative to the working directory of the task. Absolute paths are not respected.\n\nNote that this value must not overlap with any other inputs or outputs. Each output results in a new empty directory that your task should place artifacts in; if the path overlaps it'll clobber whatever files used to be there.\n\n","depth":2,"section_tag":"tasks"},"output_mapping":{"location":"task-step.html#output_mapping","title":"output_mapping","text":"Optional. A map from task output names to concrete names to register in the build plan. This allows a task with generic output names to be used multiple times in the same plan.\n\nThis is often used together with input_mapping. For example:\n\nplan:\n- get: diego-release\n- get: cf-release\n- get: ci-scripts\n- task: create-diego-release\n  file: ci-scripts/create-release.yml\n  input_mapping: {release-repo: diego-release}\n  output_mapping: {release-tarball: diego-release-tarball}\n- task: create-cf-release\n  file: ci-scripts/create-release.yml\n  input_mapping: {release-repo: cf-release}\n  output_mapping: {release-tarball: cf-release-tarball}\n","depth":4,"section_tag":"task-step"},"paused":{"location":"database-schema.html#paused","title":"paused","text":"The build scheduler does not schedule builds for jobs which are paused.\n\n","depth":5,"section_tag":"pipelineapi-objects"},"permission-matrix":{"location":"user-roles.html#permission-matrix","title":"Permission Matrix","text":"The fly CLI commands\n\n| Command | Anon | Admin | Owner | Member | Viewer |\n| abort-build | ✘ |  | ✓ | ✓ | ✘ |\n| builds | ✓ |  | ✓ | ✓ | ✓ |\n| check-resource | ✘ |  | ✓ | ✓ | ✘ |\n| checklist | ✘ |  | ✓ | ✓ | ✘ |\n| containers | ✘ |  | ✓ | ✓ | ✘ |\n| destroy-pipeline | ✘ |  | ✓ | ✓ | ✘ |\n| destroy-team | ✘ | ✓ | ✘ | ✘ | ✘ |\n| execute | ✘ |  | ✓ | ✓ | ✘ |\n| expose-pipeline | ✘ |  | ✓ | ✓ | ✘ |\n| format-pipeline | ✘ |  | ✓ | ✓ | ✘ |\n| get-pipeline | ✘ |  | ✓ | ✓ | ✘ |\n| help | ✓ |  | ✓ | ✓ | ✓ |\n| hide-pipeline | ✘ |  | ✓ | ✓ | ✘ |\n| hijack | ✘ |  | ✓ | ✓ | ✘ |\n| jobs | ✓ |  | ✓ | ✓ | ✓ |\n| login | ✓ |  |  |  |  |\n| logout | ✘ |  | ✓ | ✓ | ✓ |\n| order-pipelines | ✘ |  | ✓ | ✓ | ✘ |\n| pause-job | ✘ |  | ✓ | ✓ | ✘ |\n| pause-pipeline | ✘ |  | ✓ | ✓ | ✘ |\n| pipelines | ✓ |  | ✓ | ✓ | ✓ |\n| prune-worker | ✘ |  | ✓ | ✓ | ✘ |\n| rename-pipeline | ✘ |  | ✓ | ✓ | ✘ |\n| rename-team | ✘ | ✓ | ✘ | ✘ | ✘ |\n| set-pipeline | ✘ |  | ✓ | ✓ | ✘ |\n| set-team | ✘ | ✓ | ✓ | ✘ | ✘ |\n| status | ✓ |  | ✓ | ✓ | ✓ |\n| sync | ✓ |  | ✓ | ✓ | ✓ |\n| targets | ✓ |  | ✓ | ✓ | ✓ |\n| teams | ✘ | ✓ | ✓ | ✓ | ✓ |\n| trigger-job | ✘ |  | ✓ | ✓ | ✘ |\n| unpause-job | ✘ |  | ✓ | ✓ | ✘ |\n| unpause-pipeline | ✘ |  | ✓ | ✓ | ✘ |\n| validate-pipeline | ✓ |  | ✓ | ✓ | ✓ |\n| volumes | ✘ |  | ✓ | ✓ | ✘ |\n| watch | ✓* |  | ✓ | ✓ | ✓ |\n| workers | ✘ |  | ✓ | ✓ | ✘ |\n\nWeb UI\n\n| Page | Action | Owner | Member | Viewer |\n| Home (HD/Dashboard) | View | ✓ | ✓ | ✓ |\n|  | Login | ✘ | ✘ | ✘ |\n|  | Logout | ✓ | ✓ | ✓ |\n|  | Download The fly CLI | ✓ | ✓ | ✓ |\n|  | Pause Pipeline | ✓ | ✓ | ✘ |\n|  | Resume Pipeline | ✓ | ✓ | ✘ |\n|  | Reorder Pipeline | ✓ | ✓ | ✘ |\n| Pipeline Page | View | ✓ | ✓ | ✓ |\n|  | Click to Resource | ✓ | ✓ | ✓ |\n|  | Click to Build | ✓ | ✓ | ✓ |\n|  | Click on Group | ✓ | ✓ | ✓ |\n| Resource Page | View Resource | ✓ | ✓ | ✓ |\n|  | View Version Details | ✓ | ✓ | ✓ |\n|  | Pin Version | ✓ | ✓ | ✘ |\n|  | Paginate (\u003c- -\u003e) | ✓ | ✓ | ✓ |\n| Build Page | Trigger new Build | ✓ | ✓ | ✘ |\n|  | View Build | ✓ | ✓ | ✓ |\n|  | Build Details | ✓ | ✓ | ✓ |\n| Job Page | View Job Page | ✓ | ✓ | ✓ |\n|  | Pause Job | ✓ | ✓ | ✘ |\n|  | Trigger new Build | ✓ | ✓ | ✘ |\n|  | Build History | ✓ | ✓ | ✓ |\n|  | Paginate (\u003c- -\u003e) | ✓ | ✓ | ✓ |\n\n","depth":4,"section_tag":"permission-matrix"},"php-example":{"location":"php-example.html","title":"PHP Application","text":"","depth":3,"section_tag":"php-example"},"pipeline":{"location":"php-example.html#pipeline","title":"","text":"You can run the tests for a PHP application.\n\n","depth":4,"section_tag":"pipeline"},"pipeline-groups":{"location":"pipeline-groups.html","title":"Grouping Jobs","text":"As more resources and jobs are added to a pipeline it can become difficult to navigate in the web UI. Pipeline groups allow you to group jobs together under a header and have them show on different tabs in the user interface. Groups have no functional effect on your pipeline.\n\nNote: once you have added groups to your pipeline then all jobs must be in a group.\n\nEach group in the pipeline has the following attributes:\n\n","depth":3,"section_tag":"pipeline-groups"},"pipeline-jobs":{"location":"pipelines.html#pipeline-jobs","title":"jobs","text":"A set of jobs for the pipeline to continuously check.\n\n","depth":2,"section_tag":"pipelines"},"pipeline-resource-types":{"location":"pipelines.html#pipeline-resource-types","title":"resource_types","text":"A set of resource types for resources within the pipeline to use.\n\n","depth":2,"section_tag":"pipelines"},"pipeline-resources":{"location":"pipelines.html#pipeline-resources","title":"resources","text":"A set of resources for the pipeline to continuously check.\n\n","depth":2,"section_tag":"pipelines"},"pipeline-vars":{"location":"setting-pipelines.html#pipeline-vars","title":"Pipeline ((vars))","text":"The pipeline configuration can contain template variables in the form of ((foo-bar)). They will be replaced with values populated by repeated --var, --yaml-var, or --load-vars-from flags, or at runtime via a credential manager.\n\nThis allows for credentials to be extracted from a pipeline config, making it safe to check in to a public repository or pass around.\n\nFor example, if you have a pipeline.yml as follows:\n\nresources:\n- name: private-repo\n  type: git\n  source:\n    uri: git@...\n    branch: master\n    private_key: ((private-repo-key))\n...you could then configure this pipeline like so:\n\n$ fly -t example set-pipeline \\\n    --pipeline my-pipeline \\\n    --config pipeline.yml \\\n    --var \"private-repo-key=$(cat id_rsa)\"\nOr, if you had a credentials.yml as follows:\n\nprivate-repo-key: |\n  -----BEGIN RSA PRIVATE KEY-----\n  ...\n  -----END RSA PRIVATE KEY-----\n...you could configure it like so:\n\n$ fly -t example set-pipeline \\\n    --pipeline my-pipeline \\\n    --config pipeline.yml \\\n    --load-vars-from credentials.yml\nConcatenation is supported for string values - foo-((var)) with var: bar will resolve to foo-bar.\n\nIf both --var/--yaml-var and --load-vars-from are specified, the --var and --yaml-var flags take precedence.\n\nValues other than strings (e.g. bools, arrays) may also be specified by using either --yaml-var or --load-vars-from.\n\n","depth":5,"section_tag":"pipeline-vars"},"pipelineapi-objects":{"location":"database-schema.html#pipelineapi-objects","title":"Pipeline/API Objects","text":"The following tables manage objects exposed directly to users, either via the API or by configuring pipelines.\n\n","depth":5,"section_tag":"pipelineapi-objects"},"pipelines":{"location":"pipelines.html","title":"Pipelines","text":"A pipeline is the result of configuring Jobs and Resources together. When you configure a pipeline, it takes on a life of its own, to continuously detect resource versions and automatically queue new builds for jobs as they have new available inputs.\n\nPipelines are configured as declarative YAML files, fitting the following schema:\n\n","depth":2,"section_tag":"pipelines"},"pipelines-table":{"location":"database-schema.html#pipelines-table","title":"pipelines","text":"The pipelines table contains details about a pipeline; its name, which team it belongs to, whether it is paused, and whether it is public.\n\n","depth":5,"section_tag":"pipelineapi-objects"},"pivotal":{"location":"thanks.html#pivotal","title":"Pivotal","text":"Concourse wouldn't be what it is today without Pivotal. This goes beyond the sponsorship, which began in early 2015 - without the experiences we had and the practices we learned while working on Cloud Foundry and BOSH, we would have neither the technical experience nor the strong opinions that led to Concourse being born.\n\nPivotal's sponsorship continues strong into 2019, where we have a team of full-time engineers, PMs, and designers dedicated to pushing Concourse forward.\n\n","depth":3,"section_tag":"pivotal"},"plugins":{"location":"community.html#plugins","title":"Plugins","text":"concourse-vis package, an Atom plugin to preview Concourse pipelines.\n\nConcourse CI Pipeline Editor, a VSCode plugin to help with editing Concourse pipelines.\n\n","depth":3,"section_tag":"plugins"},"postgresql-node":{"location":"postgresql-node.html","title":"Running a PostgreSQL node","text":"Concourse uses PostgreSQL for storing all data and coordinating work in a multi-Running a web node installation.\n\n","depth":3,"section_tag":"postgresql-node"},"project":{"location":"project.html","title":"Project","text":"Concourse started as a side-project by @vito (hi!) and @xoebus in 2014. Since then, Concourse has grown into a small but dedicated team of full-time engineers and part-time contributors.\n\nWhy make Concourse?\n\nWhen working on a sizable project, having a pipeline to reliably test, deploy, and publish the product is crucial for rapid iteration.\n\nBut with every CI system we used, we found ourselves constantly dealing with the same old problems: complicated configs hidden in many pages of the web UI, not knowing who changed what \u0026 when, managing dependencies and state on the workers, build pollution, annoying UX...\n\nOur project was growing larger, and with every box we checked and for every worker we hand-tweaked, the anxiety of having to do it all over again if something went wrong grew and grew. We started writing software to manage our CI instead of writing the software for the product we wanted to build.\n\nWe built Concourse to be a CI system that lets you sleep easier at night. A CI that's simple enough to fully grok and easy to manage as your project grows; in both the complexity of the product and the size of your team. We wanted to build a CI with strong abstractions and fewer things to learn, so that it can be easier to understand and so that Concourse can age gracefully.\n\nHow can I help?\n\nIt's pretty hard to write a CI system that makes everyone happy! Concourse is by no means perfect, and it sometimes takes us while to understand a problem space well enough to figure out how it should work in Concourse's puritanical world.\n\nWe tend to move slowly rather than tack on feature request after feature request. We are also extremely cautious about anti-patterns and introducing ways for users to shoot themselves in the foot.\n\nConcourse is getting bigger and bigger, and we really appreciate any help we can get. There are many ways to contribute to Concourse; only some of them involve writing code! You help us out by being active in GitHub issues, voting with reactions for issues that matter to you, engaging in discussions while we map out a feature request, hanging out in our forums, writing documentation, coming up with new designs, and of course contributing code!\n\nAs a contributor, you can jump on over to Contribute for a more complete list of resources for contributors\n\n","depth":1,"section_tag":"project"},"providing-multiple-inputs":{"location":"running-tasks.html#providing-multiple-inputs","title":"Providing multiple inputs","text":"Tasks in Concourse can take multiple inputs. Up until now we've just been submitting a single input (our current working directory) that has the same name as the directory.\n\nTasks must specify the inputs that they require as inputs. For fly to upload these inputs you can use the -i or --input arguments with name and path pairs. For example:\n\n$ fly -t example execute \\\n    --config build-stemcell.yml \\\n    --input code=. \\\n    --input stemcells=../stemcells\nThis would work together with a build-stemcell.yml if its inputs: section was as follows:\n\ninputs:\n- name: code\n- name: stemcells\nIf you specify an input then the default input will no longer be added automatically and you will need to explicitly list it (as with the code input above).\n\nThis feature can be used to mimic other resources and try out combinations of input that would normally not be possible in a pipeline.\n\n","depth":5,"section_tag":"providing-multiple-inputs"},"providing-values-for-params":{"location":"running-tasks.html#providing-values-for-params","title":"Providing values for params","text":"Any params listed in the task configuration can be specified by using environment variables.\n\nSo, if you have a task with the following params:\n\nparams:\n  FOO: fizzbuzz\n  BAR:\n...and you run:\n\nBAR=hello fly execute\nThe task would then run with BAR as \"hello\", and FOO as \"fizzbuzz\" (its default value).\n\n","depth":5,"section_tag":"providing-values-for-params"},"put-step":{"location":"put-step.html","title":"put step","text":"Pushes to the given Resource. All artifacts collected during the plan's execution will be available in the working directory.\n\nWhen the put succeeds, the produced version of the resource will be immediately fetched via an implicit get step. This is so that later steps in your plan can use the artifact that was produced. The source will be available under whatever name put specifies, just like as with get.\n\n","depth":4,"section_tag":"put-step"},"put-step-get-params":{"location":"put-step.html#put-step-get-params","title":"get_params","text":"Optional. A map of arbitrary configuration to forward to the resource that will be utilized during the implicit get step.  Refer to the resource type's documentation to see what it supports.\n\n","depth":4,"section_tag":"put-step"},"put-step-inputs":{"location":"put-step.html#put-step-inputs","title":"inputs","text":"Optional. If specified, only the listed artifacts will be provided to the container. If not specified, all artifacts will be provided.\n\n","depth":4,"section_tag":"put-step"},"put-step-params":{"location":"put-step.html#put-step-params","title":"params","text":"Optional. A map of arbitrary configuration to forward to the resource. Refer to the resource type's documentation to see what it supports.\n\n","depth":4,"section_tag":"put-step"},"put-step-put":{"location":"put-step.html#put-step-put","title":"put","text":"Required. The logical name of the resource being pushed. The pushed resource will be available under this name after the push succeeds.\n\n","depth":4,"section_tag":"put-step"},"put-step-resource":{"location":"put-step.html#put-step-resource","title":"resource","text":"Optional. Defaults to name. The resource to update, as configured in resources.\n\n","depth":4,"section_tag":"put-step"},"rails-example":{"location":"rails-example.html","title":"Rails Application","text":"","depth":3,"section_tag":"rails-example"},"random-strategy":{"location":"container-placement.html#random-strategy","title":"The random strategy","text":"With the random strategy, the Running a web node places get, put and task containers on any worker, ignoring any affinity.\n\nAs this is truly random, this will be fine until one day it's not fine.\n\nTo use this strategy, set the following env var on the Running a web node:\n\nCONCOURSE_CONTAINER_PLACEMENT_STRATEGY=random\n","depth":4,"section_tag":"random-strategy"},"reducing-redundant-data":{"location":"global-resources.html#reducing-redundant-data","title":"Reducing redundant data","text":"The majority of Concourse resources will benefit from having versions shared globally because most resource versions have an external source of truth.\n\nFor example, a check for the git resource that pulls in the concourse/concourse repository will always return the same set of versions as an equivalent resource pointing to the same repository. By consolidating the checks and the versions, there will essentially only be one set of versions collected from the repository and saved into the database.\n\n","depth":5,"section_tag":"reducing-redundant-data"},"references":{"location":"php-example.html#references","title":"References","text":"* Jobs\n\n* Steps\n\n* Tasks\n\n","depth":4,"section_tag":"references"},"reliable-resource-version-history":{"location":"global-resources.html#reliable-resource-version-history","title":"Reliable Resource Version History","text":"Prior to global resources, a resource's version history was directly associated to the resource name. This meant that any changes to a resource's configuration without changing its name would basically append the versions from the new configuration after the old versions, which are no longer accurate to the current configuration.\n\nGlobal resources instead associates the resource versions to the resource's type and source. Therefore, whenever a resource definition changes, the versions will \"reset\" and change along with it, resulting in truthful and reliable version histories.\n\n","depth":5,"section_tag":"reliable-resource-version-history"},"resource-certs":{"location":"implementing-resource-types.html#resource-certs","title":"Certificate Propagation","text":"Certificates can be automatically propagated into each resource container, if the worker is configured to do so. The BOSH release configures this automatically, while the concourse binary must be given a --certs-dir flag pointing to the path containing the CA certificate bundle.\n\nThe worker's certificate directory will then be always mounted at /etc/ssl/certs, read-only, in each resource container created on the worker. There's no single standard path for this so we picked one that would work out of the box in most cases.\n\nThis approach to certificate configuration is similar in mindset to the propagation of http_proxy/https_proxy - certs are kind of a baseline assumption when deploying software, so Concourse should do its best to respect it out-of-the-box, especially as they're often used in tandem with a man-in-the-middle corporate SSL proxy. (In this way it doesn't feel too much like the anti-pattern of hand-tuning workers.)\n\n","depth":5,"section_tag":"resource-certs"},"resource-check":{"location":"implementing-resource-types.html#resource-check","title":"check: Check for new versions.","text":"A resource type's check script is invoked to detect new versions of the resource. It is given the configured source and current version on stdin, and must print the array of new versions, in chronological order, to stdout, including the requested version if it's still valid.\n\nThe request body will have the following fields:\n\n* source is an arbitrary JSON object which specifies the location of the resource, including any credentials. This is passed verbatim from the resource configuration.\n\n  For git this would be the repo URI, which branch, and a private key if necessary.\n\n* version is a JSON object with string fields, used to uniquely identify an instance of the resource. For git this would be a commit SHA.\n\n  This will be omitted from the first request, in which case the resource should return the current version (not every version since the resource's inception).\n\nFor example, here's what the input for a git resource may look like:\n\n{\n  \"source\": {\n    \"uri\": \"git://some-uri\",\n    \"branch\": \"develop\",\n    \"private_key\": \"...\"\n  },\n  \"version\": { \"ref\": \"61cebf\" }\n}\nUpon receiving this payload the git resource would probably do something like:\n\n[ -d /tmp/repo ] || git clone git://some-uri /tmp/repo\ncd /tmp/repo\ngit pull \u0026\u0026 git log 61cbef..HEAD\nNote that it conditionally clones; the container for checking versions is reused between checks, so that it can efficiently pull rather than cloning every time.\n\nAnd the output, assuming d74e01 is the commit immediately after 61cbef:\n\n[\n  { \"ref\": \"61cbef\" },\n  { \"ref\": \"d74e01\" },\n  { \"ref\": \"7154fe\" }\n]\nThe list may be empty, if there are no versions available at the source. If the given version is already the latest, an array with that version as the sole entry should be listed.\n\nIf your resource is unable to determine which versions are newer then the given version (e.g. if it's a git commit that was push -fed over), then the current version of your resource should be returned (i.e. the new HEAD).\n\n","depth":5,"section_tag":"resource-check"},"resource-check-container":{"location":"container-internals.html#resource-check-container","title":"Resource Check Container","text":"Resource Check Containers are created from the resource type's image and are used to check for new versions of a resource.  There will be one per resource config.\n\n","depth":6,"section_tag":"resource-check-container"},"resource-check-every":{"location":"resources.html#resource-check-every","title":"check_every","text":"Optional. Default 1m. The interval on which to check for new versions of the resource. Acceptable interval options are defined by the time.ParseDuration function.\n\n","depth":2,"section_tag":"resources"},"resource-get-container":{"location":"container-internals.html#resource-get-container","title":"Resource Get Container","text":"Resource Get Container are created when a get step is executed in a build plan. They are based on the resource type's image and are used to download the bits for a given version of resource.\n\nThere will be one per resource config.\n\n","depth":6,"section_tag":"resource-get-container"},"resource-metadata":{"location":"implementing-resource-types.html#resource-metadata","title":"Metadata","text":"When used in a get step or a put step, metadata about the running build is made available via the following environment variables:\n\n$BUILD_ID: The internal identifier for the build. Right now this is numeric but it may become a guid in the future. Treat it as an absolute reference to the build.\n\n\n$BUILD_NAME: The build number within the build's job.\n\n\n$BUILD_JOB_NAME: The name of the build's job.\n\n\n$BUILD_PIPELINE_NAME: The pipeline that the build's job lives in.\n\n\n$BUILD_TEAM_NAME: The team that the build belongs to.\n\n\n$ATC_EXTERNAL_URL: The public URL for your ATC; useful for debugging.\n\n\n\nIf the build is a one-off, $BUILD_NAME, $BUILD_JOB_NAME, and $BUILD_PIPELINE_NAME will not be set.\n\nNone of these variables are available to /check.\n\nThese variables should be used solely for annotating things with metadata for traceability, i.e. for linking to the build in an alert or annotating an automated commit so its origin can be discovered.\n\nThey should not be used to emulate versioning (e.g. by using the increasing build number). They are not provided to task steps to avoid this anti-pattern.\n\n","depth":5,"section_tag":"resource-metadata"},"resource-name":{"location":"resources.html#resource-name","title":"name","text":"Required. The name of the resource. This should be short and simple. This name will be referenced by build plans of jobs in the pipeline.\n\n","depth":2,"section_tag":"resources"},"resource-retention":{"location":"caching-and-retention.html#resource-retention","title":"Resource Retention","text":"","depth":5,"section_tag":"resource-retention"},"resource-source":{"location":"resources.html#resource-source","title":"source","text":"Optional. The location of the resource. This varies by resource type, and is a black box to Concourse; it is blindly passed to the resource at runtime.\n\nTo use git as an example, the source may contain the repo URI, the branch of the repo to track, and a private key to use when pushing/pulling.\n\nBy convention, documentation for each resource type's configuration is in each implementation's README.\n\nYou can find the source for the resource types provided with Concourse at the Concourse GitHub organization.\n\n","depth":2,"section_tag":"resources"},"resource-tags":{"location":"resources.html#resource-tags","title":"tags","text":"Optional. Default []. A list of tags to determine which workers the checks will be performed on. You'll want to specify this if the source is internal to a worker's network, for example. See also tags step modifier.\n\n","depth":2,"section_tag":"resources"},"resource-type":{"location":"resources.html#resource-type","title":"type","text":"Required. The type of the resource.\n\nResource types are available from two sources:\n\n* Resource Types defined in the same pipeline. In this case the string value refers to the name.\n\n* Linux workers are pre-loaded with a set of \"base\" resource types, available to all pipelines that run on them. This is visible in fly workers -d.\n\n","depth":2,"section_tag":"resources"},"resource-type-check-every":{"location":"resource-types.html#resource-type-check-every","title":"check_every","text":"Optional. Default 1m. The interval on which to check for new versions of the resource type. Acceptable interval options are defined by the time.ParseDuration function.\n\n","depth":3,"section_tag":"resource-types"},"resource-type-name":{"location":"resource-types.html#resource-type-name","title":"name","text":"Required. The name of the new resource type. This should be short and simple. This name will be referenced by resources defined within the same pipeline, and image_resources used by tasks running in the pipeline.\n\n","depth":3,"section_tag":"resource-types"},"resource-type-params":{"location":"resource-types.html#resource-type-params","title":"params","text":"Optional. Arbitrary params to pass when fetching the resource.\n\n","depth":3,"section_tag":"resource-types"},"resource-type-privileged":{"location":"resource-types.html#resource-type-privileged","title":"privileged","text":"Optional. Default false. If set to true, the resource's containers will be run with full capabilities, as determined by the Garden backend the task runs on. For Linux-based backends it typically determines whether or not the container will run in a separate user namespace, and whether the root user is \"actual\" root (if set to true) or a user namespaced root (if set to false, the default).\n\nThis is a gaping security hole; only configure it if the resource type needs it (which should be called out in its documentation). This is not up to the resource type to decide dynamically, so as to prevent privilege escalation via third-party resource type exploits.\n\n","depth":3,"section_tag":"resource-types"},"resource-type-source":{"location":"resource-types.html#resource-type-source","title":"source","text":"Optional. The location of the resource type's resource. This varies by resource type, and is a black box to Concourse; it is blindly passed to the resource at runtime.\n\nTo use docker-image as an example, the source would contain something like repository: username/reponame. See the Docker Image resource (or whatever resource type your resource type uses) for more information.\n\n","depth":3,"section_tag":"resource-types"},"resource-type-tags":{"location":"resource-types.html#resource-type-tags","title":"tags","text":"Optional. Default []. A list of tags to determine which workers the checks will be performed on. You'll want to specify this if the source is internal to a worker's network, for example. See also tags step modifier.\n\n","depth":3,"section_tag":"resource-types"},"resource-type-type":{"location":"resource-types.html#resource-type-type","title":"type","text":"Required. The type of the resource used to provide the resource type's container image. Yes, this is a bit meta. Usually this will be docker-image, as the resource type must result in a container image, though there may be other image formats (possibly themselves defined as pipeline resource types!).\n\n","depth":3,"section_tag":"resource-types"},"resource-type-unique-version-history":{"location":"resource-types.html#resource-type-unique-version-history","title":"unique_version_history","text":"Optional. Default false. Only relevant when Global Resources (experimental) is enabled. When set to true, resources using this resource type will have a version history that is unique to the resource, rather than sharing a global version history.\n\n","depth":3,"section_tag":"resource-types"},"resource-types":{"location":"resource-types.html","title":"Resource Types","text":"Each resource in a pipeline has a type. The resource's type determines what versions are detected, the bits that are fetched when used for a get step, and the side effect that occurs when used for a put step.\n\nOut of the box, Concourse comes with a few resource types to cover common CI use cases like dealing with Git repositories and S3 buckets.\n\nBeyond these core types, each pipeline can configure its own resource types by specifying resource_types at the top level. Each resource type is itself defined as a resource that provides the container image for the pipeline resource type (see Implementing a Resource Type). You will almost always be using the registry-image resource when doing this.\n\nPipeline-provided resource types can override the core resource types, and can be defined in terms of each other. Also, a pipeline resource type can use the core type that it's overriding. This is useful if you want to e.g. provide your own custom docker-image resource, by overriding the core one (and using it one last time for the override itself), and then using it for all other pipeline resource types.\n\nSimilar to Resources, each configured resource type consists of the following attributes:\n\n","depth":3,"section_tag":"resource-types"},"resource-version":{"location":"resources.html#resource-version","title":"version","text":"Optional. A version to pin the resource to across the pipeline. This has the same effect as setting version on every get step referencing the resource.\n\nResources can also be temporarily pinned to a version via the API and web UI. However this functionality is disabled if the resource is pinned via configuration, and if a pipeline is configured to have a version pinned while also pinned in the web UI, the configuration takes precedence and will clear out the temporary pin.\n\n","depth":2,"section_tag":"resources"},"resource-webhook-token":{"location":"resources.html#resource-webhook-token","title":"webhook_token","text":"Optional. If specified, web hooks can be sent to trigger an immediate check of the resource, specifying this value as a primitive form of authentication via query params.\n\nAfter configuring this value, you would then configure your hook sender with the following painfully long path appended to your external URL:\n\n/api/v1/teams/TEAM_NAME/pipelines/PIPELINE_NAME/resources/RESOURCE_NAME/check/webhook?webhook_token=WEBHOOK_TOKEN\nNote that the request payload sent to this API endpoint is entirely ignored. You should configure the resource as if you're not using web hooks, as the resource config is still the \"source of truth.\"\n\n","depth":2,"section_tag":"resources"},"resource_cache_uses-table":{"location":"database-schema.html#resource_cache_uses-table","title":"resource_cache_uses","text":"A cache use is a join table between a resource cache and a 'user', which lets us know when the use is no longer needed. Think of it as a reference counter.\n\nA use can be tied to a builds, in which case the use can be removed when the build is finished. A cache is in use by a build when the build is fetching it as an input. This is to prevent the garbage collector from reaping caches that are no longer desired in the long run, but are still in use by a build.\n\nA use can be tied to a containers, in which case the use can be removed when the container goes away. A cache is in use by a container when it's being used as the container's image, either via image_resource or a resource type.\n\n","depth":5,"section_tag":"abstract-objects"},"resource_caches-table":{"location":"database-schema.html#resource_caches-table","title":"resource_caches","text":"A resource cache represents all information necessary to fetch a particular set of bits, i.e. the cache.\n\nA resource cache always points to its resource_configs, and also specifies the version and params.\n\nNote that resource_configs and resource_caches both point to each other. This loop is closed by a resource config that points to a base resource type at the end of the chain. This represents the case of a resource cache provided by a custom resource type, which in turn may be provided by another custom resource type, and so on, until they reach a base type.\n\n","depth":5,"section_tag":"abstract-objects"},"resource_config_check_sessions-table":{"location":"database-schema.html#resource_config_check_sessions-table","title":"resource_config_check_sessions","text":"A resource config check session is used for periodic resource checking of resources and resource_types by pointing to their resource config. There is only one check container per resource config, even if it's defined in many pipelines.\n\nA check session's expiry is tied to its worker's uptime, as a simple way to force balancing of check containers across workers more quickly after a deploy.\n\n","depth":5,"section_tag":"abstract-objects"},"resource_configs-table":{"location":"database-schema.html#resource_configs-table","title":"resource_configs","text":"A resource config is a distinct configuration of a resource type, not specific to any pipeline resource.\n\nA resource config is used for discovering versions. Unlike pipeline resources, they have no name, and are identified entirely by their type and source. Resource configs are shared across pipelines and teams, since identical resource configurations can be reused.\n\nA resource config's type is either a base resource type or, in the case of a custom resource type, the resource cache of its custom type's image resource.\n\n","depth":5,"section_tag":"abstract-objects"},"resource_types-table":{"location":"database-schema.html#resource_types-table","title":"resource_types","text":"An entry in the resource types table represents a custom resource type defined in a pipleine, and all of the information neccessary to pull the image bits for a custom resource type.  This includes the resource type used to fetch the resource type's image (either a base_resource_type or another custom resource) , the version of the image to fetch using that resource, and the configuration of the custom resource type.\n\n","depth":5,"section_tag":"pipelineapi-objects"},"resources":{"location":"resources.html","title":"Resources","text":"Resources are the heart and soul of Concourse. They represent all external inputs to and outputs of jobs in the pipeline.\n\nResources are listed under the resources: key in the pipeline configuration. Each configured resource consists of the following fields:\n\n","depth":2,"section_tag":"resources"},"resources-table":{"location":"database-schema.html#resources-table","title":"resources","text":"An entry in the resources table represents a resource defined in a pipeline. The resources table has columns for configuration, name, type, and details about resource version checking.\n\n","depth":5,"section_tag":"pipelineapi-objects"},"restarting-a-worker":{"location":"concourse-worker.html#restarting-a-worker","title":"Restarting a Worker","text":"Workers can be restarted in-place by sending SIGTERM to the worker process and starting it back up. Containers will remain running and Concourse will reattach to builds that were in flight.\n\nThis is a pretty aggressive way to restart a worker, and may result in errored builds - there are a few moving parts involved and we're still working on making this airtight.\n\nA safer way to restart a worker is to land it by sending SIGUSR1 to the worker process. This will switch the worker to landing state, and Concourse will stop scheduling new work on it. When all builds running on the worker have finished, the process will exit.\n\nYou may want to enforce a timeout for draining - that way a stuck build won't prevent your workers from being upgraded. This can be enforced by common tools like start-stop-daemon:\n\nstart-stop-daemon \\\n  --pidfile worker.pid \\\n  --stop \\\n  --retry USR1/300/TERM/15/KILL\nThis will send SIGUSR1, wait up to 5 minutes, and then send SIGTERM. If it's still running, it will be killed after an additional 15 seconds.\n\nOnce the timeout is enforced, there's still a chance that builds that were running will continue when the worker comes back.\n\n","depth":5,"section_tag":"restarting-a-worker"},"restarting-and-upgrading":{"location":"concourse-web.html#restarting-and-upgrading","title":"Restarting \u0026 Upgrading","text":"The web nodes can be killed and restarted willy-nilly. No draining is necessary; if the web node was orchestrating a build it will just continue where it left off when it comes back or, or the build will be picked up by one of the other web nodes.\n\nTo upgrade a web node, stop its process and start a new one using the newly installed concourse. Any migrations will be run automatically on start. If web nodes are started in parallel, only one will run the migrations.\n\nNote that we don't currently guarantee a lack of funny-business if you're running mixed Concourse versions - database migrations can perform modifications that confuse other web nodes. So there may be some turbulence during a rolling upgrade, but everything should stabilize once all web nodes are running the latest version.\n\n","depth":5,"section_tag":"restarting-and-upgrading"},"risks-and-side-effects":{"location":"global-resources.html#risks-and-side-effects","title":"Risks and Side Effects","text":"","depth":4,"section_tag":"risks-and-side-effects"},"rotating-the-encryption-key":{"location":"encryption.html#rotating-the-encryption-key","title":"Rotating the Encryption Key","text":"To swap out the encryption key, you'll need to pass the previous key as --old-encryption-key (or old_encryption_key), and the new key as --encryption-key (or encryption_key).\n\nOn startup, the ATC: web UI \u0026 build scheduler will decrypt all existing data and re-encrypt it with the new key, in one go. If it encounters a row which is already encrypted with the new key, it will continue on (as may be the case when restarting with the flags again, or if the ATC died in the middle of rotating).\n\nIf the ATC encounters a row which cannot be decrypted with neither the old key nor the new one, it will log loudly and fail to start, telling you which row it choked on. This data must be dealt with in some way, either by re-configuring the key the row was encrypted with as the old key, or manually performing database surgery to remove the offending row. Hopefully this doesn't happen to you!\n\n","depth":4,"section_tag":"rotating-the-encryption-key"},"running-tasks":{"location":"running-tasks.html","title":"Running Tasks","text":"One of the most common use cases of fly is taking a local project on your computer and submitting it up with a task configuration to be run inside a container in Concourse. This is useful to build Linux projects on OS X or to avoid all of those debugging commits when something is configured differently between your local and remote setup.\n\n","depth":3,"section_tag":"running-tasks"},"runtime":{"location":"database-schema.html#runtime","title":"Runtime","text":"","depth":5,"section_tag":"runtime"},"scaling":{"location":"concourse-web.html#scaling","title":"Scaling","text":"The Running a web node can be scaled up for high availability. They'll also roughly share their scheduling workloads, using the database to synchronize. This is done by just running more web commands on different machines, and optionally putting them behind a load balancer.\n\nTo run a cluster of Running a web nodes, you'll first need to ensure they're all pointing to the same PostgreSQL server.\n\nNext, you'll need to configure a peer URL. This is a URL that can be used to reach this web node's web server from other web nodes. Typically this uses a private IP, like so:\n\nCONCOURSE_PEER_URL=http://10.10.0.1:8080\nFinally, if all of these nodes are going to be accessed through a load balancer, you'll need to configure the external URL that will be used to reach your Concourse cluster:\n\nCONCOURSE_EXTERNAL_URL=https://ci.example.com\nAside from the peer URL, all configuration must be consistent across all web nodes in the cluster to ensure consistent results.\n\n","depth":5,"section_tag":"scaling"},"scaling-workers":{"location":"concourse-worker.html#scaling-workers","title":"Scaling Workers","text":"More workers should be added to accomodate more pipelines. To know when this is necessary you should probably set up Metrics and keep an eye on container counts. If it's starting to approach 200 or so, you should probably add another worker. Load average is another metric to keep an eye on.\n\nTo add a worker, just create another machine for the worker and follow the Running instructions again.\n\nNote: it doesn't really make sense to run multiple workers on one machine, since they'll both just be contending for the same physical resources. Workers should be given their own VMs or physical machines.\n\n","depth":5,"section_tag":"scaling-workers"},"scheduling: full duration (ms)":{"location":"metrics.html#scheduling: full duration (ms)","title":"scheduling: full duration (ms)","text":"This is the time taken (in milliseconds) to schedule an entire pipeline including the time taken to load the version information from the database and calculate the latest valid versions for each job.\n\nAttributes pipeline\n\n: The pipeline which was being scheduled.\n\n\n\n\n\n","depth":4,"section_tag":"whats-emitted"},"scheduling: job duration (ms)":{"location":"metrics.html#scheduling: job duration (ms)","title":"scheduling: job duration (ms)","text":"This is the time taken (in milliseconds) to calculate the set of valid input versions when scheduling a job. It is emitted once for each job per pipeline scheduling tick.\n\nAttributes pipeline\n\n: The pipeline which was being scheduled.\n\n\njob\n\n: The job which was being scheduled.\n\n\n\n\n\n","depth":4,"section_tag":"whats-emitted"},"scheduling: loading versions duration (ms)":{"location":"metrics.html#scheduling: loading versions duration (ms)","title":"scheduling: loading versions duration (ms)","text":"This is the time taken (in milliseconds) to load the version information from the database.\n\nAttributes pipeline\n\n: The pipeline which was being scheduled.\n\n\n\n\n\n","depth":4,"section_tag":"whats-emitted"},"security-contact":{"location":"community.html#security-contact","title":"Security Contact","text":"To be notified of any security issues or vulnerabilities, join the Concourse Security mailing list. To report a security issue, send an email to concourseteam+security@gmail.com.\n\n","depth":2,"section_tag":"security-contact"},"serial-job-example":{"location":"serial-job-example.html","title":"Serial Job","text":"","depth":3,"section_tag":"serial-job-example"},"setting-pipelines":{"location":"setting-pipelines.html","title":"Setting Pipelines","text":"Pipelines are configured entirely via the The fly CLI. There is no GUI.\n\n","depth":3,"section_tag":"setting-pipelines"},"setting-roles":{"location":"managing-teams.html#setting-roles","title":"Setting User Roles","text":"By default, authorization config passed to set-team configures the Team Member role.\n\nMore advanced roles configuration can be specified can be specified through the --configuration or -c flag.\n\nThe -c flag expects a .yml file with a single field, roles:, pointing to a list of role authorization configs.\n\nAll of the attributes in each config will vary by provider. Consult the appropriate section for your provider under Configuring Auth for specifics.\n\nFor example, the following config sets three roles with different auth config for each role's provider:\n\nroles:\n- name: owner\n  github:\n    users: [\"admin\"]\n- name: member\n  github:\n    teams: [\"org:team\"]\n- name: viewer\n  github:\n    orgs: [\"org\"]\n  local:\n    users: [\"visitor\"]\n","depth":5,"section_tag":"setting-roles"},"some-resources-should-opt-out":{"location":"global-resources.html#some-resources-should-opt-out","title":"Some resources should opt-out","text":"Sharing versions isn't always a good idea. For example, the time resource is often used to generate versions on an interval so that jobs can fire periodically. If version history were to be shared for all users with e.g. a 10 minute interval, that would lead to a thundering herd of builds storming your workers, leading to load spikes and a lot of unhappy builds.\n\nFor this reason, resource types can opt out of sharing version history for all resources that use them. This way all existing usage of the time resource don't have to change, and continue to have their own version history, unique to the pipeline resource.\n\nThe time resource opts out of this by configuring unique_version_history: true in its metadata.json - but this is something that only \"core\" resource types can do. We plan on supporting this as part of the Resources v2 RFC.\n\nUsers can also set this value themselves by configuring unique_version_history on the resource type.\n\nAnother case where version history shouldn't be shared is when resources \"automagically\" learn their auth credentials using things like IAM roles. In these cases, the credentials aren't in the source. If version history were to be shared, anyone could configure the same source:, not specifying any credentials, and see the version history discovered by some other pipeline that ran its checks on workers that had access via IAM roles.\n\nFor this reason, any resource types that acquire credentials outside of source: should not share version history. Granted, the user won't be able to fetch these versions, but it's still an information leak.\n\nIAM roles are a bit of a thorn in our side when it comes to designing features like this. We're planning on introducing support for them in a way that doesn't have this problem in concourse #3023.\n\n","depth":5,"section_tag":"some-resources-should-opt-out"},"stark-and-wayne":{"location":"thanks.html#stark-and-wayne","title":"Stark \u0026 Wayne","text":"The Concourse Tutorial by Stark \u0026 Wayne  is probably the only reason a lot of people were able to learn Concourse. It's a great asset and does its job so well that we've decided to basically just kill our tutorials and delegate to theirs. Thanks to Dr. Nic for writing and maintaining it, and everyone else that has helped out!\n\n","depth":3,"section_tag":"stark-and-wayne"},"steps":{"location":"steps.html","title":"Steps","text":"Each Job has a single build plan. When a build of a job is created, the plan determines what happens.\n\nA build plan is a sequence of steps to execute. These steps may fetch down or update Resources, or execute Tasks.\n\nA new build of the job is scheduled whenever get steps with trigger: true have new versions available.\n\nTo visualize the job in the pipeline, resources that appear as get steps are drawn as inputs, and resources that appear in put steps appear as outputs.\n\n","depth":3,"section_tag":"steps"},"tags":{"location":"tags-step-modifier.html#tags","title":"tags","text":"Optional. Default []. The tags by which to match workers.\n\nFor example, if [a, b] is specified, only workers advertising the a and b tags (or any others) will be used for running the step.\n\n","depth":4,"section_tag":"tags-step-modifier"},"tags-step-modifier":{"location":"tags-step-modifier.html","title":"tags step modifier","text":"Any step can be directed at a pool of workers for a given set of tags, by adding the tags attribute to it.\n\n","depth":4,"section_tag":"tags-step-modifier"},"taking-artifacts-from-the-build-with---output":{"location":"running-tasks.html#taking-artifacts-from-the-build-with---output","title":"Taking artifacts from the build with --output","text":"If a task specifies outputs then you're able to extract these back out of the build and back to your local system. For example:\n\n$ fly -t example execute \\\n    --config build-stemcell.yml \\\n    --input code=. \\\n    --output stemcell=/tmp/stemcell\nThis would work together with a build-stemcell.yml if its outputs: section was as follows:\n\noutputs:\n- name: stemcell\nThis feature is useful to farm work out to your Concourse server to build things in a repeatable manner.\n\n","depth":5,"section_tag":"taking-artifacts-from-the-build-with---output"},"targeting-a-specific-worker-with---tag":{"location":"running-tasks.html#targeting-a-specific-worker-with---tag","title":"Targeting a specific worker with --tag","text":"If you want to execute a task on a worker that has a specific tag, you can do so by passing --tag:\n\nfly -t example execute --config task.yml --tag bar\nThis will execute the task specified by task.yml on a worker that has been tagged bar.\n\n","depth":5,"section_tag":"targeting-a-specific-worker-with---tag"},"task":{"location":"task-step.html#task","title":"task","text":"Required. A freeform name for the task that's being executed. Common examples would be unit or integration.\n\n","depth":4,"section_tag":"task-step"},"task-caches":{"location":"tasks.html#task-caches","title":"caches","text":"Where cache is:\n\nOptional. The cached directories shared between task runs.\n\nOn the task's first run, all cache directories will be empty. It is the responsibility of the task to populate these directories with any artifacts to be cached. On subsequent runs, the cached directories will contain those artifacts.\n\nCaches are scoped to the worker the task is run on, so you will not get a cache hit when subsequent builds run on different workers. This also means that caching is not intended to share state between workers, and your task should be able to run whether or not the cache is warmed.\n\nCaches are also scoped to a particular task name inside of a pipeline's job. As a consequence, if the job name, step name or cache path are changed, the cache will not be used. This also means that caches do not exist for one-off builds.\n\n","depth":2,"section_tag":"tasks"},"task-containers":{"location":"container-internals.html#task-containers","title":"Task Containers","text":"Task containers are created when a task step is executed in a build plan. They are based on the image produced by the configured image_resource or image.\n\n","depth":6,"section_tag":"task-containers"},"task-environment":{"location":"task-environment.html","title":"Task Environment","text":"A task runs in a new container every time, using the image provided by image_resource as its base filesystem (i.e. /).\n\nThe command specified by run will be executed in a working directory containing each of the inputs. If any inputs are missing the task will not run (and the container will not even be created).\n\nThe working directory will also contain empty directories for each of the outputs. The task must place artifacts in the output directories for them to be exported. This meshes well with build tools with configurable destination paths.\n\nIf your build tools don't support output paths you'll have to copy bits around. If it's a git repo that you're modifying you can do a local git clone ./input ./output, which is much more efficient than cp, and then work out of ./output.\n\nAny params configured will be set in the environment for the task's command, along with any environment variables provided by the task's image (i.e. ENV rules from your Dockerfile).\n\nThe user the command runs as is determined by the image. If you're using the Docker Image resource, this will be the user set by a USER rule in your Dockerfile, or root if not specified.\n\nAnother relevant bit of configuration is privileged, which determines whether the user the task runs as will have full privileges (primarily when running as root). This is intentionally not configurable by the task itself, to prevent privilege escalation by way of pull requests to repositories containing task configs.\n\nPutting all this together, the following task config:\n\n---\nplatform: linux\n\nimage_resource:\n  type: docker-image\n  source:\n    repository: golang\n    tag: '1.6'\n\nparams:\n  SOME_PARAM: some-default-value\n\ninputs:\n- name: some-input\n- name: some-input-with-custom-path\n  path: some/custom/path\n\noutputs:\n- name: some-output\n\nrun:\n  path: sh\n  args:\n  - -exc\n  - |\n    whoami\n    env\n    go version\n    find .\n    touch some-output/my-built-artifact\n...will produce the following output:\n\n+ whoami\nroot\n+ env\nUSER=root\nHOME=/root\nGOLANG_DOWNLOAD_SHA256=5470eac05d273c74ff8bac7bef5bad0b5abbd1c4052efbdbc8db45332e836b0b\nPATH=/go/bin:/usr/local/go/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nGOPATH=/go\nPWD=/tmp/build/e55deab7\nGOLANG_DOWNLOAD_URL=https://golang.org/dl/go1.6.linux-amd64.tar.gz\nGOLANG_VERSION=1.6\nSOME_PARAM=some-default-value\n+ go version\ngo version go1.6 linux/amd64\n+ find .\n.\n./some-input\n./some-input/foo\n./some\n./some/custom\n./some/custom/path\n./some/custom/path/bar\n./some-output\n+ touch some-output/my-built-artifact\n...and propagate my-built-artifact to any later task steps or put steps that reference the some-output artifact, in the same way that this task had some-input as an input.\n\n","depth":3,"section_tag":"task-environment"},"task-image-resource":{"location":"tasks.html#task-image-resource","title":"image_resource","text":"Where resource is:\n\nOptional. The base image of the container, as provided by a resource definition.\n\nYou can use any resource that returns a filesystem in the correct format (a /rootfs directory and a metadata.json file in the top level) but normally this will be the Docker Image resource. If you'd like to make a resource of your own that supports this please use that as a reference implementation for now.\n\nIf you want to use an artifact source within the plan containing an image, you must set the image in the plan step instead.\n\n","depth":2,"section_tag":"tasks"},"task-inputs":{"location":"tasks.html#task-inputs","title":"inputs","text":"Where input is:\n\nOptional. The set of artifacts used by task, determining which artifacts will be available in the current directory when the task runs.\n\nThese are satisfied by get steps or outputs of a previous task. These can also be provided by -i with fly execute.\n\nIf any required inputs are missing at run-time, then the task will error immediately.\n\n","depth":2,"section_tag":"tasks"},"task-outputs":{"location":"tasks.html#task-outputs","title":"outputs","text":"Where output is:\n\nOptional. The artifacts produced by the task.\n\nEach output configures a directory to make available to later steps in the build plan. The directory will be automatically created before the task runs, and the task should place any artifacts it wants to export in the directory.\n\n","depth":2,"section_tag":"tasks"},"task-params":{"location":"tasks.html#task-params","title":"params","text":"Optional. A key-value mapping of values that are exposed to the task via environment variables.\n\nUse this to provide things like credentials to a task.\n\n","depth":2,"section_tag":"tasks"},"task-passing-artifact-example":{"location":"task-passing-artifact-example.html","title":"Tasks Passing Artifact","text":"","depth":3,"section_tag":"task-passing-artifact-example"},"task-platform":{"location":"tasks.html#task-platform","title":"platform","text":"Required. The platform the task should run on. By convention, windows, linux, or darwin are specified. This determines the pool of workers that the task can run against. The base deployment provides Linux workers.\n\n","depth":2,"section_tag":"tasks"},"task-rootfs-uri":{"location":"tasks.html#task-rootfs-uri","title":"rootfs_uri","text":"Optional. A string specifying the rootfs uri of the container, as interpreted by your worker's Garden backend.\n\nimage_resource is a preferred way to specify base image and rootfs_uri is not recommended. With rootfs_uri image fetching is delegated to backend which does not guarantee image caching and might result in some permission errors. You should only use this if you cannot use image_resource for some reason, and you know what you're doing.\n\n","depth":2,"section_tag":"tasks"},"task-run":{"location":"tasks.html#task-run","title":"run","text":"Where run-config is:\n\nRequired. The command to execute in the container.\n\nNote that this is not provided as a script blob, but explicit path and args values; this allows fly to forward arguments to the script, and forces your config .yml to stay fairly small.\n\n","depth":2,"section_tag":"tasks"},"task-run-args":{"location":"tasks.html#task-run-args","title":"run.args","text":"Optional. Arguments to pass to the command. Note that when executed with Fly, any arguments passed to Fly are appended to this array.\n\n","depth":2,"section_tag":"tasks"},"task-run-dir":{"location":"tasks.html#task-run-dir","title":"run.dir","text":"Optional. A directory, relative to the initial working directory, to set as the working directory when running the script.\n\n","depth":2,"section_tag":"tasks"},"task-run-path":{"location":"tasks.html#task-run-path","title":"run.path","text":"Required. The command to execute.\n\nThis is commonly a path to a script provided by one of the task's inputs, e.g. my-resource/scripts/test. It could also be a command like bash (respecting standard $PATH lookup rules), or an absolute path to a file to execute, e.g. /bin/bash.\n\n","depth":2,"section_tag":"tasks"},"task-run-user":{"location":"tasks.html#task-run-user","title":"run.user","text":"Optional. Explicitly set the user to run as. If not specified, this defaults to the user configured by the task's image. If not specified there, it's up to the Garden backend, and may be e.g. root on Linux.\n\n","depth":2,"section_tag":"tasks"},"task-step":{"location":"task-step.html","title":"task step","text":"Executes a Task, either from a file fetched via the preceding steps, or with inlined configuration.\n\nIf any task in the build plan fails, the build will complete with failure. By default, any subsequent steps will not be performed. You can perform additional steps after failure by adding a on_failure or ensure step hook.\n\nWhen a task completes, the files in its declared outputs will be made avaliable to subsequent steps. This allows those subsequent steps to process the result of a task.\n\n","depth":4,"section_tag":"task-step"},"task-step-config":{"location":"task-step.html#task-step-config","title":"config","text":"Required. The task config to execute. An alternative to file.\n\n","depth":4,"section_tag":"task-step"},"task-step-file":{"location":"task-step.html#task-step-file","title":"file","text":"Required. A dynamic alternative to config.\n\nfile points at a .yml file containing the task config, which allows this to be tracked with your resources.\n\nThe first segment in the path should refer to another source from the plan, and the rest of the path is relative to that source.\n\nFor example, if in your plan you have the following get step:\n\n- get: something\nAnd the something resource provided a unit.yml file, you would set file: something/unit.yml.\n\nThe content of the config file may contain template ((vars)), which will be filled in using vars or a configured credential manager.\n\n","depth":4,"section_tag":"task-step"},"task-step-image":{"location":"task-step.html#task-step-image","title":"image","text":"Optional. Names an artifact source within the plan containing an image to use for the task. This overrides any image_resource configuration present in the task configuration.\n\nThis is very useful when part of your pipeline involves building an image, possibly with dependencies pre-baked. You can then propagate that image through the rest of your pipeline, guaranteeing that the correct version (and thus a consistent set of dependencies) is used throughout your pipeline.\n\nFor example, here's a pipeline building an image in one job and propagating it to the next:\n\nresources:\n- name: my-project\n  type: git\n  source: {uri: https://github.com/my-user/my-project}\n\n- name: my-task-image\n  type: docker-image\n  source: {repository: my-user/my-repo}\n\njobs:\n- name: build-task-image\n  plan:\n  - get: my-project\n  - put: my-task-image\n    params: {build: my-project/ci/images/my-task}\n\n- name: use-task-image\n  plan:\n  - get: my-task-image\n    passed: [build-task-image]\n  - get: my-project\n    passed: [build-task-image]\n  - task: use-task-image\n    image: my-task-image\n    file: my-project/ci/tasks/my-task.yml\nThis can also be used in the simpler case of explicitly keeping track of dependent images, in which case you just wouldn't have a job building it (build-task-image in the above example).\n\n","depth":4,"section_tag":"task-step"},"task-step-params":{"location":"task-step.html#task-step-params","title":"params","text":"Optional. A map of task parameters to set, overriding those configured in config or file.\n\nThe difference between params and vars is that vars allows you to interpolate any template variable in an external task, while params can be used to overwrite task parameters (i.e. env variables) specifically. Also, params can have default values declared in the task.\n\nFor example:\n\nplan:\n- get: my-repo\n- task: integration\n  file: my-repo/ci/integration.yml\n  params:\n    REMOTE_SERVER: 10.20.30.40:8080\n    USERNAME: my-user\n    PASSWORD: my-pass\nThis is often used in combination with ((vars)) in the pipeline.\n\nFor example:\n\nplan:\n- get: my-repo\n- task: integration\n  file: my-repo/ci/integration.yml\n  params:\n    REMOTE_SERVER: 10.20.30.40:8080\n    USERNAME: ((integration-username))\n    PASSWORD: ((integration-password))\nLooking into the task.yml, a common pattern is to list the param in the task definition with no value. This indicates that you expect the pipeline to provide the value.\n\nFor example, in the task.yml:\n\nparams:\n  FOO: fizzbuzz\n  BAR:\nAnd in the pipeline.yml:\n\nparams:\n  BAR: qux\nIf the pipeline used this task.yml but did not set BAR, the value of $BAR would be set to the empty string in the task container.\n\n","depth":4,"section_tag":"task-step"},"task-step-privileged":{"location":"task-step.html#task-step-privileged","title":"privileged","text":"Optional. Default false. If set to true, the task will run with full capabilities, as determined by the Garden backend the task runs on. For Linux-based backends it typically determines whether or not the container will run in a separate user namespace, and whether the root user is \"actual\" root (if set to true) or a user namespaced root (if set to false, the default).\n\nThis is a gaping security hole; use wisely and only if necessary. This is not part of the task configuration to prevent privilege escalation via pull requests.\n\n","depth":4,"section_tag":"task-step"},"task-step-vars":{"location":"task-step.html#task-step-vars","title":"vars","text":"Optional. A map of template variables to pass to an external task. Only works with external tasks defined in file.\n\nFor example:\n\nplan:\n- get: my-repo\n- task: integration\n  file: my-repo/ci/task.yml\n  vars:\n    text: \"Hello World!\"\nThis is often used in combination with ((vars)) in the pipeline.\n\nplan:\n- get: my-repo\n- task: integration\n  file: my-repo/ci/task.yml\n  vars:\n    text: ((text))\nAnd task.yml:\n\n---\nplatform: linux\n\nimage_resource:\n  type: docker-image\n  source:\n    repository: my.local.registry:8080/my/image\n    username: ((myuser))\n    password: ((mypass))\n\nrun:\n  path: echo\n  args: [\"((text))\"]\nThis will resolve \"((text))\" to \"Hello World!\", while ((myuser)) and ((mypass))  will be resolved in runtime via a credential manager, if it has been configured.\n\n","depth":4,"section_tag":"task-step"},"tasks":{"location":"tasks.html","title":"Tasks","text":"The smallest configurable unit in a Concourse pipeline is a single task. A task can be thought of as a function from inputs to outputs that can either succeed or fail.\n\nGoing a bit further, ideally tasks are pure functions: given the same set of inputs, it should either always succeed with the same outputs or always fail. This is entirely up to your script's level of discipline, however. Flaky tests or dependencies on the internet are the most common source of impurity.\n\nOnce you have a running Concourse deployment, you can start configuring your tasks and executing them interactively from your terminal with the Fly commandline tool.\n\nOnce you've figured out your tasks's configuration, you can reuse it for a Job in your Pipeline.\n\nConventionally a task's configuration is placed in the same repository as the code it's testing, possibly under some ci directory.\n\nA task's configuration specifies the following:\n\n","depth":2,"section_tag":"tasks"},"team-member-role":{"location":"user-roles.html#team-member-role","title":"Team Member role","text":"Team Member lets users operate within their teams in a read \u0026 write fashion; but prevents them from changing the auth configurations of their team.\n\n","depth":4,"section_tag":"team-member-role"},"team-owner-role":{"location":"user-roles.html#team-owner-role","title":"Team Owner role","text":"Team Owners have read, write and auth management capabilities within the scope of their team. For those familiar with Concourse today, the scope of allowed actions for a Team Owner is very closely aligned to today’s Concourse team member. The new change is that you can no longer rename your own team or destroy your own team as an owner.\n\n","depth":4,"section_tag":"team-owner-role"},"team-viewer-role":{"location":"user-roles.html#team-viewer-role","title":"Team Viewer role","text":"Team Viewer gives users “read-only” access to a team. This locks everything down, preventing users from doing a set-pipeline or hijack.\n\n","depth":4,"section_tag":"team-viewer-role"},"teams":{"location":"auth.html","title":"Auth \u0026 Teams","text":"A single Concourse installation can accomodate many projects and users.\n\nPipelines, builds, and all other user data are owned by teams. A team is just a conceptual owner and a separate namespace, tied to an authorization config. For example, a team may authorize all members of the concourse GitHub organization to be a member.\n\nWhen a user authenticates, each team's authorization config is checked against the user to determine which role, if any, to grant for the team. This information is then stored in the user's token to determine access control for future requests.\n\n","depth":2,"section_tag":"auth"},"teams-caveats":{"location":"teams-caveats.html","title":"Security Caveats","text":"At present, teams only provide trusted multi-tenancy. This means it should be used for cases where you know and trust who you're allowing access into your Concourse cluster.\n\nThere are a few reasons it'd be a bad idea to do otherwise:\n\n* Any team can run builds with privileged tasks. A bad actor in the mix could easily use this to harm your workers and your cluster.\n\n  In the future, we'll probably have this as a flag on a team, indicating whether they're permitted to run privileged builds.\n\n* There are no networking restrictions in place, and traffic to and from the workers is currently unencrypted and unauthorized. Anyone could run a task that does horrible things to your worker's containers, possibly stealing sensitive information.\n\n  This can be remedied with configuration specified on Garden to restrict access to the internal network, but this is not detailed in our docs, and we'll probably want to find a better answer than configuration in the future.\n\n","depth":3,"section_tag":"teams-caveats"},"thanks":{"location":"learning.html#thanks","title":"Thanks","text":"A big thank you to our community of contributors for making these tutorials, guides, and articles for Concourse. Your contributions are deeply appreciated and you have our thanks 👏\n\nIf you'd like to add a tutorial or guide to this list, fork the concourse/docs repository, edit lit/tutorials.lit, then make a pull request.\n\n","depth":3,"section_tag":"thanks"},"time-triggered-job-example":{"location":"time-triggered-job-example.html","title":"Time Triggered Job","text":"","depth":3,"section_tag":"time-triggered-job-example"},"timeout":{"location":"timeout-step-modifier.html#timeout","title":"timeout","text":"The amount of time to limit the step's execution to, e.g. 30m for 30 minutes.\n\nWhen exceeded, the step will be interrupted, with the same semantics as aborting the build (except the build will be failed, not aborted, to distinguish between human intervention and timeouts being inforced).\n\n","depth":4,"section_tag":"timeout-step-modifier"},"timeout-step-modifier":{"location":"timeout-step-modifier.html","title":"timeout step modifier","text":"Any step can have a hard time limit enforced by attaching timeout and the number of seconds to limit it to.\n\n","depth":4,"section_tag":"timeout-step-modifier"},"tools":{"location":"community.html#tools","title":"Tools","text":"Concourse Up by @EngineerBetter. A tool for easily deploying Concourse in a single command.\n\nConcourse Helm Chart. The latest stable Helm chart for deploying Concourse into k8s. Official support coming soon.\n\nConcourse Formula by @marco-m. All-in-one Concourse installation using Vagrant and Virtualbox with S3-compatible storage and Vault secret manager.\n\nBUCC by @starkandwayne. The bucc command line utility allows for easy bootstrapping of the BUCC stack (BOSH UA Credhub and Concourse). Which is the starting point for many deployments.\n\nofcourse a library and a project skeleton generator for making your own Concourse resources in Go, with an emphasis on testability. Written and maintained by @cloudboss\n\n","depth":3,"section_tag":"tools"},"trademarks":{"location":"trademarks.html","title":"Trademarks","text":"* PURPOSE. Pivotal Software, Inc. (“Pivotal”) owns a number of international trademarks and logos that identify the Concourse community and individual Concourse projects (“Concourse Marks”). These trademarks include, but are not limited to:\n\n  * Words: CONCOURSE\n\n  * Logos: {image: images/trademarks/concourse-black.png}\n\n  This policy outlines Pivotal’s policy and guidelines about the use of the Concourse trademarks by members of the Concourse development and user community.\n\n* WHY HAVE TRADEMARK GUIDELINES? The Concourse Marks are a symbol of the quality and community support associated with the Concourse open source software. Trademarks protect not only those using the marks but the entire community as well. Our community members need to know that they can rely on the quality and capabilities represented by the brand. We also want to provide a level playing field. No one should use the Concourse marks in ways that mislead or take advantage of the community or make unfair use of the trademarks. Also, use of the Concourse Marks should not be in a disparaging manner because we prefer that our marks not be used to be rude about the Concourse open source project or its members.\n\n* OPEN SOURCE LICENSE VS. TRADEMARKS. The Apache 2.0 license gives you the right to use, copy, distribute and modify the Concourse software. However, open source licenses like the Apache 2.0 license do not address trademarks.  Concourse Marks need to be used in a way consistent with trademark law, and that is why we have prepared this policy – to help you understand what branding is allowed or required when using our software under the Apache license.\n\n* PROPER USE OF THE CONCOURSE MARKS. We want to encourage a robust community for the Concourse open source project. Therefore, you may do any of the following, as long as you do so in a way that does not devalue, dilute, or disparage the Concourse brand. In other words, when you do these things, you should behave responsibly and reasonably in the interest of the community, but you do not need a trademark license from us to do them.\n\n  * Nominative Use. You may engage in “nominative use” of the Concourse name, but this does not allow you to use the logo.  Nominative use is sometimes called “fair use” of a trademark, and does not require a trademark license from us.  Here are examples:\n\n    * You may use the Concourse Marks in connection with the development of tools, add-ons, or utilities that are compatible with bit-for-bit identical copies of official Concourse software. For example, if you are developing a Foobar tool for Concourse, acceptable project titles would be “Foobar for Concourse\".\n\n    * You may use the Concourse Marks in connection with your non-commercial redistribution of (1) bit-for-bit identical copies of official Concourse software, and (2) unmodified copies of official Concourse source packages.  For example, if your Foobar product included a full redistribution of official Concourse Software or source code packages: \"Concourse-powered Foobar Product\". We strongly discourage, and likely would consider it a trademark problem, to use a name such as “Concourse Foobar.”\n\n    * If you offer maintenance, support, or hosting services for Concourse software, you may accurately state that in your marketing materials or portfolio, without using the Concourse logo.\n\n    * You may modify the Concourse software and state that your modified software is “based on the Concourse software” or a similar accurate statement, without using the Concourse logo.\n\n    * You may engage in community advocacy. The Concourse software is developed by and for its community. We will allow the use of the word trademark in this context, provided:\n\n      * The trademark is used in a manner consistent with this policy.\n\n      * There is no commercial purpose behind the use.\n\n      * There is no suggestion that your project is approved, sponsored, or affiliated with Concourse.\n\n      * You may create Concourse user or development groups, and publicize meetings or discussions for those groups.\n\n  * Attribution. Identify the trademarks as trademarks of Concourse, as set forth in Section 7.\n\n  * Redistribution of Binaries. If you redistribute binaries that you have downloaded from the Concourse repository, you should retain the logos and name of the product.  However, if you make any changes to the binaries (other than configuration or installation changes that do not involve changes to the source code), or if you re-build binaries from our source code, you should not use our logos.  Our logos represent our quality control, so they should be retained where the product has been built by us, but not otherwise.\n\n  * Capitalization. “Concourse” should always be capitalized and one word.\n\n* IMPROPER USE OF THE TRADEMARKS AND LOGOS. Use of the logo is reserved solely for use by Concourse in its unaltered form. Examples of unauthorized use of the Concourse trademarks include:\n\n  * Commercial Use: You may not use the Concourse Marks in connection with commercial redistribution of Concourse software (commercial redistribution includes, but is not limited to, redistribution in connection with any commercial business activities or revenue-generating business activities) regardless of whether the Concourse software is unmodified.\n\n  * Entity Names. You may not form a company, use a company name, or create a software product name that includes the “Concourse” trademark, or implies any foundational or authorship role. If you have a software product that works with Concourse, it is suggested you use terms such as ‘\u003cproduct name\u003e for Concourse’ or ‘\u003cproduct name\u003e, Concourse Edition.” If you wish to form an entity for a user or developer group, please contact us and we will be glad to discuss a license for a suitable name.\n\n  * Class or Quality. You may not imply that you are providing a class or quality of Concourse (e.g., \"enterprise-class\" or \"commercial quality\") in a way that implies Concourse is not of that class, grade or quality, nor that other parties are not of that class, grade, or quality.\n\n  * Combinations. Use of the Concourse Marks to identify software that combines any portion of the Concourse software with any other software, unless the combined distribution is an official Concourse distribution. For example, you may not distribute a combination of the Concourse  software with software released by the Foobar project under the name “Concourse Foobar Distro”.\n\n  * False or Misleading Statements. You may not make false or misleading statements regarding your use of Concourse (e.g., \"we wrote the majority of the code\" or \"we are major contributors\" or \"we are committers\").\n\n  * Domain Names. You must not use Concourse or any confusingly similar phrase in a domain name. For instance “www.concoursehost.com” is not allowed.  If you wish to use such a domain name for a non-commercial user or developer group to engage in community advocacy, please contact us and we will be glad to discuss a license for a suitable domain name.  Because of the many persons who, unfortunately, seek to spoof, swindle or deceive the community by using confusing domain names, we must be very strict about this rule.\n\n  * Merchandise. You must not manufacture, sell or give away merchandise items, such as T-shirts and mugs, bearing the Concourse logo, or create any mascot for the project. If you wish to use the logo to do this for a non-commercial user or developer group to engage in community advocacy, please contact us and we will be glad to discuss a license to do this.\n\n  * Variations, takeoffs or abbreviations. You may not use a variation of the Concourse name or logo for any purpose other than common usage of these in community communications. For example, the following are not acceptable:\n\n    * CONCRSE\n\n    * MyConcourse\n\n    * ConcourseDB\n\n    * ConcourseHost\n\n    * ConcourseGuru\n\n  * Endorsement or Sponsorship. You may not use the Concourse trademarks in a manner that would imply Concourse’s affiliation with or endorsement, sponsorship, or support of a product or service.\n\n  * Rebranding. You may not change the trademark on unmodified Concourse software to your own brand.  You may not hold yourself out as the source of the Concourse software, except to the extent you have modified it as allowed under the Apache 2.0 license, and you make it clear that you are the source only of the modification.\n\n  * Combination Marks. Do not use our trademarks in combination with any other marks or logos (for example Foobar Concourse, or the name of your company or product typeset to look like the Concourse logo).\n\n  * Web Tags. Do not use the Concourse trademark in a title or meta tag of a web page to influence search engine rankings or result listings, rather than for discussion or advocacy of the Concourse project.\n\n* PROPER ATTRIBUTION. When you use a Concourse trademark you should include a statement attributing the trademark to Concourse. For example, \"Concourse is a trademark of Concourse in the U.S. and other countries.\"\n\n* MORE QUESTIONS? If you have questions about this policy, please contact us at concourseteam+trademarks@gmail.com.\n\n","depth":2,"section_tag":"trademarks"},"trigger-then-continue-example":{"location":"trigger-then-continue-example.html","title":"Trigger Then Continue","text":"","depth":3,"section_tag":"trigger-then-continue-example"},"try":{"location":"try-step.html#try","title":"try","text":"Performs the given step, swallowing any failure.\n\nThis can be used when you want to perform some side-effect, but you don't really want the whole build to fail if it doesn't work.\n\n","depth":4,"section_tag":"try-step"},"try-step":{"location":"try-step.html","title":"try step","text":"","depth":4,"section_tag":"try-step"},"tutorials":{"location":"learning.html#tutorials","title":"Tutorials","text":"If you're just getting started with Concourse we'd recommend that you start with the Concourse Tutorials developed and maintained by our friends at Stark \u0026 Wayne.\n\nBelow you'll also find additional tutorials, guides and articles written by various members of the Concourse community.\n\nThese guides are not written or maintained by the Concourse maintainers. As usual, use at your own risk!\n\n","depth":2,"section_tag":"tutorials"},"types-of-containers":{"location":"container-internals.html#types-of-containers","title":"Types of Containers","text":"These are the types of containers:\n\n","depth":5,"section_tag":"types-of-containers"},"types-of-volumes":{"location":"volume-internals.html#types-of-volumes","title":"Types of Volumes","text":"","depth":5,"section_tag":"types-of-volumes"},"user-roles":{"location":"user-roles.html","title":"User Roles \u0026 Permissions","text":"Concourse comes with four roles: Concourse Admin, Team Owner, Team Member, Team Viewer.\n\n","depth":3,"section_tag":"user-roles"},"v0730":{"location":"download.html#v0730","title":"v0.73.0","text":"The fly destroy-pipeline command now runs much quicker. Unfortunately to implement this there's a massive database migration. Expect anywhere from a few minutes of downtime up to a few hours when you upgrade to this version, depending on how many builds you have and how chatty they are. Sorry about that.\n\nThere is no way of predicting how long this migration may take for your instance. It depends on the chattiness of your builds and the performance of your database. For a rough approximation: running the migration on the Concourse team's server took 20 minutes to migrate 13 million build events.\n\nYou can find out how many build events you have by running the following query against your Concourse database:\n\nSELECT relname, n_live_tup\nFROM pg_stat_user_tables\nWHERE relname = 'build_events';\nThe root of the issue is the amount of data in the build_events table. If you don't have many build logs then then you can probably just upgrade and not worry about the rest of this. If the upgrade is going to take too long then you'll need to find some way to reduce the rows in that table.\n\nBefore you delete any data from the system you should make sure to take a backup of your database and make doubly sure you can restore it while blind-folded and upside-down in case anything goes wrong.\n\nIf you don't care about your old build logs then you can simply run TRUNCATE build_events; before upgrading and the migration will be quick and painless.\n\nMany of you probably do care about your build events but maybe only those which were created in the past X months. If this sounds like a good idea then have I got the SQL query for you! Run this against your Concourse database (change the X to the number of months you'd like to keep):\n\nDELETE FROM build_events\nWHERE build_id IN (\n  SELECT builds.id\n  FROM jobs JOIN builds ON jobs.id = builds.job_id\n  WHERE builds.end_time \u003c NOW() - INTERVAL 'X month'\n  ORDER BY builds.id\n);\nThis query may take a while to execute but your Concourse can be online the entire time that it is running.\n\nIf you want to keep all of your build logs and have a fast migration then I'm sorry, I can't help you. :( Maybe upgrade over a weekend?\n\nThe Docker Image resource no longer produces the docker saved image by default. This is to reduce disk usage when using the resource as an image_resource. You must now pass save: true as part of params on the get step to produce the image file.\n\nInputs to jobs that are not configured to trigger the job when new versions appear will now be rendered with a dashed line. This makes it easier to see which resources automate the pipeline flow, and which jobs are only ever manually triggered.\n\nA pending build will now indicate why it's pending, via a checklist that appears at the top of the build output. (Yay!)\n\nThe theme selector is gone. So are all but one of the themes. This new theme is the product of our research and your feedback. I'm confident that it's perfect in every way. But let us know if you have any major problems with it.\n\nImproved the caching of resources used for image_resource. Previously if the same version was fetched multiple times on the same worker, we'd keep all of them around so long as they were the latest version. We'll now only keep one.\n\nThe fly destroy-pipeline command learnt the -n option which when used will not ask you to confirm the deletion of the pipeline. Useful for scripts. Dangerous for users.\n\nThe Docker Image resource no longer worked for images configured with ENTRYPOINTs as of v0.72.0, and ended up running whatever the entrypoint was, with our internal binary tool as an argument. It now works again. Our bad.\n\nThe GitHub release resource would have issues when fetching artefacts from S3 via GitHub when using an access token. It no longer has these issues.\n\nThe Docker Image resource now supports a dockerfile parameter for specifying a path to the Dockerfile to use.\n\nThe Git resource now supports producing annotated tags via the annotate parameter.\n\nThe Git resource now supports checking for tag patterns like *-production via the tag_filter source configuration.\n\nThe Git resource now includes git lfs.\n\n","depth":2,"section_tag":"v0730"},"v0740":{"location":"download.html#v0740","title":"v0.74.0","text":"You can now configure resource types in your pipeline rather than redeploying your workers with additional resource types. This should make it much easier to use the community resources that people have built!\n\nAutoscroll on a build page is back and is now implemented in a way that doesn't kill the browser when you have many build events.\n\nIf the Docker image you specify in the image_resource section of your task has a custom user then we will now respect that when running the task. This user will also be used when hijacking in to a build container.\n\nHijacked connections will no longer cause connection timeouts at interim load balancers if there is no input or output.\n\nThe pipeline graph rendering now has large portions of the computation cached. This should provide a significant speedup and decrease in CPU load when viewing a pipeline.\n\nWe've made some tweaks to the ATC's build scheduling that should fix \"deadlock\" scenarios with serial groups.\n\nPreviously, if a pipeline of \"A -\u003e B -\u003e C\" had all 3 jobs in a serial group, and the builds were enqueued in order of C, B, then A (manually), nothing could ever run, as the scheduling was based on the order of the builds being enqueued, and C would never be satisifed. This is now fixed by collecting inputs and then scheduling only once they're available, so that C never gets scheduled, and so A is able to be scheduled, followed by B, and then C.\n\nThe icon font that was broken in Safari by v0.73.0 are now unbroken by v0.74.0.\n\nInterrupting a fly execute that was fetching outputs will no longer panic if you cancel it in the middle.\n\nIf you try and trigger a build while you are not logged in then we'll now redirect you back to the build page you were on rather than the main pipeline page.\n\nThe Pool resource will now ignore in-place modifications when working out if a log aquisition is still valid.\n\nThe fly CLI will now print the target it will be interacting with at the start of every command.\n\nThe fly CLI default value of the -t flag has been removed. If you're using the VirtualBox distribution then you'll need to start logging in and supplying a target. This is to get people in this habit before they progress to a bigger deployment.\n\nThe fly CLI has a more sensible timeout and a better error message if it cannot reach the targeted Concourse.\n\nWe bumped to Go 1.6. You should see absolutely no change.\n\n","depth":2,"section_tag":"v0740"},"v0750":{"location":"download.html#v0750","title":"v0.75.0","text":"The Semver resource now creates a file called version containing the version number, making it consistent with other resources that provide a version.\n\nWe still create number for backwards compatibility, but you should switch.\n\nSpecifying both file and config on a task step is now deprecated. You should receive warnings when running fly set-pipeline and when running a task that specifies both.\n\nInstead, you should be specifying params, input_mapping, and output_mapping.\n\nThe fly CLI and the web UI now know their own version! We probably should have done this years ago. So fly -v now works instead of printing a shrugging emoticon, and the web UI now has the version at the bottom right (it even live updates, for all your CI monitors out there).\n\nIn addition, fly will print a warning if the versions are slightly out of sync (patch release), and straight up prevent itself from running if they're significantly out of sync (i.e. minor or major).\n\nTagged workers are now supported by image_resource.\n\nThe duration that containers stick around for after finishing is now configurable via new atc.retention.* BOSH properties (and corresponding flags to the ATC).\n\nfly intercept now sorts its container list, which should aid in frustration with finding the container to intercept.\n\nfly containers now shows the TTL (as we've configured it) and validity (actual expiration, which counts down to 0) for each container. This will be useful to know which ones are sticking around because they failed, and which ones are sticking around because of a build that's running too frequently.\n\nfly learned the fly abort-build command, thanks to a pull request from @zachgersh.\n\nfly learned the fly trigger-job command, thanks to a pull request from @aminjam.\n\nThe BOSH deployment resource now supports deploying to a director using UAA client ID/secret for auth.\n\nWe've bumped the version of Buildroot that many of our resources are based on, which should bring in updated CA certificates and other miscellaneous things.\n\nPreviously resources that had params involving lists of objects would cause Concourse to blow up instead of working. It should now work.\n\n","depth":2,"section_tag":"v0750"},"v0760":{"location":"download.html#v0760","title":"v0.76.0","text":"v0.75.0 introduced a client-side limit of 64 connections to the database, which no one would ever hit so we didn't bother putting it in the release notes. Then a bunch of people with large deployments hit it and their Concourse went cold. Sorry.\n\nWe're removing the limit and are going to do some investigation into the ATC's connection pool characteristics before considering adding it back.\n\nThe pipelines sidebar is now scrollable. Some of y'all had a lot of them and got tired of buying larger monitors.\n\nJobs can now have their manual triggering disabled, via disable_manual_trigger.\n\nThe BOSH deployment resource now supports BOSH 2.0 manifests. Previously it would explode instead.\n\nThe ATC can now be configured to authenticate against a GitHub Enterprise deployment, thanks to @aequitas!\n\nCleaned up some internals to fix the root cause some noisy but harmless log lines (failed-to-lookup-ttl).\n\nThe Semver resource now supports OpenStack Swift as a storage backend, thanks to @ChrisPRobinson!\n\nThe Time resource can now be configured to only yield new timestamps on certain days of the week, thanks to @joek!\n\nfly learned the fly rename-pipeline command, thanks to @zachgersh!\n\nThe Docker Image resource should now be more durable to flaky Docker registries, by retrying with exponential backoff on network errors or 5xx responses.\n\nThe BOSH deployment resource now downloads the deployment manifest when used as a get step.\n\nPreviously the Pool resource would require you to specify retry_delay in nanoseconds, which was a bit silly. It now accepts Go duration format, e.g. 30s.\n\nThe Tracker resource now correctly handles rejected stories by only delivering them if a new commit has been made after they were rejected.\n\n","depth":2,"section_tag":"v0760"},"v100":{"location":"download.html#v100","title":"v1.0.0","text":"We made it!\n\nThis release, although relatively small on its own, is built on years of feedback and iteration. So these notes will be a bit more broad and cover all the things you may have missed since you last checked in on our little CI system.\n\nFirst off, a huge thanks to Pivotal for sponsoring our project and letting us work on it full-time. Over the past year we've had 17 team members rotating through, including 2 designers. Pretty sweet.\n\nWith 1.0.0 comes a more rigid release policy on our end. You may see deprecations here and there, so keep an eye out for those via the tags next to each release note, but nothing should change backwards-incompatibly until 2.0.0. We'll still be releasing at the same cadence as before, so we'll probably end up at v1.23.0 pretty soon.\n\nHere's a text-form 80's montage of all the things you may have missed since v0.17.0, our first release:\n\nSteps replaced the old style job config.\n\nA standalone binary distribution of Concourse has been introduced. (Download links to the right.)\n\nCaching and more efficient artifact propagation: resources fetched by get steps are cached on the workers and efficiently propagated throughout steps in the build plan.\n\nA single Concourse can be configured with multiple pipelines dynamically.\n\nGitHub auth!\n\nCustom resource types can now be added via Resource Types in the pipeline, rather than reconfiguring your workers.\n\nLots of performance improvements and optimizations, and resilience to flaky networks.\n\nThe fly CLI has been entirely rewritten and is much more consistent in UX.\n\nConcourse knows its own version number and will warn you if your CLI is out of date.\n\nTasks have explicit inputs and outputs, making artifact consumption and production a lot easier to follow.\n\nA new color scheme that's more colorblind-friendly.\n\nA whole bunch of improvements to core resources.\n\n...and now for the actual 1.0 release notes, if you're upgrading from v0.76.0:\n\nThe Concourse BOSH release is now built for BOSH 2.0. You will need a recent director to upgrade.\n\nResources backed by a resource type defined in Resource Types will now periodically check for new versions of the resource type and use the latest one for checking. Previously the same container would be reused forever even if a new version of the resource type was released.\n\nWe've added aria-label attributes to all buttons in the UI, which should improve accessibiltiy for folks using screen readers. Still a ways to go overall, but this is a start.\n\nLots of dots in sequence in build output will now word-wrap once again.\n\nThe BOSH release can now be configured to use GitHub enterprise endpoints for GitHub auth.\n\nConnections from ATC to Baggage Claim will now retry on connection errors.\n\nFixed an issue where volumes would \"expire\" even though a build was still using them. Did a bunch of refactoring and now it should all be pretty airtight.\n\nWe've fixed a goroutine leak on the ATC which would occur every time image_resource was used.\n\n","depth":2,"section_tag":"v100"},"v110":{"location":"download.html#v110","title":"v1.1.0","text":"Workers can now configure proxies to use for containers that are spun up on them.\n\nIf you're using the binaries, all you have to do is set the standard $http_proxy, $https_proxy, and $no_proxy environment variables. There are also equivalent flags you can pass to concourse worker, which were added for discoverability's sake.\n\nIf you're using BOSH, just set the http_proxy_url, https_proxy_url, and no_proxy properties on the groundcrew job.\n\nA task's run can now specify the working directory by setting run.dir.\n\nfly learned the fly targets command, which, surprise surprise, lists the currently saved targets.\n\nThe blackbox job in the BOSH release will now once again emit logs, by autodiscovering them from /var/vcap/sys/log/*/*.log.\n\nFixed rendering of leading whitespace on lines of output in build logs.\n\nFixed the scrolling behavior of the pipelines sidebar list to not cut off the last couple of entries.\n\nThe Docker Image resource is now durable to resource images that do not contain a /etc/password file.\n\nPreviously renaming a pipeline made bad things happen to the automatic resource checking and scheduling for said pipeline. Instead of doing this it now renames the pipeline and the pipeline continues to work.\n\nPreviously a put step occurring at the start of the plan would not have its source directory created (as there were no artifacts), which would cause some resources to break. We now ensure this directory exists.\n\nYou can now scroll up more easily when viewing a finished build.  You are all free now!\n\nYou can now run fly help and it'll show its help text instead of \"unknown command.\"\n\nPreviously if you had an entry in Resource Types and Resources with the same name the ATC catch on fire. It now doesn't.\n\nUsers who are present in more than 30 GitHub organizations and/or teams can now authorize with Concourse. You should be rewarded for your popularity.\n\nPiping input into fly intercept will now send an EOF when the input is exhausted (e.g. echo foo | fly intercept ... cat).\n\n","depth":2,"section_tag":"v110"},"v120":{"location":"download.html#v120","title":"v1.2.0","text":"fly learned the fly check-resource command, which allows you to force detection of versions, notably those in the past. This is useful if you've configured a new resource but want to use a version that's not the latest one.\n\nAs part of this change we've slightly tweaked how check: Check for new versions. works (in a backwards-compatible way). Your check script should now include the requested version in the response if it's still valid. This is so that you can run check-resource with the version that you want, rather than the one before it.\n\nget steps can now be pinned to a specific version.\n\nExample:\n\nplan:\n- get: my-repo\n  version: {ref: cb0ed22c4cfc6b7524bcafc1664b2d27035521f9}\nThis will lock the my-repo step to the specified version. Note that the version must be valid, must be collected in the resource's version history (which means you may want to use fly check-resource), and must also satisfy any passed constraints listed on the step.\n\nSee version for more information.\n\nget steps can now be configured to run with every version of its resource, rather than skipping to the latest.\n\nExample:\n\nplan:\n- get: pull-requests\n  version: every\nThis will allow the build to run with every version of the resource, which is probably a bad idea for certain git repos (where folks may push 100 commits at once), but can make a lot of sense for other things (security auditing, handling all pull requests, processing commits across multiple branches, etc.).\n\nSee version for more information.\n\nWe've fixed the rendering of multi-field versions in the UI to be substantially less confusing.\n\nAs part of this we've tweaked how we render steps in the UI. The checkboxes are now more subtle and less button-like, and aggregate steps look cooler.\n\nThe ATC now validates that its URL flags are valid URLs. Previously you could configure an --external-url of example.com, which is missing the scheme, so some things would break.\n\nWe've bumped to Go 1.6.1. You probably don't care.\n\nThe docker-image resource now requests the correct schema version of manifests from the registry, which should fix cases where it would pull the wrong digest.\n\nThis is thanks to a PR from @databus23.\n\nThe s3 resource now issues a shouty warning if you're still using from and to.\n\nIt's configured to blink but our web UI doesn't (YET) support blinking text. Consider this a warning. You have one release to comply or be met with red, blinking text in your builds.\n\nThe semver resource can now be configured with an identify for the commits made with the git backend, thanks to a PR from @shinji62.\n\nfly trigger-job now has a -w flag for watching the build that was created.\n\nfly now respects $http_proxy and $https_proxy for communication to the Concourse server, thanks to a PR from @ArthurHlt.\n\nThe docker-image resource now has a tag_as_latest param for tagging the image with latest, in addition to any specified tag, thanks to a PR from @shinji62.\n\n","depth":2,"section_tag":"v120"},"v130":{"location":"download.html#v130","title":"v1.3.0","text":"We have switched Garden backends to Garden runC. This new runC-based backend has proven in our testing to be far more portable, allowing our binaries to work on just about any stack that's using a recent enough Linux kernel (3.19+).\n\nAs part of this upgrade, your existing workers will need to be recreated.\n\nWith BOSH, you can do this with bosh deploy --recreate when deploying the new releases.\n\nFor the binaries, you'll need to stop the old worker, nuke the --work-dir, and then start the new one.\n\nIn addition, we now explicitly manage all aspects of container images. This should dramatically reduce disk usage on your workers, as there's no longer a redundant copy from importing the image into Garden's graph, as long as you're using image_resource. This also means we're now using btrfs for the whole stack, which makes running Docker in Concourse tasks much easier.\n\nAs part of this, the binary distribution no longer supports image in the task config. Supporting it has always been a portability nightmare, and we've been discouraging use of image for some time now.\n\nJobs can now be configured with build_logs_to_retain, which is a number indicating how many builds for which to keep the build output. All build logs except for the most recent N builds will be reaped. You can flip this on for already-existing jobs with thousands of builds and we'll slowly reap them in batches.\n\nA task step in a plan can now be configured with an image field specifying an artifact source to use. This allows for build-and-test flows, where your pipeline produces an image and then propagates the exact image to a task that uses it as its rootfs.\n\nfly volumes now includes much more information about each volume, including its disk usage. This should help track down what's using so much disk, and whether you really just need more space to accomodate your workload.\n\nA hg resource is now included as part of the core distribution.\n\nWhen a build is stuck \"waiting for a suitable set of input versions\", it will now show what input it cannot find versions for, and why.\n\nPreviously workers could end up with very poor balancing of containers, in the worst case resulting in one worker handling the bulk of the resource checking load. We now balance checking across workers over time, by only reusing the check containers for up to an hour.\n\nThe ATC itself can now be configured to listen with TLS, rather than relying on an upstream component like HAProxy or an ELB for SSL termination.\n\nThis also means the ATC can handle HTTP/2 traffic, thanks to Go's magic net/http package. We've seen noticeable speed boosts in the web UI from this alone.\n\nWhen TLS is configured the ATC will redirect any non-HTTPS GET and HEAD requests to HTTPS.\n\nIn addition to HTTP/2, we've done some optimizations that make the pipeline UI much faster and more responsive.\n\nfly intercept's help text now indicates that you can run an arbitrary command.\n\nThe git resource now includes branches and tags in its metadata for each commit.\n\nPreviously the time resource would accidentally report two versions within the boundary of a time range configured with start and stop. This has been fixed.\n\nThe docker-image resource can now be configured with SSL CA certs to trust when communicating with the registry. This allows you to use private registries securely, rather than listing the address as insecure.\n\nThe git resource will now detect the full history of tags when configured with tag_filter, rather than just the latest one.\n\nAll core resources now include bash in their image, which should make hijacking more pleasant. We also stripped out extra stuff from some resources, so on the whole the resource images should be a bit smaller.\n\nWe've bumped all core images to Buildroot v2016.05, and are now continously integrating with Buildroot.\n\nThe git resource can now be configured to NOT skip commits with [ci skip] in them, thanks to a PR from @zachgersh and @ryanmoran. This is useful when you're pointing at commits of an external repo with an unrelated CI.\n\nThe git resource, s3 resource, and semver resource now support basic auth when talking to Git repos, thanks to PRs from @MatthiasWinzeler and @JamesClonk.\n\nThe docker-image resource can now be configured with a registry mirror, thanks to a PR from @gregarcara.\n\n","depth":2,"section_tag":"v130"},"v131":{"location":"download.html#v131","title":"v1.3.1","text":"Bumping Buildroot brought in git version 2.8.2, which breaks handling of nested submodules. We've moved ahead to master of Buildroot which bumps git to 2.8.3, which should fix the issue.\n\nThe github-release resource resource will now retry on failed uploads, up to 10 times.\n\nThe build numbers made in automated commits to the pool resource are now escaped with backticks so that GitHub doesn't auto-link them to bogus issues. Thanks @geramirez!\n\n","depth":2,"section_tag":"v131"},"v140":{"location":"download.html#v140","title":"v1.4.0","text":"We've revamped our container retention configuration.\n\nPreviously, containers used by failed builds would stick around for 1 hour, and containers for succeeded builds would stick around for 5 minutes. This was pretty dumb. It meant if you had frequently failing builds, containers (and disk usage) would pile up, and if you had a build that failed overnight, you wouldn't be able to investigate anything in the morning.\n\nInstead, as long as the most recent build of a job is failed or errored, we'll keep it around indefinitely. It will be let go as soon as a new build finishes successfully, or fails, in which case that build will be retained instead.\n\nWe've fixed a hairy issue that resulted in artifacts sometimes disappearing in the middle of a build. This issue primarily affected users with more than one worker.\n\nThe new container retention semantics also fix the \"volume mounted to container is missing\" bug with hijacking.\n\nWe've bumped the version of the Go AWS SDK used by the S3 resource. This should fix some issues related to long-running uploads and downloads.\n\nfly sync now shows a progress bar. You're welcome.\n\nSome of y'all with BIG DATA had volumes too large to fit their reported size in the database. That should work now.\n\nWe've bumped to Garden-runC v0.4.0, which should fix the iptables \"resource temporarily unavailable\" error.\n\nWe've gone back to a safer method of killing container process when aborting a build. We had initially switched to signalling the parent process and then killing it if it didn't exit after 10 seconds, however in a lot of cases this would just result in things not exiting when the process tree is sufficiently complex. This also resulted in the pool resource not giving up in its attempt loop when aborted.\n\nPreviously if a worker left the pool at an inopportune moment, Concourse would forget about its volumes, which led to things getting into a wedged state. This is now fixed. You should never have to pause your pipeline to \"let it breathe\" again.\n\n","depth":2,"section_tag":"v140"},"v141":{"location":"download.html#v141","title":"v1.4.1","text":"A bug introduced by v1.4.0 caused custom resource types that override worker-provided resource types (e.g. git, s3, docker-image) to lead to containers being created repeatedly until your workers couldn't take anymore.\n\nFixed. Our bad.\n\nThe TLS redirecting feature introduced as part of v1.3.0 made fly execute work only 50% of the time when running two ATCs. With three ATCs it would work 33.3%, repeating of course, of the time, and so on.\n\nfly execute now works 100% of the time.\n\nThe commit message format in the pool resource has been once again tweaked so as to not incorrectly trigger GitHub's issue reference syntax, thanks to a PR from @geramirez.\n\n","depth":2,"section_tag":"v141"},"v150":{"location":"download.html#v150","title":"v1.5.0","text":"When connectivity to Concourse is lost on the pipeline page, a fancy warning message will be shown.\n\nThis started as a PR from @fmy - thanks!\n\nLoading the logs of a build is now much faster (up to 12x improvements have been observed). Rendering performance is unchanged, but we found that for chatty builds the bulk of the time was spent simply downloading the logs.\n\nWe will now only fetch a given resource (including image_resource) once per worker. Previously they would all fetch concurrently and each populate the cache, which would storm the worker with network traffic and CPU load. Now one will start fetching and the rest will wait.\n\nWe will no longer create no-op containers for cache hits. This should reduce the number of overall containers used by the pipeline.\n\nThe build view was only showing the last 100 builds. And none of you noticed! It'll show all of'em now.\n\nBOSH-deployed workers' names will be set to their BOSH instance ID, rather than their hostname. This should make identifying them a bit easier.\n\nThe docker-image resource will now correctly handle private registry URIs without their port included.\n\nWe now limit the total number of database connections to 64 per ATC, and have removed a debugging utility that led to deadlocks when a connection limit was reached (and also may have led to those connection limits being reached in the first place).\n\n","depth":2,"section_tag":"v150"},"v151":{"location":"download.html#v151","title":"v1.5.1","text":"A bug introduced by v1.5.0 as part of the resource fetching synchronizing led to hanging get steps. It affected resources with large values in source or params. It is now fixed.\n\nA task can now specify the user to run the process as by configuring run.user in run.\n\n","depth":2,"section_tag":"v151"},"v160":{"location":"download.html#v160","title":"v1.6.0","text":"We now provide an official Docker repository at concourse/concourse!\n\nAs part of this, the binary distribution has been updated to support environment variables for configuration, in addition to flags. Because the environment is perfectly safe.\n\nThanks to @gregarcara and @MeteoGroup for maintaining Concourse images until we started on this ourselves!\n\nThe bosh-io-release resource will now verify SHA1 checksums, and place them in the fetched directory as sha1. The bosh-io-stemcell resource has also been updated so that they both have the same behavior.\n\nThe docker-image resource now supports ECR! There were a couple issues and pull requests opened for this; thanks to all who kept the ball rolling!\n\n","depth":2,"section_tag":"v160"},"v200":{"location":"download.html#v200","title":"v2.0.0","text":"TEEEEEEEEEEEEEEEEEEEEEEEEAMS!\n\nSo, you'll notice that version number made quite a jump. This is why. The long-awaited \"teams\" feature brings (trusted) multi-tenancy to Concourse.\n\nThe following breaking changes have been made:\n\n* The --publicly-viewable configuration is gone, and is now set on a pipeline-by-pipeline basis, via fly expose-pipeline and fly hide-pipeline. Newly configured pipelines are hidden by default, and all existing pipelines will be hidden upon upgrade, so make sure to expose the ones you intend to be public!\n\n* Many API routes now require the team to specified in the URL, e.g. /api/v1/teams/foo/pipelines. Our API still not yet an official interface to Concourse; we continue to encourage using fly until we turn it into a properly versioned and documented API.\n\n  The web UI routes now also have the team name in them. Old URLs should continue to work, and will now redirect to the new URL.\n\nEverything else (pipelines and such) should continue to work the same as before, only now they'll belong to the The main team.\n\nWe have deprecated the /builds page, previously known as \"the second hamburger menu button\". Its button in the header was taking up valuable UI space, so we unceremoniously demoted it.\n\nMany of you feel bad for this page.  That is because you crazy. It has no feelings! fly builds is much better.\n\nBefore we kill it off completely, we'll make sure fly builds does everything you'd need from the page, which is really just a matter of having a column for the build URL so you can view it in the web UI. Aside from that, fly builds is better in every way: if your next step is fly intercept, it keeps you in the terminal. You can also change the number of results with -c, and filter it to a job with -j, both of which should be much faster for finding what you're looking for.\n\nThe favicon will now change color when viewing a build to reflect its status. Pretty neat, right?\n\nThanks to @zachgersh and @rmasand for the inspiration! - \u003c3 @kimeberz\n\nAs an incremental step in our march towards a fully Elm-based single-page app, we've made some parts of the navigation much snappier. Switching between builds of a job will now update the UI in-place, and toggling pipeline groups now immediately re-renders the existing data set, rather than reloading the entire page.\n\nWe'll be focusing more in the upcoming weeks on bringing more of the web UI up to par, ultimately resulting in one big snappy single-page app (hopefully with none of the gotchas that made me hate them at first, i.e. inconsistent data that forces a page refresh and distrust of the entire app as a result).\n\nPreviously if the database or network became sluggish, ATC's locking mechanism would stop functioning, resulting in multiple ATCs trying to manage the same build, among other things. We've switched to Postgres session locks, which should be much more airtight.\n\nThe ATC now supports being configured with Generic oAuth. Huge thanks to @poida for doing the PR for this!\n\nThe ATC now supports being configured with CF/UAA auth.\n\nThe docker-image resource can now discover older versions. Previously it would only ever emit the current version. This can be used to roll back to a previously known-good image digest.\n\nWe've bumped to Go 1.7 everywhere, and made this process continuous. Whenever Go 1.7.1 or 1.8 come out, we'll automatically pick it up. Turns out there's this pretty neat CI system that can do that kind of thing. You may have heard of it. (It's not Jenkins.)\n\nFly learned the fly-pause-resource and fly-unpause-resource commands, thanks to pull requests from @gregarcara!\n\nThe bar along the top of the page will now turn blue on already-rendered pages if the pipeline the page belongs to is paused.\n\nThe fly login command now accepts a --ca-cert flag, which should be used instead of -k. The cert will be persisted for the target (even if its file goes away).\n\nWe've refactored the internal scheduler component of the ATC, reducing query usage and generally making it easier to work on in the future.\n\nThis refactor also resulting in fixing behavior with version: every.\n\nFixed volume deletion in BaggageClaim on a few platforms. May have been primarily situations where the root disk was btrfs.\n\nThe s3 resource now supports encryption options, thanks to a PR from @jmcarp!\n\nThe github-release resource now creates a body file when fetching a resource, thanks to a pull request from @shinji62.\n\nNow you can continuously read Concourse's release notes with to determine whether to auto-update!\n\nThe Docker repository image now bakes in the default CA certs, thanks to a PR from @billimek!\n\nThe git resource's handling of merge commits now makes a lot more sense.\n\nPreviously, a merge commit would result in the history of the merged branch showing up in the version set. For tools like git log this makes sense, but from a CI standpoint, you only really care about the effect on the branch that it was merged into: it's all or nothing. This is now fixed, and only the merge commit itself will be yielded as a version.\n\nShout-out to @chipx86 for helping us reason through this on GitHub!\n\nThe time resource works now.\n\nIt was pretty broken before, because time is hard.\n\nThere were a couple issues:\n\n* If your start and stop were configured in some non-UTC timezone, say, -0700, it straight up wouldn't work if the times were late enough in the day. For real.\n\n* If you configured days and start and stop, the days would be treated as UTC, rather than respecting the timezone in start or stop. What's more, start and stop could be emitted, leaving there no place for a location for the days to even be specified.\n\n  We've added a location field, which should be used instead of embedded offsets, and then days will respect it.\n\nThe BOSH release will now leave 10GB of space free for the system, rather than allocating all of it for BaggageClaim. This is mainly to make the failure mode better. Without this overhead, BaggageClaim would fill up the host's disk, then fail to write to that, and then panic and go read-only, making it unrecoverable. Now the BaggageClaim volume will still fill up, but it'll at least be able to expire volumes and such, and the host machine will still function within its 10GB overhead.\n\nThis is all thanks to some sleuthing and a pull request from @alext.\n\nThe bosh-deployment resource now has the BOSH cli v1.3262.4, thanks to a PR from @alex-slynko!\n\n","depth":2,"section_tag":"v200"},"v201":{"location":"download.html#v201","title":"v2.0.1","text":"Previously if you were using Safari the pipeline would not render. Well, technically, it would render, but within a \u003cdiv\u003e element with 0px height. We have sighed, flailed at the CSS monster, triggered our web-development pipeline, and prevailed.\n\nAlso the top bar used to shrivel up and die if the size of the content page became too large. It, uh, doesn't anymore.\n\nWe've restored the pre-teams API endpoint for the job status badges, so you all don't have to update your READMEs immediately. Sorry about that. (You should probably still update them, though.)\n\nWe've improved the error message returned when the file used by a task step does not exist.\n\n","depth":2,"section_tag":"v201"},"v202":{"location":"download.html#v202","title":"v2.0.2","text":"Turns out when you wait a month between releases a few things can go wrong once you finally ship. This is, like, probably the last patch on v2.0.0. Maybe. We'll see. We may save our pride and release v2.1.x next irregardless.\n\nLuckily we have this pipeline thing that lets us continuously fix our own mistakes, not just ship them!\n\nPreviously a cluster of multiple ATCs could get into a deadlocked state when checking for resources. This would manifest itself as jobs being stuck in a \"pending\" state. This release, our first ever X.0.2 release, fixes that.\n\n","depth":2,"section_tag":"v202"},"v210":{"location":"download.html#v210","title":"v2.1.0","text":"We've reduced the number of queries by about 60%, including removing constant write loads which may have led to increased CPU usage on RDS.\n\nThe resource page is now much much more responsive. We've rewritten it in Elm, implementing live-updating along the way. It used to take a few (maybe quite a few) seconds to load, and now takes on the order of milliseconds. Pretty rad.\n\nTriggering a build will now update the UI in-place rather than redirecting.\n\nAutoscrolling is back and better than ever before. Keyboard controls for scrolling (e.g. Cmd+Down, Spacebar) should also now work as normal.\n\nWe've updated to Go 1.7.1, which should fix a few DNS-related quirks.\n\nCmd-click and other non-vanilla clicks should now work for build links in the header of the build view.\n\nThe ATC can now be configured with a --auth-duration flag, making the duration for which tokens are valid configurable. This is thanks to a PR from @fmy!\n\nThe git resource now supports GPG verification for commits, thanks to a PR from @alext!\n\nThe docker-image resource now emits the output of docker inspect \u003cimage\u003e as docker_inspect.json, thanks to a PR from @endzyme!\n\nConcourse now rejects traffic from web crawlers by providing a robots.txt. We may make this a bit more targeted in the future, but the intent is to reduce unwanted traffic as there are many many many links to click in Concourse.\n\nThanks to @databus23 the Docker Image resource can now cache things once again! This regressed with Docker 1.10 as the semantics for caching and layer reuse changed to require some additional work/metadata.\n\nThe git resource now supports [skip ci] in addition to [ci skip], thanks to @fmy!\n\n","depth":2,"section_tag":"v210"},"v220":{"location":"download.html#v220","title":"v2.2.0","text":"The Algorithm has become much faster. The Algorithm is what computes the candidate set of inputs for a job, and is the second hardest problem in Concourse (behind the pipeline UI).\n\nIn some cases, e.g. when disabling an oft-used resource version, The Algorithm would go buck-wild and use 100% CPU trying to locate the new set of version candidates. This was undesirable.\n\nLuckily, we prepared for this kind of degenerative case, and made it easy to capture the data sets that replicate the issue. We captured the data set, observed the slowness (had a build running for \u003e12 hours before we gave up), thought long and hard, and did a bunch of work to bring that down to ~19 seconds.\n\nHopefully that's the last of the 100% CPU monster. Overall scheduling performance has also improved across the board.\n\nThe pipeline view will now only redraw if the data has changed. We've also fixed a regression in v2.0.0 that led to redrawing multiple times on an interval, likely leading to the tab crashing if left in the background.\n\nTurns out fly set-team made it stupidly easy to configure a team (or reconfigure an existing team) with no auth credentials. I'm not saying something bad happened, but uh, it'll now warn you and force you to type a really long flag, and even shame you a little bit even when you use it.\n\nThe fly binaries are now build natively on each platform, rather than cross-compiled. This removes a few surprises like native DNS and OS X Keychain functionality not working.\n\nThey're also now available for download alongside the rest of Concourse, rather than having to download from a Concourse installation.\n\nThe --auth-duration flag introduced in v2.1.0 is now available as a BOSH property (auth_duration, surprise surprise). Thanks to @JamesClonk for the PR!\n\nfly checklist now generates a Checkfile with the team name present, thanks to a PR by @Amit-PivotalLabs.\n\nBe sure to upgrade Checkman as well for this to work.\n\nGeneric oAuth now supports checking presence of a scope, thanks to a PR by @LinuxBozo!\n\nThe docker-image resource now supports build args, thanks to a PR from @o-orand!\n\nfly sync will bail early if the versions already match, thanks to a PR from @geofffranks!\n\nTurns out Chrome is really, really bad at rendering our build page now. We've made some improvements to this but I think more work is ahead of us.\n\nCompared to Firefox and Safari, Chrome seems to redraw the entire dang page on every friggin update. Which means every second when we update that stupid little ticker up top, the whole page and all its output repaints.\n\nFor shame, Chrome.\n\nIf it's unbearable you can try Firefox or Safari, which seem to render more sensibly.\n\nWe've also fixed a bug that led to interpreting the event stream multiple times for builds that have a ton of output, thereby making things even slower and jankier.\n\nThe docker-image resource skip_download parameter now works again. This broke in v2.1.0. Sorry about that.\n\n","depth":2,"section_tag":"v220"},"v221":{"location":"download.html#v221","title":"v2.2.1","text":"Finished up the build rendering performance fix on Chrome, which only affected Chrome because Safari and Firefox didn't render Flexbox properly, which is also why their autoscrolling didn't work.\n\nWeb. Development.\n\nFixed autoscrolling in Safari and Firefox.\n\nMay have talked up the algorithm release note a bit much. Someone immediately found another case where the 100% CPU monster struck.\n\nThis is fixed now. Trust me.\n\nThe bosh-io-stemcell resource has been rewritten in Go with tests and such, thanks to a PR by @zachgersh!\n\nIt now does parallel downloads, to boot.\n\nThe ATC can now be configured with a Riemann service prefix, thanks to a PR by @mastertinner!\n\nFixed an issue where the exponential backoff when talking to a flaky worker would never give up.\n\nThe Windows fly download link didn't work in the binary distribution. Because of the .exe suffix. Oi. Fixed.\n\nBOSH-deployed workers will now be named after a frankenguid taking parts from the BOSH instance ID and their hostname. This is to make it so you can correlate the worker to the BOSH instance, while also guaranteeing that when the worker is recreated it comes back under a new name.\n\n","depth":2,"section_tag":"v221"},"v230":{"location":"download.html#v230","title":"v2.3.0","text":"The whole UI now runs as a single Elm app! Pages should load much quicker, and the pipeline sidebar now remains open as you navigate around.\n\nThere's still some UX work to be done to make things a big smoother, e.g. better handling for 404 cases and more consistent loading indicators, but this is the first big step on that path.\n\nThe team name is now provided as $BUILD_TEAM_NAME along with the rest of the metadata available to resources, thanks to a PR from @SHyx0rmZ.\n\nFixed the log out menu being unclickable on the build page.\n\nThe sidebar no longer scrolls offscreen.\n\nThe github-release resource now supports publishing pre-releases, thanks to a PR from @ahelal!\n\nThe git resource can now have LFS disabled via a disable_git_ls param, thanks to a PR from @SHyx0rmZ!\n\nUnused resources in the pipeline config are now a validation error, thanks to a PR from @mmb!\n\nThe BOSH release can now be configured with arbitrary Riemann tags, thanks to a PR from @combor!\n\nWhen configured with a CloudFront endpoint, the s3 resource will now download via CloudFront, which should be much faster. This is thanks to a PR from @cunnie and @ljfranklin!\n\nThe s3 resource now supports v2 signature signing, thanks to a PR from @JamesClonk!\n\nThe bosh-deployment resource can now be configured to not redact properties from the deploy diff, thanks to a PR from @jszroberto!\n\n","depth":2,"section_tag":"v230"},"v231":{"location":"download.html#v231","title":"v2.3.1","text":"Fixed middle-clicking and other modifier keys when clicking on jobs/resources in the pipeline view.\n\n","depth":2,"section_tag":"v231"},"v240":{"location":"download.html#v240","title":"v2.4.0","text":"Worker keys can now be authorized for only a particular team. This prevents workers from being unintentionally (or maliciously) registered as a global worker, in the case where an operator is granting an external worker access to the cluster.\n\nConsult web --help for CLI docs or bosh.io for BOSH docs.\n\nWe've lowered the default memory/CPU usage of the concourse/lite Vagrant box to 2GB and 2 cores, down from 6GB and four cores, thanks to a PR from @jwiebalk!\n\nBaggageclaim will now be more durable to corrupt volumes. Previously a borked metadata file would effectively wedge the Baggageclaim API, making the worker unrecoverable. You would see an error like \"failed to list volumes\" in your builds. Baggageclaim will now pretend these volumes don't exist in the API, and reap them from the disk.\n\non_failure, on_success, and ensure can now be attached to a job, thanks to a PR from @jmcarp!\n\nfly login will now automatically transfer the token to the CLI for the oAuth flow, rather than requiring you to copy-paste it.\n\nFixed the behavior of the \"home\" button. It will now take you to your current pipeline, rather than always taking you to the first one.\n\nAfter logging in, the UI will now reflect that you're actually logged in. This used to require a refresh. Single page apps giveth and they taketh away.\n\nWhen viewing a build or a job, the groups the job are in will now be highlighted, rather than always the first group.\n\nFixed a janky synchronization issues when updating the top bar while switching between pipelines; it used to sometimes show the previous pipeline and never update.\n\nThe favicon will now reset back to the default \"grey\" flavor when switching from a build to any other page.\n\nLogging in will now redirect you back to where you were if it was initiated by some attempted action.\n\nThe bosh-io-stemcell resource now correctly returns versions in chronological order.\n\n","depth":2,"section_tag":"v240"},"v250":{"location":"download.html#v250","title":"v2.5.0","text":"Teams can now be destroyed via fly destroy-team.\n\nFixed a hairy deadlock that could lead to jobs getting stuck \"waiting for a suitable set of input versions\". We fixed it, like, really hard. Like the lock isn't even THERE anymore, man. (And it's not needed anymore, either. That's important too.)\n\nThe cf resource now has the latest CLI version again. Unbeknownst to us, the CLI team switched buckets, so we stopped getting new bits.\n\nWe've fixed the CLI download links on the \"no pipelines\" page.\n\nThe fallback flow in login for accepting the token manually is now fixed, thanks to a PR from @sharms!\n\nThe bosh-io-stemcell resource will now aggressively retry downloads, thanks to @zachgersh and @ljfranklin!\n\nThe s3 resource now supports setting a Content-Type for the file being uploaded, thanks to a PR from @pdelagrave!\n\n","depth":2,"section_tag":"v250"},"v251":{"location":"download.html#v251","title":"v2.5.1","text":"Soooo you may have noticed Chrome being really slow lately, especially the autocomplete in the URL bar. We had a bug that led to an infinite redirect loop, causing a bunch of very large URLs to enter the browser history. We've fixed this now. You may want to clear your history to speed Chrome up again. Sorry.\n\nThe concourse/lite box will now add Google DNS to the tail of the DNS chain, rather than the head, allowing local DNS resolution settings to be tried first. This is thanks to a PR from @iMartyn!\n\nThe docker-image resource will now propagate the correct --mtu value to the daemon, fixing image fetching flakiness on IaaSes like GCP, which have a default MTU lower than 1500.\n\nWhen using --ca-cert with fly login, the cert will be appended to the system cert pool, rather than an empty pool. This way the cert will be verified in the case where it's an intermediate cert signed by a root CA in the system pool.\n\nThe git resource can now be configured to force-push, thanks to a PR by @dfedde-pivotal! Use with care.\n\nThe docker-image resource now supports ECR urls in the FROM section, thanks to a PR from @donaldguy!\n\nThe build events API endpoint will now return the X-Accel-Buffering header, which hints to reverse proxies to not buffer the response, thanks to a PR from @jasonkeene.\n\nFixed the janky autorefresh on the job page.\n\n","depth":2,"section_tag":"v251"},"v260":{"location":"download.html#v260","title":"v2.6.0","text":"Workers will now, by default, wait for builds to finish before exiting. This will make it safer to perform a rolling update of a Concourse cluster.\n\nIf you're running a BOSH deployment, this feature will just start happening automatically. If you're running the binary distribution or Docker, you'll need to invoke land-worker (for a temporary in-place update, i.e. preserving containers and volumes) or retire-worker (for a permanent exit) to initiate draining, and then wait for the worker process to exit.\n\nMore docs on this are forthcoming; this release was expedited by the time resource bug, so docs are sparse at the moment.\n\nWorkers that have not heartbeated in a while will now enter a stalled state rather than just disappearing. This should improve resilience to network blips and makes the worker lifecycle much more explicit, allowing us to distinguish between accidentally-unavailable workers and intentionally-removed workers. This way we can continue to retry and wait for the worker to return.\n\nNew workloads will not be placed on stalled workers. Stalled workers that will not be coming back can be cleaned up with the new command, fly prune-worker.\n\nThe time resource did not know to compare years. Yep. Pretty silly. So any interval triggers stopped triggering. Time is hard.\n\nFly learned the fly logout command, which can be used to forget a target and its token. This is thanks to a PR by @mkreibe!\n\nFly learned the fly validate-pipeline command, which can be used to...validate a pipeline. It does this without needing an external server, either, making it handy for quick local verification or automated testing.\n\nThis was a PR submitted by @jmcarp - thanks!\n\nPreviously if you configured a job with multiple get steps with the same name, the job would never be able to schedule. @cnelson did a PR to add a validation for this borked configuration - thanks!\n\nThe fly CLI should now support colors on Windows, thanks to a PR from @alex-slynko!\n\nThe BOSH release was changed to default Baggageclaim to consuming (disk size - 5GB), which was all fine and good until your disk was \u003c= 5GB, which caused it to fail. It will now use the full 5GB. Though...you should probably just get more GBs.\n\nResource checking can now be directed at a tagged worker by specifying tags on the resource.\n\n","depth":2,"section_tag":"v260"},"v270":{"location":"download.html#v270","title":"v2.7.0","text":"A long-standing bug in Golang's golang.org/x/crypto/ssh package has been fixed. This bug led to workers becoming stuck and/or unregistered after 1GB of data was transferred over their SSH connection. This resulted in builds being stuck in a pending/started state, and resource checking no longer occurring.\n\nThis bug affected many people with workers forwarding their registration through the TSA, as is the default for the binary distribution, and is a common configuration for external workers.\n\nFor more info, read on.\n\nContext: Workers register via a single long-lived SSH connection. As a baseline, heartbeating and logging goes over this connection, but if the worker is forwarded through the TSA, all API calls and data transfer will also be sent over this encrypted connection (rather than directly to the worker).\n\nThe bug: Per the SSH RFC, after some amount of data transfer (1GB by default), a new key is negotiated, so that the connection's encryption has sufficient entropy. The Golang library had a logic error that led to a deadlock during this key negotiation. This led to the connection being \"alive\" but with the SSH server no longer able to transfer data to and from the worker. This meant API calls would hang, and the worker would eventually unregister as it would fail to heartbeat. The client-side of the worker registration would be stuck waiting for a keepalive response, and so it would never break the connection and recover.\n\nGitHub issues #18439, and later #18711 and #18850 track the journey through debugging and the path to the fix. Thanks to @hanwen for fixing it, and @databus23 for helping keep track of all this!\n\nThe fly targets command will now show the team saved for each target, thanks to a PR from @joonas!\n\nThe fly login command will now remember the team you were targeted with, making it easier to log back in to the same team and have per-team targets.\n\nPreviously if you configured an --external-url using a hostname (e.g. http://some.dns.name:8080) the ATC would have bogus links for the login flow. This has been fixed.\n\nPreviously if you were logged in, and then your cookie became invalid but didn't expire (e.g. your session signing key got rotated, possibly via a stemcell update), you wouldn't be able to log in again via basic auth until the cookie was deleted. This has now been fixed.\n\nThe ATC's --development-mode flag has been removed in favor of having an explicit flag for --log-level and a flag for --no-really-i-dont-want-any-auth. The BOSH properties have also been updated accordingly.\n\nPreviously if the ATC startup was interrupted at an inopportune moment during first-time setup, the internal table for tracking migrations progress could be left in a partially-created unrecoverable state. This has been fixed.\n\nWe've fixed an issue in the worker lifecycle wherein a worker that was landing blew up and then tried to come back under a different name, but with the same IP. This could happen if a worker was initially being landed normally, but then a cosmic ray blasted into your infrastructure's datacenter and caused BOSH to recreate the VM instead.\n\nNow, instead of the new worker being unable to register, it'll...be able to register. The old, cosmic-ray-obliterated worker will still be around (under the original name, in landed state), and you'll just have to run fly prune-worker to clean it up.\n\n","depth":2,"section_tag":"v270"},"v271":{"location":"download.html#v271","title":"v2.7.1","text":"If you have a team configured with Generic oAuth, you'll want to upgrade to this release ASAP. Previously a check was missed for this provider in particular that allows users to obtain a token.\n\nWe have fixed this hard by making it impossible to forget to update that code path in future PRs. Providers now have a full interface to fill out, rather than having code that implements them strewn throughout the codebase.\n\nThis fix closes a CSRF security hole in the ATC API. Previously someone could fool you into clicking a link that executes destructive AJAX requests on your behalf. This was possible because the ATC API permits cookie-based auth so that JavaScript EventSource requests (used for streaming build logs) could be authorized.\n\nA few headers for security hygiene are now configured on the ATC: X-XSS, X-Frame-Options (configurable; default off so your CI displays will still work), and a few parameters on the cookies that the ATC uses.\n\nPreviously the fly_local_port query param used in the fly login flow for oAuth could be munged in such a way to send the token to an arbitrary website. This meant that someone could fool you into clicking a link that sent a valid token to themselves. This is now fixed by validating that the query param is numeric.\n\nPreviously the redirect query param used in the redirect flow for oAuth could be set to any URL, sending you there after you log in to your oauth provider. Now Concourse verifies that the URL is relative to your Concourse domain, and returns a 400 otherwise.\n\nFixed a memory leak in the TSA. The leak occurred every time a connection was made to Garden or Baggageclaim on a forwarded worker. This took us a while to notice and fix because we're already running the code targeting v2.8.0, which includes a large refactor which results in fewer network calls. Thanks to everyone who helped us dig into this!\n\nThe Darwin binary no longer checks that you're running it as the root user. This was initially added because in principle you can run tasks as particular users, but this feature is not well-supported yet, so it's easier to just run the binary as a user that can e.g. talk to Xcode.\n\nWe'll add this check back once we properly support running tasks as custom users.\n\nThe TSA can now be configured to register against multiple ATCs, rather than just one URL. It will pick a random ATC every time it heartbeats.\n\n","depth":2,"section_tag":"v271"},"v272":{"location":"download.html#v272","title":"v2.7.2","text":"A feature originally intended for 2.8.0 snuck in to 2.7.1 and caused breakage around SSL communication. We've disabled it by default. Sorry about that!\n\nWe'll do more extensive testing for this feature by the time it makes it to 2.8.0.\n\nThe connection to Postgres can now be configured with SSL. Along the way we've also broken the single opaque --postgres-datasource flag on the web binary into multiple more descriptive flags, which should make it easier to discover what you can or should configure. To see the flags, consult web --help.\n\nNote that the binaries still default --postgres-sslmode to disable for backwards-compatibility. Unfortunately the configuration value of prefer is not available in our Postgres DB driver of choice, so it was either require SSL by default in all configurations (which would be unreasonable for small local deployments) or just leave it off by default.\n\nThe BOSH release has always been configured via discrete properties, rather than a single DataSource, and now has a postgresql.ca_cert property among others. Consult bosh.io for more information.\n\nFixed a couple quirks related to our security fixes that affected folks with colons or any other funny characters in their pipeline names. Moral of the story: never use a regexp if you can help it. Also, y'all have weird pipeline names.\n\n","depth":2,"section_tag":"v272"},"v273":{"location":"download.html#v273","title":"v2.7.3","text":"Fix support for postgresql.address in BOSH manifests. Turns out if you type \"host and port\" long enough you start typing \"hort\" instead of \"host\". Don't laugh.\n\nThe git resource now supports pushing with merge: true, which is analogous to rebase: true but works by merging the remote branch into the current HEAD before pushing. This can be useful to preserve the history of your local commits, i.e. if there's a version tag pointing to HEAD and you don't want commits that aren't technically in the tag to be behind it.\n\nThe git resource now supports adding and pushing notes, thanks to a PR by @ahume!\n\nThe git resource, when configured with rebase: true, would previously discard merge commits, losing historical accuracy of the branch. It now preserves them, via --rebase=preserve.\n\nThe docker-image resource now uses a better caching strategy for cache: true, thanks to a PR from @databus23!\n\nThe docker-image resource no longer makes strict assumptions about the format of the repository, supporting repository names like my-repository.biz/a/b/c, thanks to a PR by @ashb!\n\nThe github-release resource resource now supports (and encourages) configuring it with owner, rather than user, which is closer to the GitHub API terminology (and generally makes more sense, since e.g. concourse is an organization, not a user). Thanks @krishicks for the PR!\n\nThe hg resource now explicitly checks against the configured branch. This was a longstanding bug fixed with a lot of patience from @Fydon and assistance from @andreasf - thanks to both!\n\nThe tracker resource previously got 400 errors back when checking for story activity, due to an API change on Tracker's part. This may have been fixed by them by now, but there was also a fix in the resource to not send a trailing ? when the query params are empty.\n\n","depth":2,"section_tag":"v273"},"v274":{"location":"download.html#v274","title":"v2.7.4","text":"We've bumped Garden-runC to 1.5.0 which fixes a bug which affected the generation of /etc/resolv.conf in some cases.\n\n","depth":2,"section_tag":"v274"},"v275":{"location":"download.html#v275","title":"v2.7.5","text":"The ATC can now be configured to emit metrics directly to InfluxDB. We've also made it easy to extend the ATC with support for more metrics emitters. For examples to reference when implementing your own, see the InfluxDB, Lager, and Riemann emitters.\n\nNote that if you switch from an \"ATC -\u003e Riemann -\u003e InfluxDB\" stack to \"ATC -\u003e InfluxDB\" (direct), you'll have to blow away your metrics database. :( This is because when Riemann emitted our metrics it emitted the values as float, and now they're int. The first point emitted to InfluxDB determines the schema, so the new writes do not succeed.\n\nAlso note that if you were previously relying on the metrics being emitted to the logs, you'll now need to pass the flag --emit-to-logs. This is now modeled as its own metric emitter and has been made opt-in, as otherwise it could just be producing a lot of useless logs if you've already configured a metrics sink or just don't care.\n\nThe ATC will now retry connecting to Postgres when the server-side connection limit is reached. It uses fancy Exponential Backoff Technology™.\n\nUpgrading from v2.7.0 to any of the previous releases would leave the user in a broken state until they logged out and back in. It would show that you're logged in, but any actions taken (e.g. pausing a job or triggering a build) would silently fail. They will now send the user to the log-in page instead.\n\nPreviously, with multiple tabs open, logging in to one tab would leave the other tabs in a weird half-logged-in state (due to our CSRF security fix). It should now...not do that.\n\nTo the user this may have appeared as being prematurely logged out, because no one really keeps track of which tab is which. We suspect this is the actual root cause.\n\nWe've bumped Garden-runC to 1.6.0 which fixes a regression that caused /etc/hosts and /etc/resolv.conf to be unmodifiable.\n\n","depth":2,"section_tag":"v275"},"v276":{"location":"download.html#v276","title":"v2.7.6","text":"We forgot to add the BOSH properties for configuring InfluxDB. Ignore these release notes and read v2.7.5 and pretend that we didn't mess up.\n\nThanks.\n\n","depth":2,"section_tag":"v276"},"v277":{"location":"download.html#v277","title":"v2.7.7","text":"We forgot to add the flags for configuring InfluxDB. Ignore these release notes and read v2.7.5 and pretend that we didn't mess up.\n\nThanks.\n\nAgain.\n\nReally though: we had done this work on the branch for the upcoming v2.8.0 release, and cherry-picked the work over for v2.7.5 and v2.7.6, but in our haste missed pulling over these changes to the different distribution formats. We've got test suites for this stuff, obviously, but there's not much coverage for metrics and other configurations that are relatively low-risk but costly to test.\n\n","depth":2,"section_tag":"v277"},"v300":{"location":"download.html#v300","title":"v3.0.0","text":"This release requires an update to your workers. You may want to upgrade them first, actually. If you don't the builds will go orange. But maybe you don't care. Read on for more info.\n\n{image: images/life.gif}\n\nMany many moons ago, in the year 2016, we embarked on a noble goal of refactoring how we do, like, everything. The issue started as More explicit worker, container, volume lifecycles, and came to be known simply as \"life\". There were many puns. They were great at first. But some people took them too far. @joshzarrabi.\n\nAnyway, we're done now.\n\nWith this upgrade, you should notice an overall reduction in container and volume counts across your workers. You should also see a substantial decrease in database queries to Postgres and network calls to Garden and BaggageClaim, as all the container and volume heartbeating is now gone.\n\nIf you're interested in the refactor, read on.\n\nThe general idea is to switch away from creating containers and volumes willy-nilly and nagging the worker every 30 seconds to keep them around. Instead, we create containers and volumes that are associated to a richer schema such that we don't have to keep heartbeating and know exactly when it should go away. So to keep something like a cache around indefinitely, we just don't destroy it, rather than pinging it all the time.\n\nBuilding on the richer schema, we are also more able to determine when we can re-use a container. For example, if you have the same exact resource configured across 10 pipelines, that will result in only one check container, rather than 10. This is because there's an abstract notion of a nameless \"resource config\". We still create one container per team so that fly intercept can't break an entire resource's checking in a multi-tenant environment.\n\nWe'll also explicitly remove containers and volumes, rather than relying on Garden and BaggageClaim to kill them once we stop caring about them. This will surface failures to delete in a way that's much easier to notice. This also means that if the ATC goes away for 5 minutes, all its containers and volumes stick around, rather than being mercilessly killed.\n\nA design document for all this will be forthcoming.\n\nWorkers are now versioned. This will allow the ATC to ignore workers that are too old if it requires a new feature or protocol change. The fly workers command will now show the version of each worker and warn you if any are out of date.\n\nAny existing workers you have will be ignored until they are upgraded, so if you upgrade your ATC first, builds that are in flight will fail to resume. If you upgrade the workers first, though, the builds will probably succeed.\n\nIn a task config, you used to be able to configure a URI specifying the rootfs for the underlying container via a config known as \"the other image, like, the one you should never use\".\n\nWe renamed it to something more descriptive and harder to confuse with \"the good image, like, the one you should use\": rootfs_uri.\n\nWhen fly intercepting a task that uses image_resource, you will no longer be prompted to intercept the check and get containers for its image. You probably never actually wanted to do that so it was more annoying than helpful. This is something that we got for free as part of the life refactor.\n\nSupport for web hooks has landed, thanks to a PR from @mainephd with help from @billimek and others! This was a long-awaited feature that should make many GitHub Enterprise admins very happy.\n\nThis feature is implemented in a way that requires no changes from any resources - it \"just works\". There's no special integration with GitHub, BitBucket, or any hosted offering; you just configure webhook_token in your resource and pass the hook URL to your service.\n\nBuilding on our fancy new schema, we now make sure to keep the image used by a one-off build around for 24 hours. Previously it would expire, like, whenever. I don't know man. It works now.\n\nWe've refactored how auth providers are configured such that all code for a given provider can be defined within a single package. Providers also have an interface to fill out that should cover everything a provider needs to do. This should make it easier (and safer) to submit PRs for auth providers.\n\nThe TSA can now be configured with multiple ATC API endpoints to register with, for HA.\n\nFixed a UI quirk with the sidebar in Firefox, thanks to a PR by @archSeer!\n\nWhen a resource is failing to check for a meta-level thing like not being able to create its container, the error will now be surfaced in the UI, thanks to a PR by @davidje13!\n\nThe docker-image resource now uses Docker v17.05.0-ce, which notably includes support for multi-stage builds!\n\nBaggageClaim will now do more efficient copying on Windows, thanks to a PR by @jdeppe-pivotal!\n\nThe s3 resource can now upload files larger than 5MB to GCS, thanks to a PR by @ljfranklin! This was a complicated intersection of API incompatibilies between IaaSes and assumptions made by SDKs. If you want to relive my confusion, see this comment on the PR.\n\nConcourse looks a bit less jank in Firefox, thanks to a PR by @archSeer which fixes the sidebar padding!\n\nThe hg resource supports a more general revset_filter configuration, thanks to a PR by @cdevienne!\n\nFly learned the fly teams command, thanks to a PR by @joonas!\n\nThe ATC will now emit a worker volumes metric.\n\nThe http response time metric now includes the request method, thanks to a PR by @aditya87!\n\n","depth":2,"section_tag":"v300"},"v301":{"location":"download.html#v301","title":"v3.0.1","text":"Fixed a regression in the handling of put steps that have no inputs. Prior to v3.0.0 we would ensure that the directory given to /opt/resource/out would exist even if there are no inputs, so that resources don't have to do a mkdir -p.\n\nWe now once again ensure the directory exists, and have coverage to ensure this doesn't happen again.\n\nFixed a regression in task steps causing their params to not be set in the environment when hijacking. This got lost in a cleanup and is now handled in a much simpler way, and also tested so it doesn't regress again.\n\nThe fly set-pipeline command will now exit 0 if the user bails out.\n\n","depth":2,"section_tag":"v301"},"v310":{"location":"download.html#v310","title":"v3.1.0","text":"Resources are now unprivileged by default. We've been wanting to do this for a while for security reasons (especially in the context of custom resource types), but we ended up needing to do it anyway as part of the change to overlay (two release notes down), in kind of a roundabout way.\n\nTo make a custom resource type privileged, configure privileged: true.\n\nThis change should only require action from folks using a custom resource type that actually has depended on being privileged, such as any forks of the docker-image resource. It may be that you (or the resource author) will not know this until it fails. Resource authors are encouraged to update their READMEs if their resource type does indeed need it.\n\nThis was motivated by the one downside of overlay that we encountered: namespacing a volume takes much longer. Namespacing a volume means re-mapping its files to have UIDs that correspond to the correct UIDs for its target container's user namespace. This typically a recursive chown that remaps host-side UID 0 to the container's equivalent (UID 4 billion something), or vice-versa.\n\nNamespacing a volume takes longer for overlay because a chown of a file necessitates copying the file to the upper layer, which takes time and space. This did not affect btrfs as the nature of its copy-on-write mechanism is vastly different.\n\nMost workloads should be unprivileged, so to optimize for that, we've made all resources unprivileged by default. This means that they generate unprivileged volumes which won't need to be namespaced for unprivileged tasks (and other resources, i.e. put).\n\nPipeline config and team auth settings can now be encrypted in the database. See Encryption for more information.\n\nThis is the first step towards a better security model around credential management. The next step will be to externalize credentials entirely to a credential manager such as Vault or CredHub.\n\nWorkers will now use overlay rather than btrfs as their filesystem of choice for BaggageClaim.\n\nIf you're morbidly curious about all the low-level mumbo jumbo that went into this venture, check out issue concourse #1045.\n\nIf you don't really care, know that upon upgrade the workers will stick with btrfs, for backwards-compatibility. It is only on recreate that they will pick overlay, and only if it makes sense to (i.e. your kernel version is \u003e 4.0, and your disk is not formatted as btrfs already).\n\nThere are many benefits to what sounds like a low-level change:\n\n* No more loopback shenanigans. Most of you aren't using btrfs as your disk's filesystem, so Concourse would always have to make a sparse file and mount a loopback device over it. This was just confusing and made disk usage hard to interpret, and possibly leak.\n\n* No more locked-up workers! Many users, including us, encountered a btrfs bug which led to reads and writes of the disk locking up. The only way out from this was to recreate the worker.\n\n* Concourse in Docker for Mac should now work! They had btrfs stripped out of the kernel image, so Concourse had to fall back on the naive driver, which is hella slow.\n\n* A noticeable performance boost under load. In our testing, overlay performs better under realstic work loads like building many Docker images at once. While overlay would edge out btrfs at low contention, it performed even better as we ramped up the concurrency. What took btrfs 10 minutes would take overlay around 7.\n\n  Also worth noting is that during testing we could pretty reliably get btrfs to lock up. It actually made it hard to collect enough data. But maybe that's a more important point than any set of numbers.\n\nFixed metadata for fetched resources often not being shown for resources configured in multiple pipelines, due to the globalization of resource caches introduced by v3.0.0.\n\nThe fly CLI now supports tab completion for -t, thanks to a PR by @jmcarp!\n\nfly set-pipeline now supports tab completion for -c, thanks to a PR by @jmcarp!\n\nAdded a timeout when connecting to Postgres.\n\nFixed running concourse web on Windows. This was caused by building it with an out-of-date version of Go. We've now bumped to the latest.\n\nBumped the BaggageClaim response header timeout to 10 minutes, up from 1. The point of this value was more to be \"not infinite\", so that ought to be enough to account for slower disks.\n\nFixed overriding a base resource type with a custom resource type of the same name.\n\nThe legend in the pipeline UI will now show the meaning for dashed vs. solid lines, thanks to a PR by @Typositoire!\n\nThe concourse/lite Vagrant box will now respect any locally configured proxy, thanks to a PR by @akumria!\n\nThe ATC will uses \u003ca\u003e instead of \u003cspan\u003e for a few clickable elements, so that things like Vimperator or Vimium can target them. This is thanks to a PR by @dolph!\n\nThe semver resource will now ignore leading/trailing whitespace in the contents of the file in the bucket, thanks to a PR by @jghiloni! This would usually happen when manually adding the file, as a trailing linebreak is a common default for most editors (or even echo).\n\nThe docker-image resource should now correctly handle fully numeric tags rather than forcing the user to quote them, thanks to a PR by @benmoss!\n\n","depth":2,"section_tag":"v310"},"v3100":{"location":"download.html#v3100","title":"v3.10.0","text":"Users upgrading to v3.10.0 with BOSH have reported issues where baggageclaim will not start after the upgrade. This issue has been reported and fixed for future versions of Concourse. In the meantime, you can workaround the problem by using the workaround described in #2125\n\n This release involves a worker protocol version bump, from 1.3 to 2.0, and also switches the default BaggageClaim: volume management driver back to btrfs. Read on for more information!\n\nWe recommend spinning up a new pool of Running a worker nodes, upgrading your Running a web nodes, and then removing the old workers. Otherwise your workers may get swarmed with containers, if only one 2.0 worker is added at a time with the web nodes already upgraded.\n\n\n\nThe concourse worker commands can now be pointed at multiple TSA: worker registration \u0026 forwarding addresses, rather than one, so that it can retry against a random node each time. As part of this, we've removed the --tsa-port flag and changed --tsa-host to instead take a host:port. (Because TECHNICALLY, they could be on different ports.)\n\nWe've revamped how fly execute gets its inputs and outputs to/from the build, so that configuring the ATC: web UI \u0026 build scheduler with an external URL is no longer required. See concourse #2069 for the nitty-gritty.\n\nWe've switched back to btrfs as the default driver. We switched away in v3.1.0 in the midst of a ton of stability issues, which we have think we've fixed in v3.9.0.\n\nThis resolves a long-standing performance issue when using privileged tasks or resource types (like the docker-image resource). For more information, see concourse #1404 and concourse #1966.\n\nBe sure to use the latest possible kernel version so that you have a btrfs with the latest fixes. We suspect that this will still be an occasional issue, though far less frequent.\n\nPreviously, tags on a resource type didn't get no respect. They are now respected.\n\nFixed an ATC: web UI \u0026 build scheduler crash that would occur when a task step errored with the next step using an attempts step modifier.\n\nThe concourse binary (and Docker image) now supports a quickstart command, which will spin up an itty bitty Concourse cluster with a single worker.\n\nThe docker-image resource now supports pushing multiple tags, thanks to a combined effort of @gcmalloc and @jerith!\n\nWhen the ATC: web UI \u0026 build scheduler is streaming data between workers, the stream will now be gzip-compressed, which should speed things up quite a bit. (This is what caused the worker version bump to 2.0.)\n\nThe ATC: web UI \u0026 build scheduler now requires TLS v1.2+ and a stricter set of cipher suites. Say that five times fast!\n\nConcourse now supports the newer umask-hardened BOSH stemcells (v3541.x).\n\nFixed a botched bashism that led to the docker-image resource to exit early on certain environments (more info here).\n\nCleaned up a noisy PostgreSQL error that would occur on start when checking for the legacy migration_version table.\n\nFixed a UI glitch that caused the last line to be misaligned with the timestamps if it had no trailing linebreak.\n\n","depth":2,"section_tag":"v3100"},"v311":{"location":"download.html#v311","title":"v3.1.1","text":"Fixed a race condition during volume creation for worker base resource types. This would result in resource actions failing with failed to create volume.\n\nUnfortunately, to propagate this fix you'll need to recreate your workers, as they're in a broken state.\n\nFixed the reporting of database query metrics.\n\n","depth":2,"section_tag":"v311"},"v3110":{"location":"download.html#v3110","title":"v3.11.0","text":"v3.11.0 has a memory leak issue in the ATC/web server concourse #2165. The severity of the memory leak will be different based on the size of your deployment and the types of workloads you run on it.\n\n The git resource will no longer have tags present in the fetched repo, thanks to a PR by @benmoss.\n\nThey are now cleared out after the fetch, because the state of all tags is prone to change after the initial fetch, as the resource's source of truth is commits. So after the fetch, the cache would have an out-of-date view of the tags, which could lead to problems when pushing.\n\n\n\nFixed a bug in the BOSH release that prevented the Running a worker nodes from starting in a fresh deploy. Sorry about that. Thanks @z4ce for the PR!\n\nThe /dashboard page looks better on phones now.\n\nThe /dashboard page makes way fewer requests now, so it's a lot faster to load and more efficient to periodically refresh.\n\nThe fly builds command can now filter by team (-t) or pipeline (-p), thanks to a PR by @andrewedstrom!\n\nFixed a couple migrations that assumed a public schema. Thanks to @vganoradg for the PR!\n\nThe fly CLI will no longer repeatedly detonate when given an invalid token during fly login.\n\nThere's a new credential manager in town for AWS's newly-launched Secrets Manager (not to be confused with Systems Manager, which is also used for managing secrets). Thanks @x6j8x for the PR!\n\nWe realize that we now have two undocumented credential managers, one called AWS SSM and one called AWS SM. We like to call this \"hard mode\". (Sorry, we'll backfill docs soon.)\n\nThe The Vault credential manager can now be configured with a --vault-auth-backend-max-ttl, after which it will force a re-login. Thanks @baptiste-bonnaudet for the PR!\n\nThe The Vault credential manager will now retry with exponential backoff when logging in, rather than retrying every second.\n\nThe time resource will now correctly handle a tricky configurations that span multiple days (e.g. 10AM - 5AM), thanks to a PR by @jleben!\n\nThe git resource will now make the commit message accessible under .git/commit_message, thanks to a PR by @elgohr!\n\nThe Running a web node can now be configured with --cookie-secure to force secure: true on its cookies. Thanks for the PRs, @jmcarp!\n\nThe github-release resource now supports a tag_filter configuration for matching arbitrary semver tags, thanks to a PR from @jmcarp!\n\nAdded a missing property to the BOSH release for configuring a CA cert for Generic oAuth. Thanks for the PR, @youngm!\n\nThe docker-image resource now supports configuring aws_session_token, thanks to a PR by @itsdalmo!\n\nThe docker-image resource now has yet another new param, cache_from, thanks to a PR by @dhinus!\n\nThis new param is like load_bases except everything loaded will also be used as a cache during the build.\n\nThe git resource will now recover from a deleted tag when configured with tag_filter, thanks to a PR by @ljfranklin!\n\nfly validate-pipeline with --strict will now be more strict with your YAML, thanks to a PR by @aeijdenberg!\n\nThe cf resource now supports verbose: true, which will tell the CLI to dump trace logs to the output. Thanks for the PR, @JohannesRudolph!\n\nThe docker-image resource now supports a target_name param for specifying the target to build in a multi-stage Dockerfile. Thanks to @irfanhabib for the PR!\n\nThe BOSH release now bakes in the glue code for use with BOSH Backup and Restore, thanks to a PR by @rkoster!\n\nThe fly set-pipeline command can now be given --no-color flag to strip out the color from the diffs. Instead of using color, + and - will be at the start of added and removed lines.\n\nNow that we're building with Go 1.10+, The fly CLI will respect socks5 proxies configured via the \"standard\" http_proxy/https_proxy env vars.\n\n","depth":2,"section_tag":"v3110"},"v3120":{"location":"download.html#v3120","title":"v3.12.0","text":"A bug was introduced in build-reaper under ATC/web that causes old build logs to accumulate in Concourse (concourse #2187).\n\n Jun 1, 2018 A bug in Chrome 67 causes it to crash when loading the Concourse UI. At the time of this notice, the dev/canary versions of Google Chrome should work, as well as other browsers like Firefox and Safari. You can follow along the issue in concourse #2236\n\n The ATC: web UI \u0026 build scheduler will now batch-delete containers and volumes, rather than making individual calls out to the worker. This is an incremental step towards concourse #1959 that should reduce the network/IO overhead on the ATC during garbage collection.\n\nThis requires a new port to be reachable on the Running a worker node from the Running a web node: 7799. This communication will go away once we fully complete parallel GC, as the workers will ask the web nodes what to delete instead.\n\n\n\nIn v3.11.0 we changed the default behavior of the git resource so that it does not fetch tags by default. In hindsight we should have been a bit more careful here, as many people depend on that behavior. We've decided to roll it back to fetching them by default, since in most cases (where tags are never deleted or re-pointed) it's not an issue to include them.\n\nTo disable fetching of tags, configure clean_tags: true in params.\n\nThanks for the PR, @mdomke!\n\nThe ATC: web UI \u0026 build scheduler can now be configured with a global default build_logs_to_retain, thanks to a PR by @aeijdenberg! This is useful for operators who want more control over their database usage.\n\nA maximum value can also be configured to ensure users don't just set it to 9 trillion. The flags are --default-build-logs-to-retain and --max-build-logs-to-retain.\n\nFixed a memory leak in the TSA: worker registration \u0026 forwarding introduced in v3.11.0.\n\nThe fly set-pipeline command will no longer prompt apply configuration? if there are no changes to apply.\n\nThere's a fly order-pipelines command now, for setting the order of pipelines.\n\nA fly status command has been added for checking whether or not you're logged in to the given target.\n\nWhen fly check-resource fails, it'll bubble up the error message rather than just saying error code 500.\n\nWork around an apparent regression/behavior change in recent versions of Chrome that prevented the pipeline UI jobs from being clickable.\n\nFixed a corner case in error handling that could cause a lock to be held forever when detecting new versions of resource types. This could lead to things like builds stuck in \"pending\" state. Thanks to @SHyx0rmZ for the PR!\n\nWhen directed to the login page from the resource page, you will now be redirected back to where you were, rather than to the moon.\n\nThe concourse web command is now capable of running the migration flags (--current-db-version and friends). It's still super janky (it'll run the TSA: worker registration \u0026 forwarding alongside your...migrations), but hey, it runs them now.\n\nWe'll probably clean this up in the future by having the migration stuff be a separate command instead.\n\nThe fly CLI will once again helpfully instruct you to log in rather than just saying error: forbidden.\n\nThe BOSH release now supports properties for configuring the new AWS Secrets Manager credential backend. Thanks for the PR, @x6j8x!\n\nWhen a previously-created volume disappears from a worker and the ATC tries to use it, the error message will now include the worker name and the volume handle. Thanks for the PR, @rfliam!\n\nThe fly check-resource command will now fail more clearly when the resource's type is not found. Thanks to @jmcarp for the PR!\n\n","depth":2,"section_tag":"v3120"},"v3130":{"location":"download.html#v3130","title":"v3.13.0","text":"Jun 1, 2018 A bug in Chrome 67 causes it to crash when loading the Concourse UI. At the time of this notice, the dev/canary versions of Google Chrome should work, as well as other browsers like Firefox and Safari. You can follow along the issue in concourse #2236\n\nFixed a bug introduced in v3.12.0 concourse #2187 where old build logs were not being reaped. Thanks @aeijdenberg for catching the issue and PR-ing a fix!\n\nAdded a new authentication provider for teams using OpenID Connect (OIDC) #2.\n\nThanks @PavelTishkin!\n\nConcourse can now emit to Datadog using statsd agent #269\n\nThanks @baptiste-bonnaudet\n\n The semver resource now supports an optional commit_message parameter #64.\n\nThanks @ElfoLiNk\n\n The dashboard now supports the \"not\" operator for searches. This can be used on pipeline name searches, team searches, and status searches. Here are some examples:\n\n* -main gives you every pipeline other than the one called main\n\n* team:-main gives you every team's pipeline other than the ones belonging to main\n\n* status:-paused gives you all pipelines that are not paused\n\n\n\n","depth":2,"section_tag":"v3130"},"v3140":{"location":"download.html#v3140","title":"v3.14.0","text":"A migration made it in to this release that slows down scheduling for pipelines with a bunch of version history. v3.14.1 has been released with the fix.\n\nThere is a known issue where you may see high CPU usage as a result of constant CredHub client connection concourse #2300\n\nYou know that \"home\" icon that you click all the time and never fully know where it'll take you? Do you remember that empty feeling of not really knowing where \"home\" is anymore?\n\nWELL IT'S GONE NOW. And you know what's there instead? BREADCRUMBS. Breadcrumbs and memories.\n\nSo now you can click the pipeline part to go to the pipeline, or the job part to go to the job.\n\nWe've moved the pipeline groups navigation from the top bar to the pipeline page, where it's free to wrap around at its leisure (if there are many groups or looooooooooooooooooooong ones) rather than being constrained to the nav bar.\n\nContainer and volume Garbage Collection will now be performed in parallel across the worker cluster.\n\nThe ATC is still the source of truth for knowing when containers and volumes are to be removed, but will no longer be responsible for performing the actual \"destroy\" API calls. This should make large-scale Concourse deployments much more efficient, removing a ton of network and CPU overhead from the ATC.\n\nThe Concourse BOSH release now includes Windows jobs: baggageclaim-windows, houdini-windows, and worker-windows. This means you can natively run a Windows worker via BOSH, equivalent in functionality to the Windows binaries!\n\nTo see how to deploy it, consult the windows-worker.yml ops file.\n\nThe dashboard will now indicate when a pipeline has a resource that is failing to check, by drawing a little orange triangle on the pipeline.\n\nThe fly execute command with -j will now use the job's pipeline's resource_types. Huzzah!\n\nfly login can now be invoked with -b to auto-launch a browser to do the oAuth dance, thanks to a PR by @novas0x2a!\n\nThe s3 resource now supports configuring an initial version \u0026 content, which can be useful for bootstrapping state. Thanks for the PR, @bandesz!\n\nThe webhook_token property can now be interpolated using a credential manager, thanks to a PR by @timrchavez!\n\nThe pool resource now supports an atomic metadata update operation, thanks to a PR by @EleanorRigby!\n\nThe git resource now has git-crypt v0.6.0, thanks to a PR by @gcapizzi!\n\nThe Prometheus metric emitter has seen some spring cleaning, thanks to @databus23! See atc #274, atc #275, atc #276, and atc #278 for more details.\n\nA couple of the dashboard footer icons looked bloated in Firefox. They're all better now.\n\nWe fixed GitHub issue concourse #2000, which is more of a moral victory than anything else. (The fix: the number in the \u003ctitle\u003e when viewing a one-off build in your browser is now consistent with the number reflected on the page.)\n\nThe git resource now supports two new parameters: submodule_recursive: false, to disable the default recursive fetching, and submodule_remote: true to fetch submodules with --remote. Thanks to @ppaulweber for the PR!\n\n@SHyx0rmZ fixed up a few API endpoints so that they correctly return Content-Type: application/json. Thanks!\n\nPublishing draft releases with the github-release resource will no longer error, thanks to a PR by @antonu17!\n\nAny errors when checking for a resource's type to have new versions will be bubbled up as resource checking errors. This includes failure to fetch credentials.\n\nThe dashboard page now has \"Dashboard\" in the title.\n\nFixed an \"Aw, snap!\" browser crash that affected some versions of Chrome when viewing the pipeline page.\n\nThe ATC will no longer fail to start if configured with CredHub and CredHub isn't running. It'll just try and reach it later instead.\n\nThe ATC will now fail gracefully early if no session signing key is specified, rather than failing ungracefully and late.\n\nIn addition, one will be generated automatically if not given to concourse web. Don't do this forever, though, since users will be logged out whenever you restart the instance, and things won't work at all if you're running a cluster of many web nodes (they all need to have the same session signing key). Thanks for PRing this, @SHyx0rmZ!\n\nWe accidentally kept the quickstart command hidden from the concourse --help dialogue. It's there now, thanks to @osis!\n\nThe cf resource now supports client credentials -based auth, thanks to a PR by @jmcarp!\n\nThe concourse land-worker command will no longer panic if invoked with no session signing key.\n\n","depth":2,"section_tag":"v3140"},"v3141":{"location":"download.html#v3141","title":"v3.14.1","text":"There is a known issue where you may see high CPU usage as a result of constant CredHub client connection concourse #2300\n\n Fixed a scheduling performance regression caused by a wonky database index. This upgrade will apply cleanly for those who have manually removed it.\n\n\n\n","depth":2,"section_tag":"v3141"},"v320":{"location":"download.html#v320","title":"v3.2.0","text":"We've pulled in a newer, less crappy templating system for The fly CLI! New params look like ((this)) and support more than just strings: boolean values, arrays, and other YAML structures can be templated in. It also supports one big thing you've all been waiting for: inter((pola))tion!\n\nThe older, rougher-around-the-edges {{og-params}} are still supported and behave exactly as they did before. You should switch to the new style at your earliest convenience, but we have no immediate plans to remove the old style as it's really not that annoying to just leave in place.\n\nAs part of our params rejigger, we no longer support specifying maps as arrays of maps. We have no idea how or why this worked previously, but we never intentionally supported it, and won't be bringing this back. For slightly more context, see concourse #1307.\n\nThe Docker repository now supports resolving addresses within Concourse containers via Docker's magic local DNS server! This fixes a longstanding hurdle our Docker users would run in to pretty frequently when wanting to e.g. point Concourse at other Docker-deployed things like a registry within the cluster. It also removes the need for setting CONCOURSE_GARDEN_DNS_SERVER.\n\nNo configuration is necessary for this change.\n\nWe've made substantial improvements to our schema that should dramatically reduce utilization of your database, especially for larger deployments. On our own instance we saw Postgres CPU usage drop from ~25% to ~7%. Larger instances will likely see a more substantial impact.\n\nThe fly CLI now supports tab-completion of pipeline names, thanks to a PR by @jmcarp!\n\nAs part of the schema improvements, we've fixed a couple edge cases that could result in a container or volume for a get step being brutally murdered in the middle of a build running. We will now also wait for get steps to finish when draining a worker.\n\nWe're sorry, get step. You deserved better.\n\nWe've fixed a goroutine leak and generally redone how the radar component of the ATC scans for resource versions. As a result of this, the goroutine count dropped by just over 50%.\n\n","depth":2,"section_tag":"v320"},"v321":{"location":"download.html#v321","title":"v3.2.1","text":"{image: images/countdown.gif}\n\nThe ATC now garbage-collects containers and volumes in parallel, rather than garbage collecting all the containers and then all the volumes. We'll make this even more parallel in the future (while being careful not to swarm the workers).\n\nA migration introduced in v3.2.0 to ensure there's only one volume per resource cache on a worker would fail if there were already duplicates, making the ATC unable to start. The migration will now nullify the duplicates so that they become garbage-collected.\n\nWe accidentally broke handling of strings passed from -v to {{params}} by making them actually be parsed as YAML values, for ((params)). Turns out that was confusing even for ((params)), so we've made it so that -v is always for verbatim strings, just as before, and added -y for the less-frequent case of wanting to provide a YAML value as a parameter.\n\nThe ATC became very picky in the last release around extra keys in your pipeline config. It will now permit extra toplevel keys, and only raise a validation error for extra keys nested under jobs, resources, etc., so you can continue to declare values at the toplevel for YAML anchoring.\n\n","depth":2,"section_tag":"v321"},"v330":{"location":"download.html#v330","title":"v3.3.0","text":"A migration introduced in this release loads all the builds into memory and then processes them. It has to do this (rather than read one row at a time and update) as it only has one transaction. This might cause some memory issues on the ATC while upgrading. This migration has been fixed in v3.3.4\n\nOur first pass at support for credential management has landed! With this you can externalize all of your credentials in to Vault (more providers coming later), preventing your credentials from ever entering the database and allowing for automatic credential rotation.\n\nFor more information on how to configure and use this feature, see Credential Management.\n\nTasks now support caching arbitrary paths by configuring caches in the task config. This can be used to speed up builds that fetch dependencies at runtime or compile into a common directory (e.g. pkg for Go). For more information, see caches.\n\nIn our initial support for encryption, we missed a spot. Build plans (an internal structure generated when a build starts) were previously saved into the database in plaintext, and would sit around forever.\n\nNow, we encrypt them and remove the build plan (encrypted or not) when the build completes.\n\nPreviously, if a serial group had a paused job in it, and the job had a build queued up, the entire serial group would wait for this build that would never run, forcing you to continuously abort the pending builds to unwedge your pipeline.\n\nIt...doesn't do that now, thanks to a PR by @jmcarp!\n\nA previous release broke rendering of older builds. They'll now render properly.\n\nThe web UI will now serve back a 404 page when the content you requested is not found, rather than just...being broken.\n\nThe login prompt will now tell you if your basic auth credentials were invalid, rather than leaving you to sit and think about what you've done wrong.\n\nConcourse now supports GitLab oAuth configuration, thanks to a PR by @markstgodard!\n\n","depth":2,"section_tag":"v330"},"v331":{"location":"download.html#v331","title":"v3.3.1","text":"The fly volumes command will now show task cache volumes as task-cache and show the name of the task the cache is for.\n\nThe last release (v3.3.0) broke resource configs with nested maps in them. This is now fixed.\n\nFixed an ATC crash that would occur when trying to list pipelines while the database is down. Along the way we also made the endpoint quicker.\n\n","depth":2,"section_tag":"v331"},"v332":{"location":"download.html#v332","title":"v3.3.2","text":"Fixed a vulnerability affecting installations using untrusted multi-tenancy (i.e. multiple teams who may be jerks). This issue affects all versions after and including v2.7.1.\n\nIf you are running a single-tenant Concourse installation, or an installation where all team members are \"trusted\" (i.e. part of your small org), you don't have much to worry about. Otherwise, you'll want to upgrade to this as soon as possible.\n\nWe strongly recommend upgrading as soon as possible.\n\nThe fly execute command with -j can now resolve the Vault credentials configured in the job's inputs.\n\nECR support is now fixed in the docker-image resource.\n\n","depth":2,"section_tag":"v332"},"v333":{"location":"download.html#v333","title":"v3.3.3","text":"Added support for params in image_resource. This probably should've always been there, but hey at least its there now!\n\n The s3 resource supports unpack, which unarchives a tar/zip file on the get command. This enables s3 to be a provider for image_resource docker images. Thanks to a PR by @krishicks!\n\n\n\n","depth":2,"section_tag":"v333"},"v334":{"location":"download.html#v334","title":"v3.3.4","text":"A migration introduced in v3.3.0 would load all the builds into memory and then process them, causing a lot of issues when upgrading. We  optimized this migration to migrate build plans in batches, rather than all at once.\n\n The unpack support for the s3 resource will no longer load the entire archive into memory, so it can be used for larger archives, thanks to a PR by @krishicks!\n\n\n\n","depth":2,"section_tag":"v334"},"v340":{"location":"download.html#v340","title":"v3.4.0","text":"We've deprecated our concourse/lite Vagrant box in favor of a bosh create-env flow. This will be much easier for us to maintain and brings a lot more flexibility around configuring and upgrading Concourse.\n\nWe've parallelized garbage collection. This should make things more durable to a slow worker, and make it harder for containers and volumes to \"pile up\" when the ATC is out of service briefly (i.e. during a deploy). Yee.\n\nThe legend on the pipeline page will now auto-hide after 10 seconds.\n\nWhen switiching between pipelines, the UI will now fit the pipeline in view.\n\nYou can also press 'F' to pay respects center a pipeline in view.\n\nYou can now log in with a personal access token when logging into a team with github auth, thanks to a PR by @Chumper!\n\nYou can now set image_resource.version on an image_resource, thanks to a PR by @krishicks!\n\nWe've removed the volume size column from The fly CLI ... previously it was always empty and no one seemed to care.\n\nThe fly validate-pipeline command can now be provided with variables in the same way that fly set-pipeline can, thanks to a PR by @jmcarp!\n\nThe bosh-deployment resource now uses the latest BOSH CLI, thanks to a PR by @selzoc!\n\nThe semver resource now supports Server Side Encryption, thanks to a PR by @miromode!\n\nThe git resource will now save the committer email to `.git/committer`, thanks to a PR by @knifhen!\n\nJobs with a pending build now have a static halo to better represent its waiting state, thanks to a PR by @d!\n\nThe fly CLI learned the fly format-pipeline command, thanks to a PR by @krishicks!\n\nThe fly abort-build command can now abort by build ID, thanks to a PR by @kurtmc!\n\nBaggageClaim's response header timeout is now configurable, which should help those with large images that they're using for privileged tasks. This is a band-aid; we'll soon be making the API this is relevant to async.\n\nFiles with the setuid and setgid permissions set on them will no longer have them removed. This used to be lost with the chown performed for namespacing the files. We'll now restore it after the chown.\n\nThe flags for configuring GitLab oAuth are now present in fly set-team.\n\nFixed an underflow in BaggageClaim's volume size detection, thanks to a PR by @SHyx0rmZ! This affected deployments with less than 10GB of space. (Psst: you should probably get more anyway.)\n\n","depth":2,"section_tag":"v340"},"v341":{"location":"download.html#v341","title":"v3.4.1","text":"Looks like the GC \"fixes\" had the opposite effect of making containers linger around longer than usual. You might want to skip this version for now. Otherwise, hang tight while we get this sorted out.\n\nRemoved a feature introduced in v3.4.0 that lets you authenticate into a team using personal GitHub tokens.\n\nGitHub's API surprisingly accepts oAuth tokens in the same flow as access tokens. This makes the GitHub auth flow supported by Concourse less secure, as oAuth tokens may be acquired by an exploited third-party service that users have authorized, thereby allowing them to log in to the user's CI system.\n\nThe fly command for fly set-team and fly destroy-team now lets you supply the flag --non-interactive. Such automation. Amaze.\n\nThanks to @aleksey-hariton for the PR!\n\nBaggageClaim volume creation APIs are now asynchronous; this should remove the need for crazy timeouts.\n\nThanks to @SHyx0rmZ for implementing this!\n\nfly now prints a URL to your build page when you run fly execute. How convenient!\n\nDeleting teams would cause the garbage collector to freak out and cause a buildup of worker containers.\n\nThat's been fixed now.\n\nAdded the appropriate headers to stop GitHub from caching badges.\n\nThanks to @belljustin and @cunnie for fixing this longstanding issue!\n\nFixed an issue where the pipeline view  will reset after a state change on the pipeline.\n\nPreviously, if a resource or resource type was parameterized via a credential manager, its check containers and caches would be mistakenly garbage-collected. They will now be kept around.\n\nCheck containers will no longer be brutally destroyed if they're used too close to their expiration time.\n\n","depth":2,"section_tag":"v341"},"v350":{"location":"download.html#v350","title":"v3.5.0","text":"Those of you using our BOSH release have been stuck with either our way-out-of-date-and-vulnerable-to-thousands-of-CVEs-and-EOL-next-year version of Postgres (9.3), or an external Postgres server. NO LONGER!\n\nWe've enabled an upgrade path to the CloudFoundry Postgres BOSH release, which is up-to-date (currently 9.6.4) and even supports release-to-release upgrade paths!\n\nThe next version of Concourse will require version 9.5+ of Postgres, so action is required either now or upon the next release to upgrade.\n\nThe postgresql job in our release should now be considered deprecated, and will be removed in the next release.\n\nTo switch off from our pitiful pre-packaged precarious Postgres, do the following:\n\n* First, deploy this version of Concourse with no changes. We have modified our Postgres job to move its data to a new location that the Cloud Foundry Postgres release will upgrade from.\n\n* After the deploy has finished, upload the Cloud Foundry Postgres release. We've tested the upgrade path with version 22, available on bosh.io.\n\n* Once the release is uploaded, add it to your deployment manifest, swap out the concourse/postgresql job for the postgres/postgres job, and update the ATC properties to explicitly configure the database and role. (That's a mouthful, but you can use our changes to the single-vm manifest as a reference.)\n\n* Note that the Postgres DB upgrade must not be combined in the same deployment operation with a stemcell update.\n\nConcourse now supports CredHub for external credential management. See The CredHub credential manager for more information.\n\nGC no longer creates crazy seesaw patterns with containers and volumes. This was an issue that was introduced in v3.4.1 when 2 or more teams have identically configured resources. It's been fixed now.\n\nFixed a leak with goroutines that happens from fly intercept\n\nWhen using groups in pipelines, fly will now let you know when you forgot to assign a job to a group. Say goodbye to hidden pipelines!\n\nJobs and resources with a forward slash in their name no longer error out when loading their details.\n\nThe semver resource now supports Google Cloud Storage. Thanks @chendrix!\n\nThe s3 resource correctly shows the progress of uploading and downloading. It will no longer report 2 TB/s. Thanks @krishicks!\n\n","depth":2,"section_tag":"v350"},"v360":{"location":"download.html#v360","title":"v3.6.0","text":"Concourse now requires PostgreSQL v9.5+. If you're already up-to-date, you've got nothing to worry about.\n\nIf you're using the BOSH release, and have already followed the instructions in v3.5.0, you're all set.\n\nIf you haven't, do so! You should be able to do the same upgrade process with this release, except that some queries will be failing in the ATC until everything is upgraded.\n\nWith PostgreSQL v9.5.0+ we've been able to dramatically improve performance of the pipeline UI. It's worth it!\n\nWe've bumped our Garden-runC dependency to v1.9.0. This upgrade requires a recreate of your workers.\n\nThe fly validate-pipeline will now validate the config field on embedded tasks.\n\nAs part of this change, we have removed support for configuring both config and file, which was deprecated about a billion years ago and has been emitting shouty warnings all this time.\n\nBuild logs now have timestamps! In addition, you can click them to link to lines, and shift-click to select ranges! WHEE!\n\nThere are some fancy-schmancy keyboard shortcuts on the build page. Press ? on the build page to learn more!\n\nFixed a case where a bunch of pipeline scheduling happening at once could result in a client-side database connection limit being reached, resulting in slowness.\n\nPreviously, when clicking and dragging in the pipeline UI, if the initial click was on a job, it would take you to the job when you let go. NO LONGER!\n\n","depth":2,"section_tag":"v360"},"v370":{"location":"download.html#v370","title":"v3.7.0","text":"This release has issues. Primarily with the BOSH distribution. You should probably just skip straight to v3.8.0.\n\nWe've ripped out the old \u0026 janky PostgreSQL job from our Concourse BOSH release. You will have no choice but to bring your own PostgreSQL database.\n\nIf you use the Concourse BOSH release and you haven't upgraded in a while, I'd suggest you check out the previous migration instructions from Concourse v3.5.0 and v3.6.0.\n\nWe've changed how we develop database migrations, so as to support down migrations in the future. This will hopefully mean that if you upgrade Concourse and for whatever reason need to back out, you'll be able to, instead of being stuck on a (possibly broken) latest version.\n\nAs part of this switch, we've also squashed our migrations into one big bang, which should also improve startup time for fresh installations. However, this means that you must first upgrade to v3.6.0 before upgrading to v3.7.0!\n\nSo, do that. You may need to anyway now that we've removed PostgreSQL (see previous note).\n\nThere are new required manifest changes to deploy this release with BOSH.\n\nOur BOSH release used to have a few magical mystical packages called generated_something. These packages would generate a RSA key every time they compiled, in service of automagically wiring up security credentials so you didn't have to put them in your manifest.\n\nThis approach was extremely clever and whoever came up with the idea was a downright genius, way ahead of their time.\n\nWe've now collectively decided that the whole approach is stupid and redundant now that BOSH manifests can generate their own typed variables. It was fun while it lasted.\n\nWhat you need to do for this change is described in concourse #1834. You can consult our changes to manifests/single-vm.yml for reference.\n\nAs an alternative to hand-editing your manifest, the next release note may pique your interest.\n\nWe have started dusting off concourse-deployment and using it as a central location for Concourse BOSHy deployment goodness. We now use it for our production deployment, as well as a few testing environments.\n\nWe are now openly gathering feedback on one of our worst-kept-secrets: the Concourse dashboard view. You can access it by visiting /beta/dashboard.\n\nIn this version of Concourse, we've tweaked some of the visual elements of the dashboard to make it more readible for installations with multiple teams. We've also fixed some of the pipeline states so that they make more sense.\n\nTell us what you think about the new dashboard by dropping us a line on concourse #1829.\n\nThe fly execute command will now default to -x, which has been replaced with a new flag, --include-ignored, to revert to the old behavior.\n\nIn addition, Fly will no longer blow up when trying to execute with an input that doesn't have a .gitignore. It will also not blow up if any inputs are files and not directories.\n\nThe ATC will now use a separate database connection pool for the API and the pipeline scheduling work. This will make it so that a bunch of slow API requests can't starve critical functionality.\n\nPipeline-provided resource types will no longer fail miserably for a minute or two when they're first configured.\n\nYou can now configure params on a pipeline resource type, thanks to a PR by @jghiloni! This will enable users to use the s3 resource with params: {unpack: true}, as an alternative to the docker-image resource.\n\nYou can now specify a on_abort hook on a step or on a job. It will run on abort. 🎉\n\nThe ATC can now be configured with a pure-random worker selection strategy, which may help users affected by our default resource affinity placement, which can result in overloaded workers. This is thanks to a PR by @phillbaker!\n\nTo use the random placement strategy, pass --container-placement-strategy=random to the web command.\n\nThe fly jobs command now has a column indicating whether any builds are pending or started for each job, thanks to a PR by @rowanjacobs!\n\nThe s3 resource now supports being configured with a session token, thanks to a PR by @keymon!\n\nGit repos encrypted with git-crypt will now be automatically decrypted by the git resource, thanks to a PR by @dmrschmidt!\n\nThe ATC can now be configured to serve a metrics endpoint for Prometheus, thanks to PRs by @TimSimmons and @jmcarp!\n\nTeams can now have BitBucket-based auth, thanks to a PR by @SHyx0rmZ!\n\nThe git resource can now be configured with a HTTPS proxy, thanks to a PR by @jghiloni!\n\nInline task configs are now validated as part of pipeline validation, thanks to a PR by @jmcarp!\n\nThe cf resource can now be configured with a Docker username/password for pushing an app using a private repository, thanks to a PR by @elgohr.\n\nThe github-release resource now supports being configured with insecure: true to support private GitHub Enterprise installations. For the long-term strategy regarding this, see concourse #1027.\n\nThe semver resource now supports being configured with skip_ssl_verification: true to support private S3-compatible blobstores, thanks to a PR by @calebwashburn.\n\nNotice how this note and the prior note have entirely different property names for doing the same thing. Blargh! See concourse #1027.\n\n ATC now has a flag for using k8s secrets when running in a cluster. This change makes using the k8s credential manager an explicit choice when running inside k8s, and also allows you to use a different credential manager when running in a cluster. Thanks for the PR and the patience by @william-tran and @farcaller\n\n\n\nWhen the ATC is configured with multiple metrics emitters, it will now error, rather than silently picking one, thanks to a PR by @jmcarp.\n\nFixed an issue where selecting/copying the build output would also select the timestamp on the left.\n\nfly login will now error if arguments are mistakenly given to it.\n\nTurns out you could easily spam the build page by holding T to trigger multiple builds. We've fixed that now so it only triggers a build once. #YOTO\n\nFixed the web UI so that it appropriately shows that you are logged out when your session expires.\n\nThe deprecated bosh-deployment resource resource now, uh, contains the bosh CLI again. Sorry about that. Switch to the CloudFoundry BOSH deployment resource if you can though!\n\nThis was fixed by a PR by @Infra-Red. Thanks!\n\nFixed an issue with the CredHub integration that made it necessary to configure --insecure-skip-verify, thanks to a PR by @aeijdenberg!\n\n","depth":2,"section_tag":"v370"},"v380":{"location":"download.html#v380","title":"v3.8.0","text":"Y'all really carried us on this release. We just shipped it. Thanks everyone!\n\nThe ATC can now be configured with an idle timeout for fly intercept sessions, thanks to a PR by @sharms and @jmcarp!\n\nIn v3.7.0 we broke the ability for BOSH deployed workers to configure an external TSA - the known_hosts file would be empty on the machine. Sorry about that! This should work again. Thanks for the PR, @christianang!\n\nAlso in v3.7.0 we mistakenly provided a default empty value for the now-required token_signing_key property on the atc and tsa jobs in the BOSH release. This is now removed, so you can get a nice juicy template resolution error rather than a busted deployment.\n\nThe container placement strategy can now be configured in the BOSH release, thanks to a PR by @williammartin.\n\nThe Generic oAuth provider can now be configured with a CA certificate, thanks to a PR by @rkoster!\n\nFixed a typo in the new Prometheus metrics endpoint that prevented volume count metrics from being emitted. Thanks @jmcarp!\n\n","depth":2,"section_tag":"v380"},"v390":{"location":"download.html#v390","title":"v3.9.0","text":"* There is a known issue with the BOSH release of v3.9.0 where the ATC will fail because function esc is not defined concourse #2029\n\n* CredHub integration has a bug in this release, please see concourse #2034 for more details\n\nCertificates can now be automatically propagated from the worker machine into resource containers. This feature took a lot of thought, trial and error, and shaking our fists at each Linux distribution maintainer for having a different approach to how certificates are stored and managed. There's a lot of context for this in concourse #1027.\n\nLong story short, we wanted a way for certificate management to be done generically across all resources, so that resource authors don't have to keep implementing various forms of ca_certs, insecure_skip_verify, etc., and things can \"just work\" securely by default.\n\nThis feature is enabled by default for our BOSH distribution. The concourse binary now has a --certs-dir flag on the worker command, which should be given something like /etc/ssl/certs or /etc/pki/tls/certs depending on your Linux distribution.\n\nSee Certificate Propagation for more information.\n\nThe btrfs volume driver should now be much more stable. This resolves a (very) long-standing issue that caused us to switch the default to overlay, which in turn introduced a known performance regression with privileged tasks and resources (i.e. Docker image building).\n\nIf you're feeling the pain of overlay, we recommend switching the driver to btrfs now and letting us know if you see any issues. Initial feedback is positive. If things are looking good we may switch the default back to btrfs.\n\nTasks now support inputs.optional inputs, thanks to a series of PRs by @rosenhouse! This is great for adding optional behavior to tasks and incrementally changing them backwards-compatibly.\n\nThe The CredHub credential manager can now be configured with mutual TLS based authentication.\n\nTeams can now be renamed via fly rename-team. (...yay!)\n\nThe docker-image resource now correctly handles complicated build args, thanks to a PR by @jfmyers9 and @ljfranklin.\n\nThe target will no longer be deleted when running fly logout - only its token.\n\nWhen viewing a resource, it will now show when it last checked.\n\nThe s3 resource will now auto-adjust the part size when uploading, so that it can upload files over 50GB. Thanks @ruurdk for the PR!\n\nThe docker-image resource now supports loading multiple images at the start via load_bases for use in multi-part Dockerfiles, thanks to a PR by @krishicks!\n\nMulti-part Dockerfiles with multiple ECR images will now correctly pull each with ECR login support, thanks to a PR by @PeteGoo.\n\nWhen using fly intercept with --url, the appropriate target will now be auto-detected based on the URL. Thanks @jmcarp for the PR!\n\nReduced the throttling when talking to k8s for credential management, thanks to a PR by @william-tran.\n\nThe Prometheus metrics endpoint now includes scheduling and database metrics, thanks to a PR by @TimSimmons!\n\nThe Prometheus metrics endpoint no longer breaks HTTP metrics down by path, because that made the cardinality too damn high. Thanks again @TimSimmons!\n\nA NewRelic Insights metric emitter has been added, thanks to a PR by @novas0x2a!\n\nSupport for using AWS SSM for credential management has been added, thanks to a PR by @surajbarkale!\n\nWhen viewing a pipeline on a teeny tiny display, the obnoxious color indicator legend thingy will no longer prevent you from clicking the stuff below it. Thanks @SwamWithTurtles for the PR!\n\nThe cf resource now has a show_app_log config for tailing the app logs while starting it up. Thanks for the PR, @aeijdenberg!\n\nThe docker-image resource will now propagate http_proxy and https_proxy when building docker images, thanks to a PR by @boazy!\n\nThe docker-image resource can now be configured with max_concurrent_downloads and max_concurrent_uploads, thanks to a PR by @drahnr!\n\nThe github-release resource will now produce a commit_sha file containing the...commit sha that the release's tag points to. Thanks @defsprite for the PR!\n\nWhen contacting CredHub, the configured CA cert is now respected. It was ignored in previous releases. Sorry about that! We've ramped up testing in our pipeline to catch silly things like this in the future.\n\nFixed finicky 500 errors when running fly volumes as a result of volumes disappearing while the API walks through and gets their info. The endpoint is still slow, but it at least won't blow up in this case.\n\nFixed missing validation for on_success, on_failure, and ensure when configured on a job, thanks to a PR by @jmcarp!\n\nFixed a subtle timing issue that could result in fly watch not finding any builds to watch when given a job.\n\n","depth":2,"section_tag":"v390"},"v391":{"location":"download.html#v391","title":"v3.9.1","text":"The build page will now render exotic ANSI text modes like faint text, framed text (...?), and... Fraktur (??????).\n\nThis is a feature, but I'll be damned if it's what makes us go to 3.10.\n\nIf you're wondering what Fraktur looks like, check out the pull request. Thanks, @evanphx!\n\nIf you're wondering why that's a standard, keep wondering.\n\nWe've optimized the rendering of the build page, which got quite a bit slower with the introduction of timestamps in v3.6.0. There is more work to do here, but we've added performance tests to catch any future egregious regressions.\n\nThe BOSH release's credhub.client_id and credhub.client_secret properties are now respected once again. Ironically this broke when we added CredHub testing to our pipeline, but in a way that avoided UAA client ID/secret configuration as it was much too heavyweight (hence the introduction of TLS auth properties).\n\nFixed a BOSH release template resolution error from an undefined esc method, which would only occur on a clean deploy thanks to the magic of global state. This is an example of why using Ruby to generate Bash scripts is a terrible idea.\n\nThanks to @calebwashburn for discovering the issue and PR-ing a fix!\n\nCleaned up those pesky atc.skymarshal.user.not-authorized logs.\n\nFixed a crash that would occur when a task step configured image but no config or file.\n\nThe fly CLI will now buffer output when rendering tables, which should make things a bit faster on Windows.\n\n","depth":2,"section_tag":"v391"},"v392":{"location":"download.html#v392","title":"v3.9.2","text":"Due to popular demand, we're graduating the pipeline dashboard out of beta and back into /dashboard. You can now also do useful things like log in and click through to the normal pipeline and build pages.\n\nThe NewRelic metrics emitter has now been improved thanks to @novas0x2a!\n\nRemoved a pesky database constraint (cannot_invalidate_during_initialization). This would occasionally bubble up to the user in weird ways, and actually isn't necessary any more.\n\nThe previous release snuck some code that wasn't quite ready yet into the BOSH release and broke registration of external workers. This is now fixed.\n\nOccasionally builds would fail when interacting with Vault with http2: no cached connection was available. NO LONGER! (We, uh, we bumped a dependency.)\n\nCertain ANSI cursor movement escape sequences would wreak havoc on the Concourse build output page because there was no window size set on the TTY, thus defaulting it to 80x24. We've set it to 500x500. That oughtta do it.\n\nFixed an issue where Firefox users couldn't click around on the build page.\n\n","depth":2,"section_tag":"v392"},"v400":{"location":"download.html#v400","title":"v4.0.0","text":"{image: images/whoami.gif}\n\nWe've completely redone auth! (Read on before upgrading - this is a huge change and there are some unsupported migration paths.)\n\nIn contrast to previous releases of Concourse, users are now central to the authentication flow. Instead of logging in as a team, you now log in as a user and can belong to one or more teams. Users can be added to a team by configuring the team's whitelist as described in Configuring Auth.\n\nThis is the first step on our march towards full role-based access control. Help us plan that out by checking out the RFC!\n\nIn addition, it is now much easier to extend Concourse to support more providers. We're leveraging CoreOS's Dex project for all the moving parts, which already supports a ton of providers (Dex calls them \"connectors\"). The only delta required for Concourse to support a Dex connector is a tiny bit of glue code in our new Skymarshal component to provide higher-level flags for our CLI.\n\nThis was a large change and it was pretty difficult to make backwards-compatible. Here's what's \"breaking\":\n\n* There are different flags to pass to the binary distribution, and the BOSH deployment requires manifest changes. Consult Configuring Auth for more information for the binaries, and the Concourse BOSH deployment repo for BOSH.\n\n* There is no support for configuring the same provider multiple times (say, multiple GitHub Enterprise instances). The migration will fail when trying to upgrade an instance with teams having different configurations for the same provider.\n\n  At the moment, you'll have to deploy multiple Concourse instances. This may be something we can support in the future.\n\n* There is no longer support for BitBucket auth. Sorry - Dex doesn't support it. :( However we do support generic LDAP, oAuth, and OIDC connectors, which you may be able to use instead.\n\n* If you have multiple teams configured with the same basic auth username, the migration will fail. This is because \"basic auth\" is now gone and in its place is local user configuration. Logging in with basic auth is now actually logging in as the configured user, so there can't be multiple.\n\n* The flags for fly set-team have been split between set-team and concourse web (because part of the config is now global).\n\n* You may also need to download and install the latest 4.0.0 fly CLI. In the past you would have been able to fly sync your way to the latest version of fly but the new auth in 4.0.0 will cause the old fly to error out.\n\nYou should definitely check for these conditions and take a database backup before attempting the upgrade. In practice, our two largest environments upgraded just fine with no intervention required, but if you're not sure, it can't hurt to be careful.\n\nThanks to Dex, we now also support LDAP based auth!\n\nThe fly teams command only lists teams of which you are a member (or all teams, if you're a member of the admin team).\n\nYou can also pass -d/--details to show each team's auth config! This should make it a lot easier to check if the auth setup is correct when someone complains about not being able to log in.\n\nOwing to the auth revamp, the fly set-team command no longer takes flags for the provider configuration (so no client IDs/secrets/etc.). This, in combination with the previous feature, should make tweaking the auth config a lot easier.\n\nThe dashboard's influence has taken hold on the web UI! The main page (/) now shows the dashboard instead of some random pipeline configured by the first team on the instance. We've also made the dashboard more powerful with pipeline pausing and re-ordering. We hope you like it because we've removed the sidebar from the pipeline view...it's just cleaner.\n\nWe've also spread the colour scheme to the rest of the UI and changed the font everywhere to Inconsolata.\n\nWe've made significant improvements to the performance of the build page while keeping its functionality exactly the same.\n\nSee this GitHub comment for the nitty-gritty!\n\nThere's still more work that could be done, and we put some planning on the issue, but we figured a 6.5x improvement is a good start so we can get back to big juicy features like space. If it's still not fast enough for you, we could really use your help! We're happy to provide guidance for anyone looking to contribute.\n\nPreviously, if a resource was only ever used as an explicit output of a job, it would always show up as black even if it was erroring. It will now show up as orange, like the other resources.\n\nWe've updated some of the messaging in the UI to be less confusing. When viewing a build that has not been made public it'll now say you're not authorized, rather than telling you to log in, only to tell you to log in again, because that didn't change anything.\n\nWhen we redid the container lifecycle way back in v3.0.0, one side effect was that containers failed one-off builds would be garbage collected almost immediately, making it pretty difficult to debug (you'd pretty much have to hijack while it was running).\n\nThanks to a PR from @databus23, there is now a configurable \"grace period\" after which these containers will be garbage collected! The flag is --gc-one-off-grace-period on the concourse web command, and it defaults to 5 minutes.\n\nWe fixed a regression with the CredHub integration that caused very high CPU usage on the ATC. In addition, we've bumped our CredHub client to include a fix PRed by @takeyourhatoff which even further reduces CPU usage. Yay!\n\nThe interval on which resource types are checked for new versions can now be set globally via --resource-type-checking-interval, or per-resource-type in a pipeline via check_every.\n\nWe fixed a couple situations in the UI where jobs or pipelines with spaces in their name would render incorrectly. (Please don't do this though. It looks so weird and just makes the CLI hard to use! We may have to tighten up naming restrictions in the future, and keeping spaces is pretty low priority. Let us know if you have a real good reason though.)\n\nClicking the pipeline in the breadcrumb while already on the pipeline page (but viewing a particular group) will now reset the pipeline to the \"initial\" view. This way it behaves like a normal link.\n\nRepeated team and pipeline creation and destruction would leave a few tables around: team_build_events_XXX and pipeline_build_events_XXX. This would cause the database to increase in CPU usage over time.\n\nWe now ensure these tables get cleaned up via database triggers on pipeline/team deletion. However, we decided against writing a migration to automatically clean up existing orphaned tables because, well, it felt scary and dangerous.\n\nIf you are seeing symptoms of this problem, it should be safe to manually drop the tables that have no corresponding pipeline or team. We just didn't want to be responsible for a migration that had a high chance of data bloodlust. This way it can be your fault instead of ours!\n\nWe've made quite a few optimizations that should take a lot of load off the database. This should improve everything from garbage collection efficiency to web UI response time.\n\nThe Vault credential manager backend can now cache credentials based on their lease duration. This was a big chunk of work and should make Vault operators' lives a bit easier. To enable this feature, pass --vault-cache to concourse web. Thanks @rfliam for the PR!\n\nAs a side note, we're in need of someone to champion the next phase of credential manager support. We've collected feedback from our first (very much MVP) implementation but really need individuals who have experience with each backend to take the next step. See rfcs #5 for more information!\n\nWhen running on Windows, we will no longer shell out to tar for performing volume streaming operations, since it seems to be pretty unreliable. A native Go implementation will be used instead, which is a bit slower but much more portable. Thanks for the PR, @ankeesler!\n\nThe The fly CLI now supports --json on most commands to dump info in JSON format, rather than the human-friendly table format.\n\nRecent versions of Docker introduced an issue where dockerd could fail to start if the worker was under load. This resulted in an infinite loop in the docker-image resource.\n\nWe've made the resource more resilient to this - it'll detect a failure to start and keep resuscitating dockerd until it starts, giving up after 2 minutes.\n\nThe s3 resource now supports skip_download: true, thanks to a PR by @talset!\n\nThe BOSH release now has properties for configuring the DataDog metrics emitter, thanks to @SHyx0rmZ!\n\nWe've split the migration operations out into a separate subcommand: concourse migrate. This is just a bit easier to reason about rather than having all the options baked in to the same command that runs the ATC, and also lets you run migrations without passing all the other flags required by concourse web.\n\nThe Prometheus metrics will now automatically prune stale workers, thanks to a PR by @databus23!\n\nThe Prometheus metrics for pipeline scheduling are now counters instead of gauges, thanks to a PR by @databus23!\n\nThere are now metrics emitted for peridoc resource checking, thanks to a PR by @databus23!\n\nFixed handling of no_proxy in concourse worker, thanks to a PR by @databus23!\n\nThe docker-image resource now includes support for fetching and extracting xz packages in ADD commands, thanks to a PR by @et7peho.\n\nThe cf resource now supports no_start: true, thanks to a PR by @klakin-pivotal!\n\nThe docker-image resource now has a tag_file param which deprecates the old tag command which does the same thing. This is in the interest of clearer naming. Thanks for the PR, @ghostsquad!\n\n","depth":2,"section_tag":"v400"},"v410":{"location":"download.html#v410","title":"v4.1.0","text":"This release unintentionally broke --postgres-data-source, which was deprecated back in v2.7.2. We're going to fully remove it in the next release, so now's a good time to switch to the new flags!\n\nFixed an annoying issue affecting deployments with multiple Running a web nodes. We were configuring an in-memory store for the Dex auth flow, meaning your login session would only work on one ATC at a time, and could fail partway through the redirect dance.\n\nWe now store this state in the same Postgres database that you're already configuring today, so everything should \"just work\" from here on.\n\nTask caches can now be cleared via fly clear-task-cache, thanks to a few PRs by @edtan! This is handy when something has gone terribly wrong with your cache and you need to reset everything. It happens.\n\nWorkers can now be registered with a --ephemeral flag. When specified, the worker will be immediately removed once it stalls.\n\nThis is useful for situations where you don't have careful control over when and how the worker goes away, for example with preemptible machines or when running with Docker Compose or on a development machine.\n\nThanks to @tanner-bruce for kicking off the PR for this!\n\nWe've removed the bosh-deployment resource from our core set of resources, as it has been deprecated for over a year. Use the CloudFoundry BOSH deployment resource instead! It's much better.\n\nYou can now force an immediate check of a resource type via fly check-resource-type. This should help shorten feedback loops when testing your own resources types.\n\nNote that the need for this may soon go away if we follow through with RFC #8 which proposes merging Resource Types into Resources.\n\nThe concourse/concourse Docker image now contains the file command, which is useful for...working. Without it the btrfs volume driver setup would fail. Sorry about that, and thanks to @ElfoLiNk for submitting the PR!\n\nThe --external-url and --peer-url flags for concourse web will now infer defaults that match the configured --bind-ip and --bind-port, rather than blindly defaulting to http://127.0.0.1:8080 and breaking when the bind IP/port are changed.\n\nInputs that are \"new\" will now have a yellow icon. This replaces the input background highlighting that was accidentally removed with 4.0's recoloring. We opted to make the icon yellow instead of highlighting the background as we were quickly approaching 50 Shades of Grey.\n\nWe've fixed a scary container snowballing failure mode that could happen when check containers failed to create. This was a somewhat hairy bug; see concourse #2454 for more information.\n\nResources can now be pinned across the pipeline as part of the pipeline config by specifying version on the resource definition in the pipeline. This is analogous to setting version on every get step that references the resource.\n\nPipeline credentials can now be verified via a new --check-creds flag available on fly set-pipeline. This will simply try fetching all of them from the configured credential manager, and let you know which ones couldn't be interpolated. Thanks for the PRs, @edtan!\n\nThe git resource is now smart enough to handle shallow clones while still being able to fetch commits that would not normally be included by the configured depth. This should make configuring depth: 1 safe, so we've removed the condescending warning from the README. Thanks for the PR, @norbertbuchmueller!\n\nThe ATC now exposes an API endpoint for performing a health-check against the configured credential manager, at /api/v1/info/creds. It'll propagate whatever information may be useful, depending on your credential manager backend. Note that this endpoint is only accessible by admin users (members of the main team).\n\nWhen logging in to The fly CLI, if you're already logged in via the web UI it'll just shimmy the existing token over to fly rather than requiring you to go through the login flow all over again.\n\nThis also fixes the annoying behavior of having to log back in to the web UI whenever logging in to fly. Huzzah!\n\nWe gave up on using third-party Go migration libraries and wrote our own. This should make failed migrations a lot easier to troubleshoot and recover from. All of our migrations run in transactions, so there's no more confusing \"dirty\" state, and failed migrations will record the failure error in the database.\n\nWe'll be extracting this package from the ATC soon as a proper library.\n\nPreviously Concourse would allow you to configure arbitrary params in a pipeline, even if the task file itself didn't configure them. This was confusing as it meant the tasks could not be trusted to describe all their required parameters.\n\nConcourse will now emit a warning to the task logs upon noticing this. A future release will turn this into an error. Thanks for the PR, @edtan!\n\nSearching on the dashboard will now live-update the URL bar, making it easy to copy-paste and share the search with your best friends. Thanks for the PR, @SHyx0rmZ!\n\nPreviously, when searching something on the dashboard that filtered out all the pipelines for a given team, the UI showed messaging that made it look like the team had no pipelines at all, when in reality they just didn't match your search terms. Now the UI will just hide the teams entirely!\n\nThe fly validate-pipeline can now be instructed to print the interpolated pipeline config via --output. Thanks for the PR, @henderjm!\n\nThe BOSH release now exposes properties for configuring LDAP auth, thanks to a PR by @JamesClonk!\n\nAll our auth CA certificate properties were broken - they were type: certificate but didn't actually pluck the certificate part off of the property. They're now fixed thanks to a PR by @ArthurHlt.\n\nIf this was working for you before, you'll have to change your manifest such that the properties specified conform to the BOSH certificate type - so you'll just need to take the existing value and nest it under certificate:.\n\nThe TSA will now respect the configured log level for worker heartbeating logs, thanks to a PR by @edtan.\n\nWe got rid of a ton of annoying and chatty logs from the TSA:\n\n* closing-channel\n\n* closing-forwarded-tcpip\n\n* waiting-for-tcpip-io\n\n* done-waiting\n\nThese were useful a long while back as it's fairly tricky to implement a TCP/IP-forwarding SSH server. But now it pretty much works and it was like printing a long line for breathing in and another for breathing out.\n\nThe docker-image resource will now fail more betterly when the build_args_file can't be parsed. Thanks @ghostsquad!\n\nThe docker-image resource will now forego starting the Docker daemon if skip_download: true is set. Thanks @norbertbuchmueller!\n\n@petrosagg a few places where Concourse couldn't compile on 32-bit platforms. See concourse #1379 for more information!\n\nThe bosh-io-release resource now supports configuring a version regexp by which to filter detected versions, thanks to a PR by @dark5un!\n\nThe OIDC auth method now supports being configured with a whitelist of Google hosted domains, thanks to a PR by @rubenv!\n\nThe search field on the dashboard will now live-update the URL, making it easy to share and bookmark pre-set filters. Thanks for the PR, @SHyx0rmZ!\n\nThe s3 resource can now be used with Dell's EMC ECS object store, thanks to a fix by @adam-power!\n\nThe docker-image resource will now fail with a clearer error when your ECR credentials are incorrect, thanks to a PR by @GrantSheehan!\n\nThe docker-image resource now supports interpolating the Concourse-provided env vars in build args, thanks to a PR by @norbertbuchmueller!\n\nThe git resource is now rocking the latest and greatest version of Git LFS, thanks to a PR by @alucillo!\n\nWe went ahead and started using ON CONFLICT in more places where we were sorely needing safe upsert mechanics prior to our bump to Postgres 9.5. Database integrity is cool and deserves to be release notes! Don't @ me.\n\nBuilding a precompiled BOSH release has been fixed. One of our Windows packages was missing the exiter.ps1 short-circuit in its spec. Thanks for the PR, @RomRider!\n\nThe git resource will now emit a short SHA to .git/short_ref, which can be useful for dynamic tagging and such. Thanks for the PR, @suda!\n\nThe s3 resource now supports skip_download: true in params.\n\nThe cf resource now supports configuring vars and vars_files, thanks to a PR by @jmcarp!\n\nThe cf resource now configures env under each application in the manifest, rather than at the top level (which is deprecated). Thanks for the PR, @jmcarp!\n\nClarified the help-text for local user configuration to mention that the password can be in plaintext, and if bcrypted it must have a minimum cost of 10.\n\nFixed a faulty default resulting in borked garbage collection on BOSH deployed workers that are configured to forward through the TSA: worker registration \u0026 forwarding (they would try to reach the instance IP rather than 127.0.0.1).\n\nFixed in-place upgrades of binary-deployed workers. Previously registration would fail with a confusing message saying something like \"base resource type already exists.\"\n\n","depth":2,"section_tag":"v410"},"v420":{"location":"download.html#v420","title":"v4.2.0","text":"{image: images/vapenaysh.png}\n\nFixed a potential information leak: when logged in and viewing a resource from some other team's (exposed) pipeline, you can no longer view the resource's check error, as it may unintentionally have sensitive info in the output. This regressed in v4.0.0.\n\nThe dashboard view will now indicate whether you are a member of each team or whether you're only seeing it because it has exposed pipelines.\n\nThe Running a web node can now be configured to periodically emit build logs to a syslog endpoint! This is configured via --syslog-X flags on the concourse web command. When enabled, build logs will be shipped off in batch as builds complete.\n\nIn v4.1.0 we accidentally broke support for --postgres-data-source. This flag has been deprecated ever since v2.7.2 (over a year ago), so we've opted to finally remove it.\n\nWhen we fixed the login bug in v4.1.0 by storing Dex state in the database (rather than in-memory), that effectively made it so that any changes made to auth settings (like local user config, GitHub config, etc) would not take effect.\n\nThis was because prior to the fix we were using an in-memory store, so all we had to do before was create all the configs anew, but now that things persist we have to do a comparison and update/remove things that were changed or removed from the flags. Thankfully @edtan noticed this and fixed it in a PR!\n\nWith switching to Dex for auth in v4.0.0 we ended up using the external URL as part of the internal login flow callbacks. This meant it would break if your external URL could not be reached (perhaps it's behind a reverse proxy with auth, or a self-signed-cert, or a firewall).\n\nThis is now fixed - the callbacks will go to the internal address only. Sorry for the turbulence! A few folks were stuck on this.\n\nThe fly intercept command will no longer list containers that are still being created and are not yet interceptible, which would lead to an unhelpful websocket: bad handshake error.\n\nFixed one more instance where logged-in users would get logged out too soon. Specifically, on first login the cookie would expire in 1 hour rather than 24 hours.\n\nThe root cause of this was silly.\n\nFixed a potential panic in the 'delete worker' API endpoint, which is used internally as part of the worker draining lifecycle.\n\nThe BOSH release now respects the configured postgresql.client_cert property, thanks to a fix by @flavorjones. This broke back in v3.3.0 when we tweaked the type of the property.\n\nRemoved an artificial limit to the garbage collector that was originally to prevent a stampede of work on a single worker. Now that workers garbage-collect themselves, this was no longer necessary, and only slowed down the database side of the garbage collection lifecycle.\n\nCleaned up our idempotent process reattaching mumbo-jumbo to not rely on Garden properties which should help quiet down the Running a worker node logs (from when we check for a property that hasn't yet been set).\n\nFixed a bug that caused the Vault login retry logic to go into a fast loop if retrying failed for long enough to exceed the maximum retry backoff. Thanks for the PR, @edtan!\n\n","depth":2,"section_tag":"v420"},"v421":{"location":"download.html#v421","title":"v4.2.1","text":"Fixed a bug in the access checking logic for a few worker-related API endpoints. This was introduced in v4.0.0.\n\nSpecifically, the API allowed any logged-in user to prune, retire, and land workers, in addition to performing a few harmless internal garbage-collection calls.\n\nThankfully, the impact of this is fairly small, as the worst someone can do is make your workers become landing or retiring. Which is annoying, but at least there is no risk of gaining access to any of your workers or sensitive pipeline data.\n\n","depth":2,"section_tag":"v421"},"v422":{"location":"download.html#v422","title":"v4.2.2","text":"Fixed an open redirect vulnerability with the login flow that enabled phishy URLs to be crafted to send your auth token to an arbitrary URL.\n\nThis issue affected all versions after and including v4.0.0. The attack vector requires user interaction, but we still highly recommend upgrading to this version now that the exploit is public.\n\n","depth":2,"section_tag":"v422"},"v423":{"location":"download.html#v423","title":"v4.2.3","text":"This release bumps our Garden-runC dependency to v1.18.2 which fixes CVE 2019-5736. We recommend that you upgrade your Concourse cluster to v4.2.3 to prevent this exploit from occurring.\n\nConcourse relies on Garden-runC to create containers for executing jobs and resource checks in pipelines. By default, all containers created by Concourse are unprivileged, and should be safe from CVE 2019-5736.\n\nHowever, if your pipelines configure privileged: true on tasks or privileged: true on resource types in your pipelines, these containers will be privileged, exposing the worker to the attack vector described in CVE 2019-5736. One common example of this is the docker-image resource, which is always privileged.\n\nThe CF/UAA auth connector has been updated to use the authorization_endpoint so that the authentication flow can be completed successfully. Previously, authentication flows would fail whenever a third-party SAML redirect is required.\n\n","depth":2,"section_tag":"v423"},"v500":{"location":"download.html#v500","title":"v5.0.0","text":"This release is a doozy. You should probably read these release notes in full - there are a ton of substantial new features and a good (bad?) amount of breaking changes.\n\nSorry this took so long! The holiday season took its toll, but we also got a bit overzealous with piling feature work on master, and well, we restructured the entire project and re-created its pipeline from scratch, so that didn't help.\n\nOn the plus side, the project restructure is now done, and we'll be implementing a new release process soon that should prevent these kinds of hold-ups from happening again in the future.\n\nSpecial thanks to the many individuals in the community who took part in this release - whether you submitted a PR, helped triage issues, helped people out on the forums or in Discord, or simply cheered us on, every little bit helps keep the project humming along. We deeply appreciate it, and look forward to delivering y'all a better and better CI system - hopefully, more continuously.\n\nWe have done a major internal overhaul of how resource versions are stored. As a result, the version history for each resource across your pipelines will be re-set upon upgrading to v5.0.\n\nThe upgrade does however preserve the state of which versions were disabled, and the data relating versions to builds they were inputs to and outputs of.\n\nIn versions prior to v5.0, resource version history was associated to a pipeline resource by name. This meant that if you changed a resource's configuration or type, those old versions would actually stick around, even though they may technically no longer be appropriate.\n\nWith v5.0, resource versions are now tied directly to an anonymous \"resource config\" - basically the source: and type: for the resource. Pipeline resources instead point to a config, and if their source: or type: changes, they'll point to a new config with its own version history.\n\nThis improves the correctness of the system as a whole, eliminating the need to ever \"purge\" the history of a resource.\n\nIn addition, now that versions are tied directly to their configs, check containers are also shared across teams, reducing the overall container count. As a result however we limited who can fly intercept check containers.\n\nBuilding on this change, we are currently experimenting with improvements that can now be made to reduce the overall checking overhead across a Concourse cluster that has many equivalent resource definitions across pipelines and teams. This is currently off by default while we learn more about the implications - see Global Resources (experimental) for more information.\n\nWe have removed --allow-all-users as almost every use has been a misuse. You must configure users explicitly now instead. This was done for development environments but even those were trivial to switch to a local user whitelist.\n\nIf you were setting this flag before, you probably didn't mean to - setting this with GitHub oAuth configured, for example, would allow literally everyone to be a part of your team and manage your pipelines.\n\nAfter upgrading, any teams that had this configured will preserve the behavior from before - they will continue to allow all users. The next time the teams are configured, however, you will have to specify something else, as the CLI no longer has the flag.\n\nThe concourse binary distribution has been rejiggered. Rather than a self-contained binary, we now ship it as a .tgz containing the binary and its dependencies pre-extracted. The .tgz should be extracted somewhere like /usr/local, resulting in /usr/local/concourse/bin/....\n\nThe main benefit of this is simplification and faster startup. The concourse worker command no longer needs to extract resource types/etc. on start, so this speeds that up quite a bit.\n\nThe concourse binary no longer directly embeds Garden-runC code, and instead ships alongside the gdn binary, copied from their releases. This simplifies the interface for configuring Garden and allows us to leverage their build process rather than risking deviation.\n\nThe \"breaking\" aspect of this is that if you have been passing esoteric flags to Garden you'll have to switch to using a config file via --garden-config instead, or pass them as env vars (e.g. CONCOURSE_GARDEN_FOO_BAR) - flags are no longer supported as those relied on directly embedding their code.\n\nWorkers can now be configured to periodically rebalance so that they don't end up all forwarding through a single Running a web node. This is done by setting the --rebalance-interval flag on concourse worker. The rebalancing makes sure to drain in-flight connections and should not disrupt any in-flight builds.\n\nAlong the way, we removed support for direct worker registration. The --peer-ip flag is no longer available on concourse worker. To transition to 5.0, just remove the flag - the worker will register via forwarding instead.\n\nForwarding is more secure as it doesn't require opening your workers up to inbound traffic. It's easier for us to just focus on one registration method and make sure it works well.\n\nThis also sets us up for enforcing TLS for all traffic to the forwarded workers in the future (concourse #2415).\n\nThe Concourse BOSH release has been redesigned and is now centered around the concourse binary.\n\nBe sure to recreate your workers after or during the deploy, as the location that the worker stores volumes has changed and the old volume directory will not be cleaned up, effectively leaking disk usage.\n\nThe additional_resource_types property can no longer be configured. We plan to add another mechanism for co-located resources in future releases.\n\nThe concourse release no longer needs to be deployed alongside a garden-runc BOSH release, and instead embeds the gdn binary directly.\n\nAlong the way, we have adopted BPM and now use it for deploying the Running a web node. We also enforce a higher nofile limit which should make large-scale deployments more...scaley.\n\nTwo flags have been modified to be more consistent with other flag syntax:\n\n* concourse web --vault-auth-param foo=bar should now be specified as concourse web --vault-auth-param foo:bar (note the :).\n\n* concourse web --tsa-team-authorized-keys team=path/to/key should now be specified as concourse web --tsa-team-authorized-keys team:path/to/key (note the :).\n\nThe Concourse GitHub repository has been completely restructured. This isn't really a feature per se, but it should make contributing a lot easier.\n\nMore on this on our blog post: The Great Process Update of 2018.\n\nA new resource, the registry-image resource, has been added to the core. This resource is intended to replace the docker-image resource image for image pulling and pushing (but not building).\n\nThis resource improves on the docker-image resource in a few ways:\n\n* It doesn't run Docker to fetch the image - it's written in pure Go, using the google/go-containerregistry package. This makes the implementation much less error-prone.\n\n* Because it doesn't run Docker, it doesn't need a privileged container. The fewer privileged containers in your cluster, the better - especially in light of recent CVE fixes.\n\n* By focusing solely on fetching and pushing, the resource is much smaller and simpler. It also has test coverage!\n\n* The output has pretty colors.\n\nThis all results in much faster, more efficient, and resilient image fetching. We recommend everyone to try switching your image_resources and Resource Types over - in most cases this is just a matter of replacing type: docker-image with type: registry-image.\n\nWe intend to deprecate and phase out support for the docker-image resource in favor of the registry-image resource. We can't really do this until there's a solid direction for image building - preferably with a task, not a resource. This is a more natural split, and supports building images without pushing them - a long awaited ask of the docker-image resource.\n\nAn experimental task for this is available at concourse/builder. This is not yet official, but we've using it in our own pipeline and it's been pretty solid. Feel free to give it a try!\n\nThe next step from here is to actually kick off an RFC for reusable tasks - we're still collecting our thoughts for that in (RF)RFC #7. Once this is done we can formalize concourse/builder.\n\nWe have introduced the first phase of role-based access control!\n\nRight now there are only a few statically defined roles. We started off by supporting the common request of having read-only team members ('team viewer'), and adding a slightly less powerful 'team member' role. See User Roles \u0026 Permissions for more information.\n\nHere's a quick rundown of how things have changed:\n\n* Existing team auth config will be transitioned to the Team Owner role - that is, anyone that can authenticate prior to the upgrade will now be authenticated as an owner of their team. This role is the closest equivalent to what they could do before.\n\n* The The main team still has special admin power, with the slight tweak that only users that are an owner of the main team have admin capabilties.\n\n* Before, teams members could rename or destroy their own team. Team owners no longer have this power - only admins can do this.\n\n* The Team Member role is a new role that allows users to have full read and write powers within the team, except for being able to modify the team itself.\n\n* The Team Viewer role is a new role that allows users to browse the team's pipelines, builds, resources, etc. without permitting any sensitive operations (like fly get-pipeline or triggering builds).\n\nFor a detailed breakdown of each role's capabilties, see the Permission Matrix. To learn how to configure these roles after upgrading, see Setting User Roles.\n\nIf you're curious about the design process for this feature, check out RFC #3 (RBAC)!\n\nWe have replaced resource pausing with resource pinning.\n\nResource pausing had the effect of disabling the periodic checking for the paused resource. However we found that in most cases it was being used in combination with disabling versions to effectively pin a resource to the most recent available version.\n\nHowever, with global resource versions, each resource actually points to a shared history, so pausing checking wouldn't be enough - if any other pipelines had the same resource, new versions would still arrive!\n\nSo instead, versions can now be pinned individually via the web UI or via the pipeline config (see version). Pinned resources will also skip periodic checking, but now even if the checking still happens (because some other pipeline had it un-pinned), the resource will stay pinned to the desired version.\n\nA comment can also be left on pinned versions for explaining to your team-mates why you decided to pin the resource.\n\nDuring the 5.0 upgrade, paused resources will be automatically transitioned to their pinned equivalent, by pinning the resource to the most recent available version. A comment will be left on any resources that are migrated so that it's clear to pipeline users.\n\nTask ((vars)) received a bit of an overhaul, thanks to a PR by @ralekseenkov!\n\n* Values for task ((vars)) can now be provided during fly execute!\n\n* In addition, values may be provided to a task step in a pipeline via vars.\n\n* Tasks can now have ((vars)) pretty much anywhere in their config, not just in image_resource.\n\nIn all cases, vars can also be satisifed via a credential manager, the same as before.\n\nAdmittedly, there is now some cause for confusion with params. This may see clarification with reusable tasks. In addition, pipeline ((params)) will now be referred to as pipeline ((vars)) instead, for consistency.\n\nThe Running a web node can now be configured with a The fewest-build-containers strategy, which will place containers on workers that have the fewest build containers.\n\nAny volumes or containers that disappeared from their worker (possibly due to a worker being re-created and then coming back under the same name) will now be automatically reaped from the database. This makes it easier for Concourse to recover from this situation rather than erroring with file not found or unknown handle errors.\n\nLogs emitted by Concourse components will now be...slightly prettier? They're still JSON (sorry), but the timestamps and log levels are at least human-readable.\n\nIf you've got anything parsing your logs, make sure to update it accordingly!\n\nConcourse will now automatically retry fetching credentials when the request to the credential manager fails, thanks to a PR by @ralekseenkov!\n\nBy default Concourse will retry 5 times, waiting 1 second between each attempt. This can be adjusted with the --secret-retry-attempts and --secret-retry-interval flags on concourse web.\n\nTasks are now permitted to have inputs, outputs, and caches with overlapping paths. This was a hold-over from older versions of the container runtime that did not support this.\n\nThis means that for simple tasks that e.g. make a commit a git repo, you no longer need to copy the input to the output. Yay!\n\nThe put step can now be explicitly given a list of inputs to use, rather than using all of them. This can be used to dramatically speed up builds that have a ton of artifacts prior to a put.\n\nThe fly login flow has been reworked a bit to better support logging in to a remote session. There's now a prettier landing page that detects when the token transfer fails by allowing you to copy the token to your clipboard instead.\n\nThe auto-login prompt will also no longer ask for the token, because that disrupts the normal flow of the command. Previously it would ask for a token but then eat half of the keystrokes from then on. Now it just won't ask for a token.\n\nThe concourse binary now has a generate-key subcommand to assist with - you guessed it - key generation. This is more portable to other platforms (I'm looking at you, Windows) and is more likely to generate keys that Concourse can actually accept (I'm looking at you, OpenSSH 7.8).\n\nThe concourse worker command can now be given a --garden-use-houdini flag on Linux to use the \"no-op\" Houdini Garden backend for those odd cases where you don't really want containerization. (Use sparingly.)\n\nThe timestamps shown in the build header will now transition to absolute instead of relative when the build is over 24 hours old. It wasn't very useful to see things like 128d 15h 30m ago when trying to compare old builds. Thanks for the PR, @Twiknight!\n\nYou may have seen a scary error cropping up around your resources now and then. Something like worker_resource_config_check__resource_config_check_sessio_fkey references unreticulated spline.\n\nWe fixed it. That thing doesn't even exist anymore. Don't worry about it.\n\nWith Concourse 4.x configured with an oAuth provider such as GitHub, a user could log in via GitHub even if they weren't technically a member of any team. They couldn't do anything, mind you, but it was confusing that they were allowed to log in in the first place.\n\nThis is no longer permitted.\n\nSimilarly, fly login will also check to make sure you've successfully logged in to the target team and return an error if the team isn't in your token.\n\nThe AWS SSM credential manager and the AWS SecretsManager credential manager previously had a turf war going on over the AWS_REGION environment variable. They both declared it as their own, meaning if you set it they would both try to be configured, which would fail.\n\nThey now have separately namespaced env vars instead.\n\nfly intercept will now give a better error when it fails to execute the command (e.g. because bash isn't installed in the image).\n\nfly execute can now specify input mappings via -m, which is useful when running with --inputs-from-job when the job renames some inputs.\n\nfly execute with --include-ignored will no longer blow up when files are removed locally.\n\nThe error message when a task's file refers to an unknown artifact source (i.e. the foo in foo/ci/task.yml) has been made more descriptive.\n\nThere's a new fly command for landing workers remotely, called... fly land-worker. This will initiate the landing process via the API and will ultimately result in the worker process exiting. (Which may end up being re-started by whatever process monitor you use, but hey, it landed.)\n\nThe web UI now explains why some get steps have a yellow icon, via a handy-dandy tooltip. (Spoiler: it means the job has never run with that version before!)\n\nfly set-pipeline will now notice when the order of Grouping Jobs has changed and show it in the diff.\n\nfly watch can now be called with --timestamps to show per-line timestamps in the build output. Thanks for the PR, @pivotal-kahin-ng!\n\nfly get-pipeline will now throw an error if the specified pipeline does not exist, rather than returning an empty pipeline config.\n\nFixed various subtle UI issues with the dashboard page: concourse #2430, concourse #2434, concourse #2435.\n\nfly login will no longer prompt for your auth method when a username/password are given via flags. It'll deduce that you're trying to do local auth.\n\nTask caches are now supported on Windows!\n\nFixed an internal bug that made UNIQUE constraints for resource_configs ineffective (concourse #2509). This was fairly low-impact, but database integrity matters!\n\nBitBucket auth support has been re-introduced thanks to PRs to Dex and Concourse by @edtan!\n\nThe /api/v1/resources and /api/v1/jobs endpoints will now return [] instead of null when there are no resources or jobs, thanks to a PR by @pivotal-kahin-ng.\n\nThe dashboard page will now indicate whether you are seeing a pipeline because it's exposed by showing an ominous \"eye\" icon.\n\nFixed handling of auth configs set from empty env vars - previously this would result in bogus Dex configuration (e.g. github:, with no org or team) and sometimes cause things to misbehave.\n\nThe legibility and anti-aliasing of text in the web UI has been improved.\n\nCleaned up some dashboard behavior when there are no pipelines:\n\n* you can now see which team you're a member of, rather than one big 'no pipelines set' page\n\n* the bar along the bottom will now show up\n\n* there's a fancy ASCII art UI now\n\n* the search function is no longer shown (since there's nothing to search)\n\n* the HD view has been disabled and just redirects to / instead, since there was nothing for it to show\n\nThe username part of the top bar will no longer detonate when viewed on a tiny mobile browser.\n\nWhen a resource's metadata is super wide, it will remain cordoned off to the side rather than uncomfortably squishing the resource's get output. Thanks for the fix, @stigtermichiel!\n\nConcourse will now send TCP keepalives for connections to the database. This will allow it to detect when the connection has been interrupted ungracefully. Thanks for the PR, @SimonXming!\n\nThe manifest.json href in the web UI used to be relative to the URL, meaning it was broken on any page except /. This is now fixed.\n\nThe web node used to leak both a connection and a goroutine for each build that completed when configured to drain build logs to syslog. This is now fixed. Sorry about that!\n\nThe resources and resource types returned by fly get-pipeline will now be in a deterministic order, thanks to a PR by @edtan!\n\nfly curl is a new command to assist with (hopefully occasional) manual API requests to Concourse. Thanks for the PR and collaboration, @simonjohansson!\n\nThe --tsa-authorized-keys flag is now optional, for situations where all authorized keys are associated to teams (via --tsa-team-authorized-keys). Thanks for the fix, @tlwr!\n\nThe fly status command will now let you know if your token has expired, rather than happily reporting that everything is fine.\n\nA fly userinfo command has been added which will let you know which teams you are logged in to and which roles you have in each team.\n\nThe positioning of the \"no results\" text when searching on the dashboard has been fixed.\n\n","depth":2,"section_tag":"v500"},"vault-approle-auth":{"location":"vault-credential-manager.html#vault-approle-auth","title":"Using the approle auth backend","text":"The approle backend allows for an app (in this case, Concourse) to authenticate with a role pre-configured in Vault.\n\nWith this backend, the Running a web node is configured with a role_id corresponding to a pre-configured role, and a secret_id which is used to authenticate and acquire a token.\n\nThe approle backend must first be configured in Vault. Vault's approle backend allows for a few parameters which you may want to set to determine the permissions and lifecycle of its issued tokens:\n\npolicies=names: This determines the policies (comma-separated) to set on each token. Be sure to set one that has access to the secrets path - see Configuring the secrets engine for more information.\n\n\ntoken_ttl=duration: This determines the TTL for each token granted. The token can be continuously renewed, as long as it is renewed before the TTL elapses.\n\n\ntoken_max_ttl=duration: This sets a maximum lifetime for each token, after which the token can no longer be renewed.\n\nIf configured, be sure to set the same value on the web node so that it can re-auth before this duration is reached:\n\nCONCOURSE_VAULT_AUTH_BACKEND_MAX_TTL=1h\n\nperiod=duration: If configured, tokens issued will be periodic. Periodic tokens are not bound by any configured max TTL, and can be renewed continuously. It does not make sense to configure both period and token_max_ttl as the max TTL will be ignored.\n\n\ntoken_num_uses=count: This sets a limit on how often a token can be used. We do not recommend setting this value, as it will effectively hamstring Concourse after a few credential acquisitions. The web node does not currently know to re-acquire a token when this limit is reached.\n\n\nsecret_id_ttl=duration and secret_id_num_uses=count: These two configurations will result in the secret ID expiring after the configured time or configured number of log-ins, respectively.\n\nYou should only set these if you have something periodically re-generating secret IDs and re-configuring your web nodes accordingly.\n\n\n\nGiven all that, a typical configuration may look something like this:\n\n$ vault auth enable approle\nSuccess! Enabled approle auth method at: approle/\n$ vault write auth/approle/role/concourse policies=concourse period=1h\nSuccess! Data written to: auth/approle/role/concourse\nNow that the backend is configured, we'll need to obtain the role_id and generate a secret_id:\n\n$ vault read auth/approle/role/concourse/role-id\nKey        Value\n---        -----\nrole_id    5f3420cd-3c66-2eff-8bcc-0e8e258a7d18\n$ vault write -f auth/approle/role/concourse/secret-id\nKey                   Value\n---                   -----\nsecret_id             f7ec2ac8-ad07-026a-3e1c-4c9781423155\nsecret_id_accessor    1bd17fc6-dae1-0c82-d325-3b8f9b5654ee\nThese should then be set on the Running a web node like so:\n\nCONCOURSE_VAULT_AUTH_BACKEND=\"approle\"\nCONCOURSE_VAULT_AUTH_PARAM=\"role_id:5f3420cd-3c66-2eff-8bcc-0e8e258a7d18,secret_id:f7ec2ac8-ad07-026a-3e1c-4c9781423155\"\n","depth":6,"section_tag":"vault-approle-auth"},"vault-cert-auth":{"location":"vault-credential-manager.html#vault-cert-auth","title":"Using the cert auth backend","text":"The cert auth method allows authentication using SSL/TLS client certificates.\n\nWith this backend, the Running a web node is configured with a client cert and a client key. Vault must be configured with TLS, which you should be almost certainly be doing anyway.\n\nThe cert backend must first be configured in Vault. The backend is associated to a policy and a CA cert used to verify the client certificate. It may also be given the client certificate itself.\n\nThe cert backend must first be configured in Vault. Vault's cert backend allows for a few parameters which you may want to set to determine the lifecycle of its issued tokens:\n\npolicies=names: This determines the policies (comma-separated) to set on each token. Be sure to set one that has access to the secrets path - see Configuring the secrets engine for more information.\n\n\nttl=duration: This determines the TTL for each token granted. The token can be continuously renewed, as long as it is renewed before the TTL elapses.\n\n\nmax_ttl=duration: This sets a maximum lifetime for each token, after which the token can no longer be renewed.\n\nIf configured, be sure to set the same value on the web node so that it can re-auth before this duration is reached:\n\nCONCOURSE_VAULT_AUTH_BACKEND_MAX_TTL=1h\n\nperiod=duration: If configured, tokens issued will be periodic. Periodic tokens are not bound by any configured max TTL, and can be renewed continuously. It does not make sense to configure both period and max_ttl as the max TTL will be ignored.\n\n\n\n$ vault auth enable cert\nSuccess! Enabled cert auth method at: cert/\n$ vault write auth/cert/certs/concourse policies=concourse certificate=@out/vault-ca.crt ttl=1h\nSuccess! Data written to: auth/cert/certs/concourse\nOnce that's all set up, you'll just need to configure the client cert and key on the web node like so:\n\nCONCOURSE_VAULT_AUTH_BACKEND=\"cert\"\nCONCOURSE_VAULT_CLIENT_CERT=vault-certs/concourse.crt\nCONCOURSE_VAULT_CLIENT_KEY=vault-certs/concourse.key\nIn this case no additional auth params are necessary, as the Vault's TLS auth backend will check the certificate against all roles if no name is specified.\n\n","depth":6,"section_tag":"vault-cert-auth"},"vault-credential-lookup-rules":{"location":"vault-credential-manager.html#vault-credential-lookup-rules","title":"Credential lookup rules","text":"When resolving a parameter such as ((foo_param)), Concourse will look in the following paths, in order:\n\n* /concourse/TEAM_NAME/PIPELINE_NAME/foo_param\n\n* /concourse/TEAM_NAME/foo_param\n\nVault credentials are actually key-value, so for ((foo)) Concourse will default to the field name value. You can specify the field to grab via . syntax, e.g. ((foo.bar)).\n\nIf the action is being run in the context of a pipeline (e.g. a check or a step in a build of a job), Concourse will first look in the pipeline path. If it's not found there, it will look in the team path. This allows credentials to be scoped widely if they're common across many pipelines.\n\nIf an action is being run in a one-off build, Concourse will only look in the team path.\n\nThe leading /concourse can be changed by specifying the following:\n\nCONCOURSE_VAULT_PATH_PREFIX=/some-other-prefix\n","depth":5,"section_tag":"vault-credential-lookup-rules"},"vault-credential-manager":{"location":"vault-credential-manager.html","title":"The Vault credential manager","text":"Concourse can be configured to pull credentials from a Vault instance.\n\nTo configure this, first configure the URL of your Vault server by setting the following env on the Running a web node:\n\nCONCOURSE_VAULT_URL=https://vault.example.com:8200\nYou may also need to configure the CA cert for Vault:\n\nCONCOURSE_VAULT_CA_CERT=path/to/ca.crt\nYou'll also need to configure how the web node authenticates with Vault - see Authenticating with Vault for more details as that step is quite involved.\n\n","depth":4,"section_tag":"vault-credential-manager"},"vault-periodic-token":{"location":"vault-credential-manager.html#vault-periodic-token","title":"Using a periodic token","text":"The simplest way to authenticate is by generating a periodic token:\n\n$ vault token create --policy concourse --period 1h\nKey                Value\n---                -----\ntoken              s.mSNnbhGAqxK2ZbMasOQ91rIA\ntoken_accessor     0qsib5YcYvROm86cT08IFxIT\ntoken_duration     1h\ntoken_renewable    true\ntoken_policies     [concourse default]\nChoose your --period wisely, as the timer starts counting down as soon as the token is created. You should also use a duration long enough to account for any planned web node downtime.\n\nOnce you have the token, just set the following env on the web node:\n\nCONCOURSE_VAULT_CLIENT_TOKEN=s.mSNnbhGAqxK2ZbMasOQ91rIA\nPeriodic tokens are the quickest way to get started, but they have one fatal flaw: if the web node is down for longer than the token's configured period, the token will expire and a new one will have to be created and configured. This can be avoided by using the approle auth backend.\n\n","depth":6,"section_tag":"vault-periodic-token"},"versioned_resources-table":{"location":"database-schema.html#versioned_resources-table","title":"versioned_resources","text":"This table contains versions of resources discovered by checking.  Versioned resources have attributes describing their version, metadata, resource type, and whether the version is enabled in the pipeline.\n\n","depth":5,"section_tag":"pipelineapi-objects"},"volume-collection":{"location":"garbage-collection.html#volume-collection","title":"Volume Collection","text":"Volume collection is quite a bit simpler than Container Collection.\n\nFirst, volumes are found for deletion. This is just a query for volumes that have NULL references for all four volume owners:\n\n* volumes (worker_resource_cache_id)\n\n* volumes (worker_base_resource_type_id)\n\n* volumes (worker_resource_cache_id)\n\n* volumes (worker_resource_cache_id)\n\nNext, each CREATED volume is transitioned to DESTROYING. This transition can fail if the volume is being used as the parent of a copy-on-write volume that is still in use (e.g. by a build).\n\nThen, for each volume for DESTROYING state, including those that were just transitioned, we execute the following in parallel (as with containers, there is a max-in-flight limit per worker):\n\n* First, look up the volume on the worker and destroy it if it's found.\n\n* Next, delete the volume from the database.\n\nAs with containers, if any part of the deletion sequence returns an error, the volume is skipped. A volume is only ever removed from the database when it's guaranteed that everything has been cleaned up.\n\n","depth":6,"section_tag":"volume-collection"},"volume-internals":{"location":"volume-internals.html","title":"Volumes","text":"","depth":4,"section_tag":"volume-internals"},"volume-lifecycle":{"location":"volume-internals.html#volume-lifecycle","title":"Lifecycle","text":"Volumes can be in one of 3 states; CREATING, CREATED, DESTROYING.\n\nCREATING, volumes are still being initialized in BaggageClaim and are not yet ready to be used. CREATING, volumes can only transition to CREATED.\n\nCREATED volumes are initialized in BaggageClaim and are ready to be used. CREATED volumes can only be transitioned to DESTROYING.\n\nDESTROYING volumes are marked for removal from BaggageClaim, and should no longer be used; they will be removed from the database when they no longer exist in the BaggageClaim server.\n\n","depth":5,"section_tag":"volume-lifecycle"},"volume-locality-strategy":{"location":"container-placement.html#volume-locality-strategy","title":"The volume-locality strategy","text":"When using volume-locality, the Running a web node places task step and put step containers on workers where a majority of their inputs are already present. This is the default strategy.\n\nThe advantage of this approach is that it reduces the likelihood that large artifacts will have to be streamed from one Running a worker node, through the Running a web node, and to the target Running a worker node. For large artifacts, this can result in quite a bit of overhead.\n\nThe disadvantage of this approach is that it can sometimes result in builds \"gravitating\" to a particular worker and overloading it, at least until the resource caches warm across the worker pool.\n\nIf your builds tend to be light on artifacts and heavy on task execution, you may want to try the The fewest-build-containers strategy instead.\n\n","depth":4,"section_tag":"volume-locality-strategy"},"volumes-container_id":{"location":"database-schema.html#volumes-container_id","title":"container_id","text":"The container this volume is associated with.\n\n","depth":5,"section_tag":"runtime"},"volumes-handle":{"location":"database-schema.html#volumes-handle","title":"handle","text":"The unique identifier of the volume in BaggageClaim.\n\n","depth":5,"section_tag":"runtime"},"volumes-state":{"location":"database-schema.html#volumes-state","title":"state","text":"The stage in the lifecycle of the volume. See Volumes.\n\n","depth":5,"section_tag":"runtime"},"volumes-table":{"location":"database-schema.html#volumes-table","title":"volumes","text":"The volumes table represents the set of all volumes across all the workers, and keeps track of their state such that no volume is ever left behind on a worker.\n\nVolumes have a handful of really important attributes.\n\n","depth":5,"section_tag":"runtime"},"volumes-worker_base_resource_type_id":{"location":"database-schema.html#volumes-worker_base_resource_type_id","title":"worker_base_resource_type_id","text":"If this volume is used for a worker base resource type this column points to it.\n\n","depth":5,"section_tag":"runtime"},"volumes-worker_name":{"location":"database-schema.html#volumes-worker_name","title":"worker_name","text":"The name of the worker where the volume located.\n\n","depth":5,"section_tag":"runtime"},"volumes-worker_resource_cache_id":{"location":"database-schema.html#volumes-worker_resource_cache_id","title":"worker_resource_cache_id","text":"If this volume is for a worker resource cache this column points to it.\n\n","depth":5,"section_tag":"runtime"},"volumes-worker_task_cache_id":{"location":"database-schema.html#volumes-worker_task_cache_id","title":"worker_task_cache_id","text":"If this volume is for a worker task cache this column points to it.\n\n","depth":5,"section_tag":"runtime"},"web-node":{"location":"concourse-web.html","title":"Running a web node","text":"The web node is responsible for running the web UI, API, and as well as performing all pipeline scheduling. It's basically the brain of Concourse.\n\n","depth":3,"section_tag":"concourse-web"},"web-operation":{"location":"concourse-web.html#web-operation","title":"Operation","text":"The web nodes themselves are stateless - they don't store anything on disk, and coordinate entirely using the database.\n\n","depth":4,"section_tag":"web-operation"},"web-prerequisites":{"location":"concourse-web.html#web-prerequisites","title":"Prerequisites","text":"Nothing special - the web node is a pretty simple Go application that can be run like a 12-factor app.\n\n","depth":4,"section_tag":"web-prerequisites"},"web-properties":{"location":"concourse-web.html#web-properties","title":"Properties","text":"CPU usage: peaks during pipeline scheduling, primarily when scheduling jobs. Mitigated by adding more web nodes. In this regard, web nodes can be considered compute-heavy more than anything else at large scale.\n\nMemory usage: not very well classified at the moment as it's not generally a concern. Give it a few gigabytes and keep an eye on it.\n\nDisk usage: none\n\nBandwidth usage: aside from handling external traffic, the web node will at times have to stream bits out from one worker and into another while executing Steps.\n\nHighly available: yes; web nodes can all be configured the same (aside from --peer-url) and placed behind a load balancer. Periodic tasks like garbage-collection will not be duplicated for each node.\n\nHorizontally scalable: yes; they will coordinate workloads using the database, resulting in less work for each node and thus lower CPU usage.\n\nOutbound traffic:\n\n* db on its configured port for persistence\n\n* db on its configured port for locking and coordinating in a multi-web node deployment\n\n* directly-registered worker nodes on ports 7777, 7788, and 7799 for checking resources, executing builds, and performing garbage-collection\n\n* other web nodes (possibly itself) on an ephemeral port when a worker is forwarded through the web node's TSA\n\nInbound traffic:\n\n* worker connects to the TSA on port 2222 for registration\n\n* worker downloads inputs from the ATC during fly execute via its external URL\n\n* external traffic to the ATC API via the web UI and The fly CLI\n\n","depth":4,"section_tag":"web-properties"},"web-running":{"location":"concourse-web.html#web-running","title":"Running","text":"The concourse CLI can run as a web node via the web subcommand.\n\nBefore running it, let's configure a local user so we can log in:\n\nCONCOURSE_ADD_LOCAL_USER=myuser:mypass\nCONCOURSE_MAIN_TEAM_LOCAL_USER=myuser\nThis will configure a single user, myuser, with the password mypass. You'll probably want to change those to sensible values, and later you may want to configure a proper auth provider - check out Auth \u0026 Teams whenever you're ready.\n\nNext, you'll need to configure the session signing key, the SSH key for the worker gateway, and the authorized worker key. Check Generating Keys to learn what these are and how they are created.\n\nCONCOURSE_SESSION_SIGNING_KEY=path/to/session_signing_key\nCONCOURSE_TSA_HOST_KEY=path/to/tsa_host_key\nCONCOURSE_TSA_AUTHORIZED_KEYS=path/to/authorized_worker_keys\nFinally, web needs to know how to reach your Postgres database. This can be set like so:\n\nCONCOURSE_POSTGRES_HOST=127.0.0.1 # default\nCONCOURSE_POSTGRES_PORT=5432      # default\nCONCOURSE_POSTGRES_DATABASE=atc   # default\nCONCOURSE_POSTGRES_USER=my-user\nCONCOURSE_POSTGRES_PASSWORD=my-password\nIf you're running PostgreSQL locally, you can probably just point it to the socket and rely on the peer auth:\n\nCONCOURSE_POSTGRES_SOCKET=/var/run/postgresql\nNow that everything's set, run:\n\nconcourse web\nAll logs will be emitted to stdout, with any panics or lower-level errors being emitted to stderr.\n\n","depth":4,"section_tag":"web-running"},"whats-emitted":{"location":"metrics.html#whats-emitted","title":"What's emitted?","text":"This reference section lists of all of the metrics that Concourse emits. We don't include the warning and critical levels as they will keep changing as we optimise the system. To find those, please refer to the source of truth: the code.\n\n","depth":4,"section_tag":"whats-emitted"},"whats-encrypted":{"location":"encryption.html#whats-encrypted","title":"What's encrypted?","text":"The following values are expected to contain credentials, and so will be encrypted:\n\n* Resource sources, as they often contain private keys and other credentials for writing to (or simply granting access to) the resource.\n\n* Resource type sources, for the same reason as above, though this is probably a less common use case.\n\n* Pipeline vars and params, in case they contain sensitive information such as usernames and/or passwords.\n\n* Put step params and get step params are also encrypted, even though they rarely should contain credentials (they're usually in source).\n\n* Team auth configurations, as they often contain things like GitHub or other oAuth client secrets.\n\nNote that the actual implementation encrypts things in a more heavy-handed way than the above list implies. For example, pipeline configs are actually encrypted as one large blob.\n\nNotably, the following things are NOT encrypted:\n\n* Build logs. If your jobs are outputting credentials, encryption won't help you. We have chosen not to tackle this initially as it would introduce a performance burden for what is not as much of an obvious win.\n\n* Resource versions. These should never contain credentials, and are often meaningless on their own.\n\n* Resource metadata. These are visible to anyone if your pipeline is exposed, and should never contain credentials.\n\n* Pipeline names, job names, etc. - anything else that is not a high-risk target for credential leakage, as opposed to regular information leaks.\n\n  Resources and jobs in particular exist in their own tables, with their names in plaintext, and only their config encrypted. In this way, names are not protected, even though the pipeline config itself is also stored as one big encrypted blob.\n\n","depth":4,"section_tag":"whats-encrypted"},"worker containers":{"location":"metrics.html#worker containers","title":"worker containers","text":"The number of containers that are currently running on your workers.\n\nAttributes worker\n\n: The name of the worker.\n\n\n\n\n\n","depth":4,"section_tag":"whats-emitted"},"worker volumes":{"location":"metrics.html#worker volumes","title":"worker volumes","text":"The number of volumes that are currently present on your workers.\n\nAttributes worker\n\n: The name of the worker.\n\n\n\n\n\n","depth":4,"section_tag":"whats-emitted"},"worker-configuration":{"location":"concourse-worker.html#worker-configuration","title":"Configuration","text":"If there's something special about your worker and you'd like to target builds at it specifically, you can configure tags like so:\n\nCONCOURSE_TAG=tag-1,tag-2\nA tagged worker is taken out of the default placement logic. To run build steps on it, specify the tags step modifier. Or, to perform resource checks on it, specify tags on the resource itself.\n\n","depth":4,"section_tag":"worker-configuration"},"worker-heartbeating-and-stalling":{"location":"concourse-worker.html#worker-heartbeating-and-stalling","title":"Worker Heartbeating \u0026 Stalling","text":"Workers will continuously heartbeat to the Concourse cluster in order to remain registered and healthy. If a worker hasn't checked in in a while, possibly due to a network partition, being overloaded, or having crashed, its state will transition to stalled and new workloads will not be scheduled on it until it recovers.\n\nIf the worker remains in this state and cannot be recovered, it can be removed using the fly prune-worker command.\n\n","depth":5,"section_tag":"worker-heartbeating-and-stalling"},"worker-internals":{"location":"worker-internals.html","title":"Workers","text":"","depth":4,"section_tag":"worker-internals"},"worker-lifecycle":{"location":"worker-internals.html#worker-lifecycle","title":"Lifecycle","text":"","depth":5,"section_tag":"worker-lifecycle"},"worker-node":{"location":"concourse-worker.html","title":"Running a worker node","text":"The worker node registers with the Running a web node and is then used for executing builds and performing resource checks. It doesn't really decide much on its own.\n\n","depth":3,"section_tag":"concourse-worker"},"worker-operation":{"location":"concourse-worker.html#worker-operation","title":"Operation","text":"The worker nodes are designed to be stateless and as interchangeable as possible. Tasks and Resources bring their own Docker images, so you should never have to install dependencies on the worker.\n\nIn Concourse, all important data is represented by Resources, so the workers themselves are dispensible. Any data in the work-dir is ephemeral and should go away when the worker machine is removed - it should not be persisted between worker VM or container re-creates.\n\n","depth":4,"section_tag":"worker-operation"},"worker-prerequisites":{"location":"concourse-worker.html#worker-prerequisites","title":"Prerequisites","text":"* Linux: kernel v3.19 or later with support for user namespaces enabled. (This is off by default in some distributions!)\n\n  To enforce memory limits on tasks, memory + swap accounting must be enabled.\n\n* Windows/Darwin: no special requirements (that we know of).\n\n  Note that containerization is fairly primitive on these two platforms, so don't expect great support for multi-tenancy.\n\n","depth":4,"section_tag":"worker-prerequisites"},"worker-properties":{"location":"concourse-worker.html#worker-properties","title":"Properties","text":"CPU usage: almost entirely subject to pipeline workloads. More resources configured will result in more checking, and in-flight builds will use as much CPU as they want.\n\nMemory usage: also subject to pipeline workloads. Expect usage to increase with the number of containers on the worker and spike as builds run.\n\nBandwidth usage: again, almost entirely subject to pipeline workloads. Expect spikes from periodic checking, though the intervals should spread out over enough time. Resource fetching and pushing will also use arbitrary bandwidth.\n\nDisk usage: arbitrary data will be written as builds run, and resource caches will be kept and garbage collected on their own life cycle. We suggest going for a larger disk size if it's not too much trouble. All state on disk must not outlive the worker itself; it is all ephemeral. If the worker is re-created (i.e. fresh VM/container and all processes were killed), it should be brought back with an empty disk.\n\nHighly available: not applicable. Workers are inherently singletons, as they're being used as drivers running entirely different workloads.\n\nHorizontally scalable: yes; workers directly correlate to your capacity required by however many pipelines, resources, and in-flight builds you want to run. It makes sense to scale them up and down with demand.\n\nOutbound traffic:\n\n* external traffic to arbitrary locations as a result of periodic resource checking and running builds\n\n* external traffic to the web node's configured external URL when downloading the inputs for a fly execute\n\n* external traffic to the web node's TSA port (2222) for registering the worker\n\nInbound traffic:\n\n* various connections from the Running a web node on port 7777 (Garden), 7788 (BaggageClaim), and 7799 (garbage collection)\n\n* repeated connections to 7788 and 7788 from the Running a web node's TSA component as it heartbeats to ensure the worker is healthy\n\n","depth":4,"section_tag":"worker-properties"},"worker-running":{"location":"concourse-worker.html#worker-running","title":"Running","text":"The concourse CLI can run as a worker node via the worker subcommand.\n\nFirst, you'll need to configure a directory for the worker to store data:\n\nCONCOURSE_WORK_DIR=/opt/concourse/worker\nThis is where all the builds run, and where all resources are fetched in to, so make sure it's backed by enough storage. Then, configure it like so:\n\nNext, point the worker at your Running a web node like so:\n\nCONCOURSE_TSA_HOST=127.0.0.1:2222\nCONCOURSE_TSA_PUBLIC_KEY=path/to/tsa_host_key.pub\nCONCOURSE_TSA_WORKER_PRIVATE_KEY=path/to/worker_key\nFinally, run:\n\n# run with -E to forward env config, or just set it all as root\nsudo -E concourse worker\nNote that the worker must be run as root, as it orchestrates containers.\n\nAll logs will be emitted to stdout, with any panics or lower-level errors being emitted to stderr.\n\n","depth":4,"section_tag":"worker-running"},"worker-table-addr":{"location":"database-schema.html#worker-table-addr","title":"addr","text":"The full address of the Garden server running for the worker.\n\n","depth":5,"section_tag":"runtime"},"worker-table-baggageclaim-url":{"location":"database-schema.html#worker-table-baggageclaim-url","title":"baggageclaim_url","text":"The full address of the worker's baggageclaim server; used to store volumes on the worker.\n\n","depth":5,"section_tag":"runtime"},"worker-table-name":{"location":"database-schema.html#worker-table-name","title":"name","text":"The unique name for the worker used to identify it in the cluster.\n\n","depth":5,"section_tag":"runtime"},"worker-table-state":{"location":"database-schema.html#worker-table-state","title":"state","text":"The current state of the worker. This keeps track of the worker's lifecycle in order to handle the lifecycle of workers and their ability to have new containers or volumes created on them.\n\n","depth":5,"section_tag":"runtime"},"worker_base_resource_types-table":{"location":"database-schema.html#worker_base_resource_types-table","title":"worker_base_resource_types","text":"An entry in the worker base resource type table represents a location on a particular worker where the rootFS for a specific version of a base_resource_type is found. When a worker registers, its base_resource_types are synced by first removing old versions and then inserting new ones.\n\n","depth":5,"section_tag":"abstract-objects"},"worker_resource_caches-table":{"location":"database-schema.html#worker_resource_caches-table","title":"worker_resource_caches","text":"A worker resource cache is a join between a resource cache and a worker base resource type.\n\nA worker resource cache is automatically removed when its worker base resource type or resource cache is removed. This is used to automatically invalidate any caches that were fetched with an old version of a base resource type.\n\n","depth":5,"section_tag":"abstract-objects"},"worker_resource_config_check_sessions-table":{"location":"database-schema.html#worker_resource_config_check_sessions-table","title":"worker_resource_config_check_sessions","text":"A worker resource config check session is a join table tying a resource config check session to a worker base resource type. When either go away, the worker resource config check session goes away. This is so that containers using an old base resource type are reaped and recreated.\n\n","depth":5,"section_tag":"abstract-objects"},"worker_task_caches-table":{"location":"database-schema.html#worker_task_caches-table","title":"worker_task_caches","text":"The worker task caches table tracks all of the details of a task cache for a specific worker. Worker task caches are created for any task step in a job for all of the paths specified task cache.\n\nThis table is used to identify worker task cache volumes by their id.\n\n","depth":5,"section_tag":"runtime"},"workers-table":{"location":"database-schema.html#workers-table","title":"workers","text":"The workers table represents all of the workers that have registered with the Concourse cluster.\n\n","depth":5,"section_tag":"runtime"}}
